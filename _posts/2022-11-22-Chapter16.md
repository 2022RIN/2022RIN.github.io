---
layout: single
title:  "Chapter16. Transformers – Improving Natural Language Processing with Attention Mechanisms: 트랜스포머 – 주의 메커니즘을 통한 자연어 처리 개선"
use_math: true
---

#### Topic

* 주의 메커니즘(attention mechanism)을 통한 RNN 개선
* 독립형 자가 주의 매커니즘 소개
* 원래 변압기 아키텍처의 이해
* 변압기 기반의 대규모 언어 모델 비교
* 정서 분류를 위한 BERT 미세 조정

## 1. Adding an attention mechanism to RNNs: RNN에 주의 메커니즘 추가

* 이 섹션에서는 예측 모델이 입력 시퀀스의 특정 부분에 다른 것보다 더 집중할 수 있도록 돕는 주의 메커니즘을 개발한 동기와 RNN의 맥락에서 원래 어떻게 사용되었는지에 대해 논의함.


#### Attention helps RNNs with accessing information

* 주의 매커니즘의 개발을 이해하려면 아래 그림과 같이 번역을 생성하기 전에 전체 입력 시퀀스(예: 하나 이상의 문장)를 구문 분석하는 언어 번역과 같은 seq2seq 작업에 대한 전통적인 RNN 모델을 고려해야함.

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.1. A traditional RNN encoder-decoder architecture for a seq2seq modeling task
</figcaption>

* RNN이 첫 번째 출력을 생성하기 전에 전체 입력 문장을 구문 분석하는 이유는 아래 그림에 설명된 것처럼 문장을 한 단어씩 번역하는 것이 문법적 오류를 초래할 가능성이 있다는 사실에 의함.

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.2. Translating a sentence word by word can lead to grammatical errors
</figcaption>

* 위 그림을 통해 seq2seq 접근법의 한 가지 한계는 RNN이 변환하기 전에 하나의 숨겨진 장치를 통해 전체 입력 시퀀스를 기억하려고 한다는 것임. 모든 정보를 하나의 숨겨진 장치로 압축하면 특히 긴 시퀀스에서 정보가 손실될 수 있음. 따라서 인간이 문장을 번역하는 방법과 유사하게 각 시간 단계에서 전체 입력 시퀀스에 액세스하는 것이 유익할 수 있음.

#### The original attention mechanism for RNNs

* 입력 시퀀스 $x=(x^{(1), x^{(2)},..., x^{(T)}가 주어지면, 주의 메커니즘은 각 요소에 가중치를 할당하고 모델이 입력의 어느 부분에 초점을 맞춰야 하는지를 식별하는 데 도움이됨. 
* 예를 들어, 입력이 문장이고, 가중치가 더 큰 단어가 전체 문장을 이해하는 데 더 기여한다고 가정, 아래 그림에 표시된 주의 메커니즘이 있는 RNN(앞에서 언급한 논문을 본떠 모델링)은 두 번째 출력 단어를 생성하는 전반적인 개념을 보여줌. 





