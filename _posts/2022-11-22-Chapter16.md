---
layout: single
title:  "Chapter16. Transformers – Improving Natural Language Processing with Attention Mechanisms: 트랜스포머 – 주의 메커니즘을 통한 자연어 처리 개선"
use_math: true
---

#### Topic

* 주의 메커니즘(attention mechanism)을 통한 RNN 개선
* 독립형 자가 주의 매커니즘 소개
* 원래 변압기 아키텍처의 이해
* 변압기 기반의 대규모 언어 모델 비교
* 정서 분류를 위한 BERT 미세 조정

## 1. Adding an attention mechanism to RNNs: RNN에 주의 메커니즘 추가


