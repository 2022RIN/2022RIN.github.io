---
layout: single
title: "Chapter 19. Reinforcement Learning for Decision Making in Complex Environments"
use_math: true
---

## Topic 

* 이 장에서는 전반적인 **보상(reward)** 를 최적화하기 위한 일련의 **행동(action)** 을 학습하는데 중점을 두고 있기에 이전 범주와는 다른 기계 학습, **강화 학습(reinforcement learning; RL)** 로 관심을 돌리며 다음과 같은 항목을 다룸.

    * 1) 강화학습의 기초를 배우고, 에이전트/환경 상호 작용에 익숙해지고, 보상 프로세스가 어떻게 작동하는지 이해
    * 2) 다양한 범주의 RL 문제, 모델 기반 및 모델 없는 학습 과제, 몬테 카를로, 시간적 차이 학습 알고리즘 도입
    * 3) 구현 표 형식의 Q-러닝 알고리즘
    * 4) RL 문제를 해결하기 위한 함수 근사치 이해, 심층 Q-러닝 알고리즘을 구현하여 RL과 딥러닝을 결합
    

## 1. Introduction – learning from experience(경험을 통한 학습)

* 이 섹션에서는 강화학습의 개념을 머신러닝의 다른 작업과 비교하여 주요 차이점을 확인함. 
* 강화학습의 기본 구성 요소를 다룸.
* 마르코프 결정과정을 기반으로 한 강화학습의 수학 공식을 살펴봄.

### Understanding reinforcement learning: 강화학습의 이해 

* 이 문서는 주로 지도 학습과 비지도 학습에 초점을 맞추었음. 
* 지도학습: 감독자가 제공하는 레이블링 된 훈련 예제에 의존함.(즉, 정답이 있는 데이터로 훈련을 함. 컴퓨터는 자신이 낸 답과 정답의 차이를 통해 지속적으로 학습함.)
* 비지도학습: 정답없이 주어진 데이터 세트의 기본 구조를 학습 또는 캡처 하여 유사한 분포를 가지는 훈련예제를 생성하는 것을 의미함.
* 강화학습: 강화학습은 지도 및 비지도 학습과 크게 다르기 때문에 종종 "기계학습의 세 번째 범주"로 간주 됨.

* 강화학습을 지도 학습 및 비지도 학습과 같은 머신 러닝의 다른 하위 작업과 구별하는 핵심 요소는 **상호 작용에 의한 학습 개념을 중심으로 한다는 것** 임. 이는 **강화학습에서 모델이 보상 함수를 최대화하기 위해 환경과의 상호 작용에서 학습한다는 것을 의미함.**
* 보상 함수를 최대화 하는 것은 지도 학습에서 손실 함수(loss function)를 최소화하는 개념과 관련이 있지만, 일련의 동작을 학습하기 위한 **올바른 레이블(정답을 의미)은 강화학습에서 사전에 알려져 있거나 정의되지 않음.** 
* 대신, **원하는 결과(게임에서 이기는 것 등)를 달성하기 위해서 환경과의 상호 작용을 통해 학습해야함.** 
* 모델(에이전트라고도 함)은 환경과 상호 작용하며, 그렇게 함으로써 **에피소드**라고 불리는 일련의 상호 작용을 생성함. 
* 이러한 상호 작용을 통해 에이전트는 환경에 의해 결정된 일련의 보상을 수집함. 이러한 **보상은 긍정적이거나 부정적**일 수 있으며, 때로는 에피소드가 끝날 때까지 에이전트에게 공개되지 않음.
* 강화학습에서 우리는 에이전트에게 일을하는 방법을 가르치지 않고, 에이전트가 성취하기를 원하는 것(목표)만을 명시할 수 있음. 그런 다음 에이전트의 승패에 따라 보상을 결정할 수 있음. 이는 특히 **문제 해결 과제가 알수 없거나 복잡한 의사 결정 문제**에서 매력적인 알고리즘으로 적용됨. 

### Defining the agent-environment interface of a reinforcement learning system: 강화학습 시스템의 에이전트-환경 인터페이스 정의 

* 강화학습에서 에이전트(agnet)와 환경(environment)라는 두 개의 별개 실체를 정의할 수 있음.
* **에이전트**: 공식적으로 결정을 내리는 방법을 배우고 조치를 취함으로써 주변 환경과 상호작용하는 엔티티
* 상호작용의 대가로 조치를 취한 결과 에이전트는 환경에 의해 통제되는 관찰과 보상 신호를 받음.
* **환경**: 에이전트 외부에 있는 모든 것. 환경은 에이전트과의 커뮤니케이션을 통해 에이전트의 작업 및 관찰에 대한 보상 신호를 결정함. 
* **보상**: 에이전트가 환경과의 상호작용하여 받는 피드백으로 일반적으로 스칼라 값 형태로 제공되며 양 또는 음일 수 있음. 보상의 목적은 에이전트에게 그것이 얼마나 잘 수행되었는지 알려주는 것임. 에이전트가 보상을 받는 빈도는 주어진 작업 또는 문제에 따라 달라짐.
* 에이전트는 일생동안 누적된 보상을 최대화하려고 시도함. 

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.1. 에이전트와 환경과의 상호 작용 
</figcaption>

* 그림 1은 에이전트와 환경 간의 상호 작용 및 통신을 보여줌.
* 그림 1과 같이 에이전트의 상태는 모든 변수 (1)의 집합임.
* 예를 들어, 로봇 드론의 경우 이러한 변수들은 드론의 현재 위치(경도, 위도, 고도), 드론의 남은 배터리 수명, 각 팬의 속도 등을 포함할 수 있음. 
* 각 시간 단계에서 에이전트는 (2)에서 사용 가능한 일련의 작업을 통해 환경과 상호작용을 함. 
* 상태 $S_{t}$에 있는 동안 $A_{t}$로 표시된 에이전트가 수행하는 동작에 기초하여 에이전트는 보상 신호 (3) $R_{t+1}$을 수신하며 상태는(4) $S_{t+1}$이 됨. 


* 학습 과정 중 에이전트는 총 누적 보상을 극대화하기 위해 **어떤 행동을 선호하고 더 자주 수행(= 활용 exploitation)**해야 하는지 점진적으로 학습할 수 있도록 **다양한 행동(= 탐험 exploration)**을 시도해야함. 
* 위 개념(활용과 탐험)을 이해하기 위해 공학도 졸업생이 학문에 대해 더 깊게 배우기 위해 석사 학위를 추구(= 활용 exploitation)해야할지 회사에서 일을 시작(= 탐험 exploration)해야할지에 대한 고민을 예로 들 수 있음.
* 일반적으로 활용(exploitation)은 단기적 보상이 더 큰 행동을 선택하는 반면, 탐험(exploration)은 장기적으로 더 큰 총 보상을 잠재적으로 초래할 수 있음. 활용과 탐험에 사이에 대한 균형은 광범위하게 연구되어 왔지만, 이 의사 결정 딜레마에 대한 보편적인 해답은 없음. 

## 2. The theoretical foundations of RL: RL의 이론적 기초

* 다음 섹션에서는 먼저 마르코프 의사결정(Markov decision processes)과정의 수학적 공식화, 에피소드 대 연속 작업, 일부 핵심 강화학습 용어 및 벨만 방적식을 사용한 동적 프로그래밍 검토

### Markov decision processes: 마르코프 의사결정 과정 

* 일반적으로 강화학습이 다루는 문제의 유형은 일반적으로 **마르코프 의사결정 프로세스(MDP)**로 공식화됨.
* MDP 문제를 해결하기 위한 표준 접근법은 동적 프로그래밍을 사용하는 것이지만 강화학습은 동적 프로그래밍에 비해 몇 가지 주요 이점을 제공함. 
* 그러나 동적 프로그래밍은 상태의 크기(즉, 가능한 구성의 수)가 상대적으로 클 때 실현 가능한 접근법이 아님. 이러한 경우, 강화학습은 MDP를 해결하기 위한 훨씬 더 효율적이고 실용적인 대안 접근법으로 간주됨. 

### The mathematical formulation of Markov decision processes: 마르코프 결정 과정의 수학적 공식화 

* 시간 단계 $t$의 결정이 후속 상황에 영향을 미치는 대화형 및 순차적 의사 결정 과정을 학습해야하는 문제 유형은 수학적으로 MDP로 공식화됨. 

* 강화학습에서 에이전트 환경과의 상호작용의 경우 에이전트 시작 상태를 $S_{0}$로 나타내면 에이전트와 환경 사이의 상호작용은 다음과 같은 순서로 이루어짐.: 
* $\{ S_{0}, A_{0}, R_{1} \}, \{ S_{1}, A_{1}, R_{2} \}, \{ S_{2}, A_{2}, R_{3} \}$
* 여기서, $S_{t}$ 와 $A_{t}$는 시간 단계 $t$에서 수행된 행동과 상태를 나타냄. 
* $R_{t+1}$는 $A_{t}$ 행동을 수행한 후 환경으로부터 받은 보상을 의미함. 


* $S_{t}$, $A_{t}$, $R_{t+1}$은 각각 $s \in \widehat{S}$, $r \in \widehat{R}$, $a \in \widehat{A}$로 표시하는 미리 정의된 유한 집합의 값을 취하는 시간 의존 랜덤 변수임. 
* MDP에서 이러한 시간 종속 랜덤 변수인 $S_{t}$ 및 $R_{t+1}$은 이전 시간 단계인 $t – 1$에서만 값에 의존하는 확률 분포를 가짐.
* $S_{t+1}=s^{'}$ 및 $R_{t+1}=r$에 대한 확률 분포는 이전 상태 $S_{t}$에 대한 조건부 확률로 작성할 수 있으며 다음과 같은 행동 $A_{t}$를 취할 수 있음. 

* $p(s^{'},r \mid s,a) = p(S_{t+1}=s^{'}, R_{t+1}=r \mid S_{t}=s, A_{t}=a)$

* 이 확률 분포는 환경(또는 환경 모델)의 역학을 완전히 정의함. 이 분포를 기반으로 환경의 모든 전이 확률을 계산할 수 있기 때문임.
* 따라서 환경 역학은 다양한 강화학습 방법을 분류하는 중심 기준임. 
* 환경의 모델을 필요로 하거나 환경의 모델(즉, 환경 역학(environment dynamics))을 배우려고 하는 강화학습 방법의 유형을 모델 프리(model-free) 방법과 반대로 모델 기반 방법(model-base)이라함. 

















