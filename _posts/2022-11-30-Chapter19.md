---
layout: single
title: "Chapter 19. Reinforcement Learning for Decision Making in Complex Environments"
use_math: true
---

## Topic 

* 이 장에서는 전반적인 **보상(reward)** 를 최적화하기 위한 일련의 **행동(action)** 을 학습하는데 중점을 두고 있기에 이전 범주와는 다른 기계 학습, **강화 학습(reinforcement learning; RL)** 로 관심을 돌리며 다음과 같은 항목을 다룸.

    * 1) 강화학습의 기초를 배우고, 에이전트/환경 상호 작용에 익숙해지고, 보상 프로세스가 어떻게 작동하는지 이해
    * 2) 다양한 범주의 RL 문제, 모델 기반 및 모델 없는 학습 과제, 몬테 카를로, 시간적 차이 학습 알고리즘 도입
    * 3) 구현 표 형식의 Q-러닝 알고리즘
    * 4) RL 문제를 해결하기 위한 함수 근사치 이해, 심층 Q-러닝 알고리즘을 구현하여 RL과 딥러닝을 결합
    

## 1. Introduction – learning from experience(경험을 통한 학습)

* 이 섹션에서는 강화학습의 개념을 머신러닝의 다른 작업과 비교하여 주요 차이점을 확인함. 
* 강화학습의 기본 구성 요소를 다룸.
* 마르코프 결정과정을 기반으로 한 강화학습의 수학 공식을 살펴봄.

#### Understanding reinforcement learning: 강화학습의 이해 

* 이 문서는 주로 지도 학습과 비지도 학습에 초점을 맞추었음. 
* 지도학습: 
* 비지도학습:
* 강화학습: 강화학습은 지도 및 비지도 학습과 크게 다르기 때문에 종종 "기계학습의 세 번째 범주"로 간주 됨.

* 강화학습을 지도 학습 및 비지도 학습과 같은 머신 러닝의 다른 하위 작업과 구별하는 핵심 요소는 **상호 작용에 의한 학습 개념을 중심으로 한다는 것** 임.
* 
