---
layout: single
title:  "Chapter16. Transformers – Improving Natural Language Processing with Attention Mechanisms: 트랜스포머 – 주의 메커니즘을 통한 자연어 처리 개선"
use_math: true
---

#### Topic

* 주의 메커니즘(attention mechanism)을 통한 RNN 개선
* 독립형 자가 주의 매커니즘 소개
* 원래 변압기 아키텍처의 이해
* 변압기 기반의 대규모 언어 모델 비교
* 정서 분류를 위한 BERT 미세 조정

## 1. Adding an attention mechanism to RNNs: RNN에 주의 메커니즘 추가

* 이 섹션에서는 예측 모델이 입력 시퀀스의 특정 부분에 다른 것보다 더 집중할 수 있도록 돕는 주의 메커니즘을 개발한 동기와 (recurrent neural networks)RNN의 맥락에서 원래 어떻게 사용되었는지에 대해 논의함.


#### Attention helps RNNs with accessing information

* 주의 매커니즘의 개발을 이해하려면 아래 그림과 같이 번역을 생성하기 전에 전체 입력 시퀀스(예: 하나 이상의 문장)를 구문 분석하는 언어 번역과 같은 seq2seq 작업에 대한 전통적인 RNN 모델을 고려해야함.

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.1. A traditional RNN encoder-decoder architecture for a seq2seq modeling task
</figcaption>

* RNN이 첫 번째 출력을 생성하기 전에 전체 입력 문장을 구문 분석하는 이유는 아래 그림에 설명된 것처럼 문장을 한 단어씩 번역하는 것이 문법적 오류를 초래할 가능성이 있다는 사실에 의함.

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.2. Translating a sentence word by word can lead to grammatical errors
</figcaption>

* 위 그림을 통해 seq2seq 접근법의 한 가지 한계는 RNN이 변환하기 전에 하나의 숨겨진 장치를 통해 전체 입력 시퀀스를 기억하려고 한다는 것임. 모든 정보를 하나의 숨겨진 장치로 압축하면 특히 긴 시퀀스에서 정보가 손실될 수 있음. 따라서 인간이 문장을 번역하는 방법과 유사하게 각 시간 단계에서 전체 입력 시퀀스에 액세스하는 것이 유익할 수 있음.

#### The original attention mechanism for RNNs

* 입력 시퀀스 $x=(x^{(1)}, x^{(2)},..., x^{(T)})$가 주어지면, 주의 메커니즘은 각 요소 $x^{(i)}$에 가중치를 할당하고 모델이 입력의 어느 부분에 초점을 맞춰야 하는지를 식별하는 데 도움이됨. 
* 예를 들어, 입력이 문장이고 가중치가 더 큰 단어가 전체 문장을 이해하는데 더 기여한다고 가정함. 아래 그림에 표시된 주의 매커니즘이 있는 RNN은 두번째 출력 단어를 생성하는 전반적인 개념을 보여줌. 

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.3. RNN with attention mechanism
</figcaption>

* 그림에 묘사된 주의 기반 아키텍처는 두 개의 RNN 모델로 구성되어 있음. 

#### Processing the inputs using a bidirectional RNN

* 그림 3에서 주의 기반 RNN의 첫 번째 RNN(RNN #1)은 컨텍스트 벡터(context vector)인 $c_{i}$를 생성하는 양방향 RNN임. 컨텍스트 벡터는 입력 벡터인 $x^{(i)}$의 증가된 버전임. 
* 즉 $c_{i}$ 입력 벡터는 주의 메커니즘을 통해 다른 모든 입력 요소의 정보도 통합함. 그림 3에서 처럼 RNN #2는 RNN #1에 의해 준비된 이 컨텍스트 벡터를 사용하여 출력을 생성함. 
* 양방향 RNN #1은 입력 시퀀스 x를 정방향$(1...T)$으로 처리, 역방향으로 시퀀스를 구문 분석하는 것은 원래 입력 시퀀스를 역순으로 읽는 것과 같은 효과가 있음. 이에 대한 근거는 현재 입력이 문장의 앞뒤 또는 둘 다에 있는 시퀀스 요소에 의존할 수 있기 때문에 추가 정보를 캡처하기 위한 것임. 
* 결과적으로, 우리는 입력 시퀀스를 두 번(즉, 앞과 뒤) 읽음으로써, 각 입력 시퀀스 요소에 대해 두 개의 숨겨진 상태를 갖게됨. 
* 예를 들어, 두 번째 입력 시퀀스 요소 $x^{(2)}$의 경우, 순방향 패스에서 숨겨진 상태 $h_{F}^{(2)}$를 얻고, 역방향 패스에서 숨겨진 상태 $h_{B}^{(2)}$를 얻음. 그런 다음 이 두 개의 숨겨진 상태는 연결되어 숨겨진 상태 $h_^{(2)}$를 형성함. 
* 예를 들어, $h_{F}^{(2)}$와 $h_{B}^{(2)}$가 모두 128차원 벡터라면, $h_^{(2)}$은 256개의 요소로 구성됨. 이는 j번째 단어의 정보를 양방향으로 포함하기 때문에 소스 단어의 "주석"으로 간주할 수 있음. 

#### Generating outputs from context vectors

* 그림 3에서 RNN #2를 출력을 생성하는 주요 RNN으로 간주할 수 있음. 숨겨진 상태 외에도, 소위 문맥 벡터를 입력으로 받음. 컨텍스트 벡터 $c_{i}$는 연결된 숨겨진 상태 $h^{(1)}...h^{(T)}$의 weighted 버전으로, 이전 하위 섹션의 RNN #1에서 얻었음. i번째 입력의 문맥 벡터를 가중의 합으로 계산할 수 있음. 

* $c_{i} =  \sum_{j=1}^T  \alpha_{i,j}h^{(j)}$

* 여기서, $alpha_{i,j}$는 i번째 입력 시퀀스에 대한 주의 가중치를 나타냄. $j=1...T$ i번째 입력 시퀀스 요소의 컨텍스트에서 각 입력 시퀀스 요소에는 고유한 주의 가중치 세트가 있음.


#### Computing the attention weights

* 마지막으로 퍼즐에서 마지막으로 누락된 조각인 주의 가중치를 살펴봄.
* 이러한 가중치는 입력(주석)과 출력(콘텍스트)을 쌍으로 연결하기 때문에 각 주의 가중치에는 두 개의 첨자가 있음. : j는 입력 인덱스의 위치를 나타내고 i는 출력 인덱스의 위치를 나타냄. 주의 가중치는 정렬 점수의 정규화된 버전으로, 정렬 점수는 위치 j 주변의 입력이 위치 i의 출력과 얼마나 잘 일치하는지 평가함. 보다 구체적으로 말하면, 주의 가중치는 다음과 같이 정렬 점수를 정규화하여 계산됨.

(1)

## 2. Introducing the self-attention mechanism: 셀프 주의 메커니즘 소개 

* 이전 섹션에서, 우리는 주의 메커니즘이 긴 시퀀스로 작업할 때 컨텍스트를 기억하는 RNN을 도울 수 있다는 것을 보았음. 다음 섹션에서 보게 될 것처럼, 우리는 RNN의 반복되는 부분 없이 전적으로 주의에 기반한 아키텍처를 가질 수 있음. 

#### Starting with a basic form of self-attention

* 자기 주의(self-attention)을 도입하기 위해 길이 $T, x^{(1)},...,x^{(T)}$의 입력 시퀀스와 출력 시퀀스 $z^{(1)},...,z^{(T)}$가 있다고 가정, 혼란을 피하기위해 $o$를 전체 변압기 모델의 최종 출력으로 사용하고 $z$는 모델의 중간 단계이기 때문에 자기 주의 계층의 출력으로 사용함. 
* 이러한 시퀀스의 각각의 $i$ 원소인 $x^(i)$와 $z^(i)$는 위치 $i$에서 입력에 대한 특징 정보를 나타내는 크기 $d$(즉, $x^(i) \in R^{d}$)의 벡터이며, 이는 RNN과 유사
* 그런 다음 seq2seq 작업의 경우 자체 주의의 목표는 현재 입력 요소의 다른 모든 입력 요소에 대한 종속성을 모델링함.
* 이를 위해 자기 주의 메커니즘은 3단계로 구성: 
* 첫째, 현재 요소와 시퀀스의 다른 모든 요소 사이의 유사성을 기반으로 중요도 가중치를 도출
* 둘째, 우리는 가중치를 정규화하는데, 이는 일반적으로 이미 익숙한 소프트맥스 함수의 사용을 포함
* 셋째, 이러한 가중치를 해당 시퀀스 요소와 함께 사용하여 주의 값을 계산
* 형식적으로, 자기 주의의 출력인 $z^(i)$는 모든 T 입력 시퀀스 $x^(i)$의 가중합임.
* 예를 들어 i번째 입력 요소의 경우 해당 출력 값은 다음과 같이 계산
* $z^{(i)} =  \sum_{j=1}^T  \alpha_{i,j}x^{(j)} $

* 따라서, $z^{(i)}$를 각각의 주의 가중치에 의해 가중치가 부여된 다른 모든 입력 시퀀스 요소를 포함하는 입력 벡터 $x^{(i)}$의 컨텍스트 인식 임베딩 벡터로 생각할 수 있음.
* 여기서 주의 가중치인 $\alpha_{i,j}$는 현재 입력 요소인 $x^{(i)}$와 입력 시퀀스의 다른 모든 요소인 $x^{(1)}...x^{(T)}$ 사이의 유사성을 기반으로 계산. 
* 이 유사성은 다음 단락에서 설명하는 두 단계로 계산: 먼저 현재 입력 요소인 $x^{(i)}$와 입력 시퀀스의 다른 요소인 $x^{(j)}$사이의 점 곱을 계산






