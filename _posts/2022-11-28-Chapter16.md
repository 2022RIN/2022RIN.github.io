---
layout: single
title:  "Chapter16. Transformers – Improving Natural Language Processing with Attention Mechanisms: 트랜스포머 – 주의 메커니즘을 통한 자연어 처리 개선"
use_math: true
---

#### Topic

* 주의 메커니즘(attention mechanism)을 통한 RNN 개선
* 독립형 자가 주의 매커니즘 소개
* 원래 변압기 아키텍처의 이해
* 변압기 기반의 대규모 언어 모델 비교
* 정서 분류를 위한 BERT 미세 조정

## 1. Adding an attention mechanism to RNNs: RNN에 주의 메커니즘 추가

* 이 섹션에서는 예측 모델이 입력 시퀀스의 특정 부분에 다른 것보다 더 집중할 수 있도록 돕는 주의 메커니즘을 개발한 동기와 (recurrent neural networks)RNN의 맥락에서 원래 어떻게 사용되었는지에 대해 논의함.


#### Attention helps RNNs with accessing information

* 주의 매커니즘의 개발을 이해하려면 아래 그림과 같이 번역을 생성하기 전에 전체 입력 시퀀스(예: 하나 이상의 문장)를 구문 분석하는 언어 번역과 같은 seq2seq 작업에 대한 전통적인 RNN 모델을 고려해야함.

![1](https://user-images.githubusercontent.com/113302607/204256814-a86ac137-51b3-4f8e-8d68-b323cfa67669.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.1. A traditional RNN encoder-decoder architecture for a seq2seq modeling task
</figcaption>

* RNN이 첫 번째 출력을 생성하기 전에 전체 입력 문장을 구문 분석하는 이유는 아래 그림에 설명된 것처럼 문장을 한 단어씩 번역하는 것이 문법적 오류를 초래할 가능성이 있다는 사실에 의함.

![2](https://user-images.githubusercontent.com/113302607/204256863-bc062277-2bf9-43c0-8de1-ca45156af627.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.2. Translating a sentence word by word can lead to grammatical errors
</figcaption>

* 위 그림을 통해 seq2seq 접근법의 한 가지 한계는 RNN이 변환하기 전에 하나의 숨겨진 장치를 통해 전체 입력 시퀀스를 기억하려고 한다는 것임. 모든 정보를 하나의 숨겨진 장치로 압축하면 특히 긴 시퀀스에서 정보가 손실될 수 있음. 따라서 인간이 문장을 번역하는 방법과 유사하게 각 시간 단계에서 전체 입력 시퀀스에 액세스하는 것이 유익할 수 있음.

#### The original attention mechanism for RNNs

* 입력 시퀀스 $x=(x^{(1)}, x^{(2)},..., x^{(T)})$가 주어지면, 주의 메커니즘은 각 요소 $x^{(i)}$에 가중치를 할당하고 모델이 입력의 어느 부분에 초점을 맞춰야 하는지를 식별하는 데 도움이됨. 
* 예를 들어, 입력이 문장이고 가중치가 더 큰 단어가 전체 문장을 이해하는데 더 기여한다고 가정함. 아래 그림에 표시된 주의 매커니즘이 있는 RNN은 두번째 출력 단어를 생성하는 전반적인 개념을 보여줌. 

![3](https://user-images.githubusercontent.com/113302607/204256893-0f7e0abb-2487-4d36-8f0c-6a8bd086618d.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.3. RNN with attention mechanism
</figcaption>

* 그림에 묘사된 주의 기반 아키텍처는 두 개의 RNN 모델로 구성되어 있음. 

#### Processing the inputs using a bidirectional RNN

* 그림 3에서 주의 기반 RNN의 첫 번째 RNN(RNN #1)은 컨텍스트 벡터(context vector)인 $c_{i}$를 생성하는 양방향 RNN임. 컨텍스트 벡터는 입력 벡터인 $x^{(i)}$의 증가된 버전임. 
* 즉 $c_{i}$ 입력 벡터는 주의 메커니즘을 통해 다른 모든 입력 요소의 정보도 통합함. 그림 3에서 처럼 RNN #2는 RNN #1에 의해 준비된 이 컨텍스트 벡터를 사용하여 출력을 생성함. 
* 양방향 RNN #1은 입력 시퀀스 x를 정방향$(1...T)$으로 처리, 역방향으로 시퀀스를 구문 분석하는 것은 원래 입력 시퀀스를 역순으로 읽는 것과 같은 효과가 있음. 이에 대한 근거는 현재 입력이 문장의 앞뒤 또는 둘 다에 있는 시퀀스 요소에 의존할 수 있기 때문에 추가 정보를 캡처하기 위한 것임. 
* 결과적으로, 우리는 입력 시퀀스를 두 번(즉, 앞과 뒤) 읽음으로써, 각 입력 시퀀스 요소에 대해 두 개의 숨겨진 상태를 갖게됨. 
* 예를 들어, 두 번째 입력 시퀀스 요소 $x^{(2)}$의 경우, 순방향 패스에서 숨겨진 상태 $h_{F}^{(2)}$를 얻고, 역방향 패스에서 숨겨진 상태 $h_{B}^{(2)}$를 얻음. 그런 다음 이 두 개의 숨겨진 상태는 연결되어 숨겨진 상태 $h_^{(2)}$를 형성함. 
* 예를 들어, $h_{F}^{(2)}$와 $h_{B}^{(2)}$가 모두 128차원 벡터라면, $h_^{(2)}$은 256개의 요소로 구성됨. 이는 j번째 단어의 정보를 양방향으로 포함하기 때문에 소스 단어의 "주석"으로 간주할 수 있음. 

#### Generating outputs from context vectors

* 그림 3에서 RNN #2를 출력을 생성하는 주요 RNN으로 간주할 수 있음. 숨겨진 상태 외에도, 소위 문맥 벡터를 입력으로 받음. 컨텍스트 벡터 $c_{i}$는 연결된 숨겨진 상태 $h^{(1)}...h^{(T)}$의 weighted 버전으로, 이전 하위 섹션의 RNN #1에서 얻었음. i번째 입력의 문맥 벡터를 가중의 합으로 계산할 수 있음. 

* $c_{i} =  \sum_{j=1}^T  \alpha_{i,j}h^{(j)}$

* 여기서, $alpha_{i,j}$는 i번째 입력 시퀀스에 대한 주의 가중치를 나타냄. $j=1...T$ i번째 입력 시퀀스 요소의 컨텍스트에서 각 입력 시퀀스 요소에는 고유한 주의 가중치 세트가 있음.


#### Computing the attention weights

* 마지막으로 퍼즐에서 마지막으로 누락된 조각인 주의 가중치를 살펴봄.
* 이러한 가중치는 입력(주석)과 출력(콘텍스트)을 쌍으로 연결하기 때문에 각 주의 가중치에는 두 개의 첨자가 있음. : j는 입력 인덱스의 위치를 나타내고 i는 출력 인덱스의 위치를 나타냄. 주의 가중치는 정렬 점수의 정규화된 버전으로, 정렬 점수는 위치 j 주변의 입력이 위치 i의 출력과 얼마나 잘 일치하는지 평가함. 보다 구체적으로 말하면, 주의 가중치는 다음과 같이 정렬 점수를 정규화하여 계산됨.

![(1)](https://user-images.githubusercontent.com/113302607/204256918-f28c589c-d9df-4cd7-840e-c36046233d9d.png)


## 2. Introducing the self-attention mechanism: 셀프 주의 메커니즘 소개 

* 이전 섹션에서, 우리는 주의 메커니즘이 긴 시퀀스로 작업할 때 컨텍스트를 기억하는 RNN을 도울 수 있다는 것을 보았음. 다음 섹션에서 보게 될 것처럼, 우리는 RNN의 반복되는 부분 없이 전적으로 주의에 기반한 아키텍처를 가질 수 있음. 

#### Starting with a basic form of self-attention

* 자기 주의(self-attention)을 도입하기 위해 길이 $T, x^{(1)},...,x^{(T)}$의 입력 시퀀스와 출력 시퀀스 $z^{(1)},...,z^{(T)}$가 있다고 가정, 혼란을 피하기위해 $o$를 전체 변압기 모델의 최종 출력으로 사용하고 $z$는 모델의 중간 단계이기 때문에 자기 주의 계층의 출력으로 사용함. 
* 이러한 시퀀스의 각각의 $i$ 원소인 $x^(i)$와 $z^(i)$는 위치 $i$에서 입력에 대한 특징 정보를 나타내는 크기 $d$(즉, $x^(i) \in R^{d}$)의 벡터이며, 이는 RNN과 유사
* 그런 다음 seq2seq 작업의 경우 자체 주의의 목표는 현재 입력 요소의 다른 모든 입력 요소에 대한 종속성을 모델링함.
* 이를 위해 자기 주의 메커니즘은 3단계로 구성: 
* 첫째, 현재 요소와 시퀀스의 다른 모든 요소 사이의 유사성을 기반으로 중요도 가중치를 도출
* 둘째, 우리는 가중치를 정규화하는데, 이는 일반적으로 이미 익숙한 소프트맥스 함수의 사용을 포함
* 셋째, 이러한 가중치를 해당 시퀀스 요소와 함께 사용하여 주의 값을 계산
* 형식적으로, 자기 주의의 출력인 $z^(i)$는 모든 T 입력 시퀀스 $x^(i)$의 가중합임.
* 예를 들어 i번째 입력 요소의 경우 해당 출력 값은 다음과 같이 계산
* $z^{(i)} =  \sum_{j=1}^T  \alpha_{i,j}x^{(j)} $


#### Parameterizing the self-attention mechanism: scaled dot-product attention : 자기 주의 메커니즘 매개 변수화: 척도화된 도트 제품 주의

* 이전 하위 섹션에서는 출력을 계산할 때 학습 가능한 매개 변수를 포함하지 않았음. 즉, 이전에 도입 된 기본적인 자기주의 메커니즘을 이용하여 변압기 모델은 주어진 시퀀스에 대한 모델 최적화 동안 주의 값을 업데이트하거나 변경할 수 있는 방법에 대해 다소 제한적임.
* 셀프 어텐션 메커니즘을 보다 유연하고 모델 최적화에 맞게 조정하기 위해 모델 훈련 중에 모델 매개 변수로 적합할 수 있는 세 가지 추가 가중치 행렬을 소개
* 우리는 이 세 가지 무게 행렬 $U_{q},U_{k},and,U_{v}$로 나타냄. 다음과 같이 쿼리, 키 및 시퀀스 요소에 입력을 투영하는데 사용
* Query sequence: $q^{(i)}=U_{q}x^{(i)} for i \in [1,T]$
* Key sequence: $k^{(i)}=U_{k}x^{(i)} for i \in [1,T]$
* Value sequence: $v^{(i)}=U_{v}x^{(i)} for i \in [1,T]$

![4](https://user-images.githubusercontent.com/113302607/204256959-e6c09cc2-6b4a-4802-a9f0-aa27fe0f3ac7.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.4. Computing the context-aware embedding vector of the second sequence element
</figcaption>

* 위 그림은 이러한 개별 구성 요소가 두 번째 입력 요소에 해당하는 컨텍스트 인식 임베딩 벡터를 계산하는 데 어떻게 사용되는지 설명함.

## 3. Attention is all we need: introducing the original transformer architecture

* 흥미롭게도, 원래 트랜스포머 아키텍처는 RNN에서 처음 사용된 주의 메커니즘을 기반으로함.
* 원래 주의 메커니즘을 사용한 목적은 긴 문장으로 작업할 때 RNN의 텍스트 생성 기능을 향상시키는 것임.
* 그러나 RNN에 대한 주의 메커니즘을 실험한 지 불과 몇 년 만에 연구원들은 주의 기반 언어 모델이 반복되는 계층을 삭제했을 때 훨씬 더 강력하다는 것을 발견했음.
* 트랜스포머 아키텍처는 원래 언어 번역을 위해 설계되었지만 영어 구성 요소 구문 분석, 텍스트 생성 및 텍스트 분류와 같은 다른 작업으로 일반화할 수 있습음. 나중에, 우리는 이 원래의 변압기 아키텍처에서 파생된 BERT 및 GPT와 같은 인기 있는 언어 모델에 대해 논의할 것임. 원래 변압기 용지에서 수정한 그림 5는 이 섹션에서 논의할 주요 아키텍처와 구성 요소를 보여줍니다.

![5](https://user-images.githubusercontent.com/113302607/204257025-9be25870-9347-4ffd-8ed2-67e848bb9ea0.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.5. The original transformer architecture
</figcaption>

#### Encoding context embeddings via multi-head attention

* 인코더 블록의 전체적인 목표는 순차적인 입력 $X=(x^{(1)},x^{(2)},...,x^{(T)})$를 받아들여 연속적인 표현 $Z=(z^{(1)},z^{(2)},...,z^{(T)})$로 매핑하여 디코더에 전달하는 것임.
* 인코더는 6개의 동일한 레이어의 스택임. 6은 여기서 마법의 숫자가 아니라 원래 변압기 논문에서 만들어진 초 매개 변수 선택일 뿐임.
* 모델 성능에 따라 레이어 수를 조정할 수 있으며 이러한 동일한 각 계층 내부에는 두 개의 하위 계층이 있음.
* 하나는 아래에서 논의할 다중 헤드 자기 주의를 계산하고, 다른 하나는 이전 장에서 이미 경험한 완전히 연결된 계층임. 


## 4. Learning a language model: decoder and masked multi-head attention 언어 모델 학습: 디코더 및 마스킹된 다중 헤드 주의

* 인코더와 유사하게, 디코더는 또한 여러 반복 레이어를 포함함. 
* 이전 인코더 섹션에서 이미 소개한 두 개의 하위 계층(다중 헤드 자기 주의 계층 및 완전 연결 계층) 외에도, 각 반복 계층에는 마스킹된 다중 헤드 주의 하위 계층도 포함됨.
* 마스킹 주의는 원래 주의 메커니즘의 변형으로, 마스킹 주의는 특정 수의 단어를 "마스킹"함으로써 제한된 입력 시퀀스만 모델로 전달함.
* 예를 들어, 레이블이 지정된 데이터 세트를 사용하여 언어 번역 모델을 구축하는 경우, 훈련 절차 중 시퀀스 위치 i에서 위치 1,…, i-1의 올바른 출력 단어만 입력함. 
* 다른 모든 단어(예: 현재 위치 뒤에 오는 단어)는 모델이 "부정행위"를 하지 않도록 모델에서 숨겨짐.
* 이것은 또한 텍스트 생성의 특성과 일치함. 훈련 중에 진정한 번역된 단어가 알려져 있지만, 실제로 우리는 기본 진실에 대해 아무것도 모름. 
* 따라서, 우리는 위치 i에서 모델이 이미 생성한 것에 대한 솔루션만 제공할 수 있음. 아래 그림은 디코더 블록에서 레이어들이 어떻게 배열되는지를 보여줌.

![6](https://user-images.githubusercontent.com/113302607/204257056-7f20854c-55ff-40a7-b4b8-6a0a07abeaa8.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.6.  Layer arrangement in the decoder part
</figcaption>


* 먼저, 이전 출력 단어(출력 임베딩)는 마스킹된 다중 헤드 주의 계층으로 전달됨. 그런 다음, 두 번째 계층은 인코더 블록으로부터 인코딩된 입력과 마스킹된 다중 헤드 주의 계층의 출력을 모두 수신함. 마지막으로, 우리는 다중 헤드 주의 출력을 전체 모델 출력, 즉 출력 단어에 해당하는 확률 벡터를 생성하는 완전히 연결된 레이어로 전달함.

#### Implementation details: positional encodings and layer normalization: 구현 세부사항: 위치 인코딩 및 계층 표준화

* 위치 인코딩은 입력 시퀀스 순서에 대한 정보를 캡처하는 데 도움이 되며 척도화된 도트 제품 주의 계층과 완전히 연결된 계층 모두 순열 불변이기 때문에 변압기의 중요한 부분임. 즉, 위치 인코딩이 없으면 단어 순서가 무시되고 주의 기반 인코딩에 아무런 영향을 미치지 않음. 
* 하지만, 우리는 어순이 문장을 이해하는데 필수적이라는 것을 알고 있음. 예를 들어, 다음의 두 문장을 생각해보면: 1. 메리는 존에게 꽃 2개를 주었다. John은 Mary에게 꽃을 준다. 두 문장에서 발생하는 단어들은 정확히 같다; 그러나, 그 의미들은 매우 다름.


## 5. Building large-scale language models by leveraging unlabeled data

* 이 섹션에서는 원래 변압기에서 등장한 인기 있는 대규모 변압기 모델에 대해 논의할 것
* 이러한 변압기의 한 가지 공통적인 주제는 레이블이 지정되지 않은 매우 큰 데이터 세트에 대해 사전 훈련된 다음 각 대상 작업에 맞게 미세 조정된다는 것임. 먼저 변압기 기반 모델의 일반적인 교육 절차를 소개하고 기존 변압기와 어떻게 다른지 설명함. 
* GPT(Generative Pre-Trained Transformer), BERT(트랜스포머의 양방향 인코더 표현), BART(Bidirectional and Auto-Regressive Transformer)를 포함한 인기 있는 대규모 언어 모델에 중점을 둘 것임.


#### Pre-training and fine-tuning transformer models

* 언어 번역은 감독 작업이며 레이블이 지정된 데이터 세트가 필요하며, 이를 얻으려면 매우 비용이 많이 들 수 있음. 레이블이 지정된 대규모 데이터 세트의 부족은 딥 러닝에서 오래 지속되는 문제이며, 특히 다른 딥 러닝 아키텍처보다 훨씬 더 데이터에 굶주린 변압기와 같은 모델의 경우 더 문제임. 
* 그러나 매일 대량의 텍스트(책, 웹 사이트 및 소셜 미디어 게시물)가 생성된다는 점을 고려할 때, 흥미로운 질문은 어떻게 이러한 레이블이 없는 데이터를 모델 훈련을 개선하는 데 사용할 수 있는지임. 우리가 변압기에서 레이블이 지정되지 않은 데이터를 활용할 수 있는지에 대한 대답은 "그렇다"이며, 트릭은 자체 지도 학습이라고 불리는 과정임. 
* 우리는 일반 텍스트 자체에서 지도 학습으로부터 "라벨"을 생성할 수 있음. 예를 들어, 레이블이 지정되지 않은 큰 텍스트 말뭉치가 주어지면 모델이 단어의 확률 분포를 학습할 수 있고 강력한 언어 모델이 되기 위한 강력한 기반을 형성할 수 있는 다음 단어 예측을 수행하도록 모델을 훈련시킴. 
* 자기 지도 학습은 전통적으로 비지도 사전 훈련이라고도 하며 현대적인 변압기 기반 모델의 성공을 위해 필수적. 감독되지 않은 사전 훈련에서 "감독되지 않은" 것은 레이블이 없는 데이터를 사용한다는 사실을 의미하는 것으로 추정되지만, 데이터의 구조를 사용하여 레이블을 생성하기 때문에(예: 이전에 언급된 다음 단어 예측 작업) 여전히 감독된 학습 프로세스임. 
* 감독되지 않은 사전 훈련과 다음 단어 예측이 어떻게 작동하는지에 대해 좀 더 자세히 설명하자면, 만약 우리가 n개의 단어를 포함하는 문장을 가지고 있다면, 사전 훈련 절차는 다음의 세 단계로 분해될 수 있음.
* 1. 시간 1단계에서 실측값 1, …, i-1을 입력
* 2. 모델에게 위치 i에서 단어를 예측하고 실제 단어 i와 비교하도록 요청
* 3. 모델 및 시간 단계 i:= i+1을 업데이트함. 1단계로 돌아가서 모든 단어가 처리될 때까지 반복함.
