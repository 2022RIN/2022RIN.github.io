---
layout: single
title:  "Chapter9. Predicting Continuous Target Variables with Regression Analysis"
use_math: true
---

* 이번 장에서는 지도 학습의 또 다른 하위 범주인 회귀 분석(Regression Analysis)에 대해 살펴봄. 
* 회귀 모델은 연속적인 척도로 목표 변수를 예측하는데 사용됨. 

## 1. Introducing linear regression
* 선형 회귀 분석의 목표는 하나 이상의 특성과 연속 대상 변수 사이의 관계를 모형화하는 것임. 
* 지도 학습의 하위 범주 중 하나인 분류와 대조적으로 회귀 분석은 범주형 클래스 레이블이 아닌 연속적인 척도로 출력을 예측하는 것을 목표로 함.

#### Simple linear regression

* 단순(일변량(univariate)) 선형 회귀 분석의 목표는 단일 특성(설명 변수(explanatory variable), x)과 연속 값 대상(반응 변수(response variable), y) 사이의 관계를 모형화하는 것
* 하나의 설명 변수를 갖는 선형 모델의 방정식은 다음과 같이 정의됨. 
* 
$y = w_{1}x+b$

* 여기서, 파라미터(바이어스 단위) $b$는 $y$축 절편을 나타내고, $w_{1}$은 설명 변수의 가중치 계수를 의미함. 
* 목표는 설명 변수와 대상 변수 사이의 관계를 설명하기 위해 선형 방정식의 가중치를 학습하는 것이며, 이는 훈련 데이터 세트의 일부가 아닌 새로운 설명 변수의 반응을 예측하는 데 사용될 수 있음. 


<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.1. A simple one-feature linear regression example
</figcaption>

* 이전에 정의한 선형 방정식을 기반으로 선형 회귀는 그림 1과 같이 훈련 예제를 통해 가장 적합한 직선을 찾는 것임. 
* 최적 적합선(best-fitting line)회귀선(regression line)고도 하며, 회귀선에서 훈련 예제에 이르는 수직선은 소위 오프셋 또는 잔차(예측 오차)임.

#### Multiple linear regression

* 선형 회귀 모델을 여러 개의 설명 변수로 일반화할 수도 있으며, 이 과정을 <mark style='background-color: #fff5b1'>다중 선형 회귀</mark>라고 함. 

$y=w_{1}x_{1}+...+w_{m}x_{m}+b = \sum_{i=1}^{m}w_{i}x_{i}+b=w^{T}x+b$

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.2. A two-feature linear regression model
</figcaption>

* 그림 2는 두 가지 특성을 가진 다중 선형 회귀 모델의 2차원 적합 초평면이 어떻게 보일 수 있는지를 보여줌.
* 3차원 산점도에서 다중 선형 회귀 초평면의 시각화는 정적 수치를 볼 때 이미 해석하기가 어려움.
* 산점도(특징이 3개 이상인 데이터 세트에 적합한 다중 선형 회귀 모델)에서 2차원의 초평면을 시각화할 수 있는 좋은 수단이 없기 때문에, 이 장의 예시와 시각화는 주로 단순 선형 회귀를 사용하여 일변량 사례에 초점을 맞춤.
* 그러나 단순 선형 회귀와 다중 선형 회귀는 동일한 개념과 동일한 평가 기법을 기반으로함. 
* 이 장에서 논의할 코드 구현도 두 가지 유형의 회귀 모델과 호환됨. 

## 2. Exploring the Ames Housing dataset
* 첫 번째 선형 회귀 모델을 구현하기 전에 2006년부터 2010년까지 아이오와 주 에임스의 개별 주거 자산에 대한 정보를 포함하는 새로운 데이터 세트인 에임즈 주택 데이터 세트에 대해 논의

#### Loading the Ames Housing dataset into a DataFrame
* 일반 텍스트 형식으로 저장된 표 형식의 데이터로 작업하는 데 권장되는 도구인 panda read_csv 함수를 사용하여 Ames Housing 데이터 세트를 로드

#### Visualizing the important characteristics of a dataset

* 탐색적 데이터 분석(Exploratory data analysis, EDA): 기계 학습 모델의 교육에 앞서 중요하고 권장되는 첫 번째 단계
* 이 섹션의 나머지 부분에서는 특이치의 존재, 데이터 분포 및 기능 간의 관계를 시각적으로 감지하는 데 도움이 될 수 있는 그래픽 EDA 도구 상자의 간단하지만 유용한 몇 가지 기술을 사용함. 
* 데이터 세트의 서로 다른 기능 간의 쌍별 상관 관계를 한 곳에서 시각화할 수 있는 산점도 행렬을 만듦.
* 산점도 행렬을 그리기 위해, mlxtend 라이브러리의 산점도 행렬 함수를 사용함. 


<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.3.  A scatterplot matrix of our data
</figcaption>

#### Looking at relationships using a correlation matrix

* 다음으로 변수 간의 선형 관계를 정량화하고 요약하기 위해 상관 행렬(correlation matrix)을 생성함.
* 상관 행렬이 공분산 행렬의 재조정된 버전이라고 해석할 수 있으며 상관 행렬은 표준화된 특징에서 계산된 공분산 행렬과 동일
* 상관 행렬은 형상 쌍 사이의 선형 의존성을 측정하는 피어슨 곱-모멘트 상관 계수(종종 Pearson's r로 약칭됨)를 포함하는 정사각형 행렬
* 상관 계수의 범위는 –1 ~ 1임. r = 1, r = 0이면 완벽한 양의 상관 관계가 있고, r = –1이면 완벽한 음의 상관 관계가 있음. 
* Pearson의 상관 계수는 x와 y(숫자)의 두 형상 사이의 공분산을 표준 편차(분모)의 곱으로 나눈 값으로 간단하게 계산할 수 있음.

$r = \frac{\sum_{i=1}^{n}[(x^{(i)}-\mu _{x})(y^{(i)}-\mu _{y})]}{\sqrt{\sum_{i=1}^{n}(x^{(i)}-\mu _{x})^{2}\sqrt{\sum_{i=1}^{n}(y^{(i)}-\mu _{y})^{2}} = \frac{\sigma _{xy}}{\sigma _{x}\sigma _{y}}$ 

* 여기서 $\mu$ 해당 특성의 평균 $\sigma _{xy}$는 특성 x와 y 사이의 공분산, $\sigma$ 는 특성의 표준편차

* 다음 코드 예제에서는 산점도 행렬에서 이전에 시각화한 5개의 형상 열에 NumPy의 코로코프 함수를 사용하고 mlxtend의 히트맵 함수를 사용하여 상관 행렬 배열을 히트맵으로 표시함.

```python
>>> import numpy as np
>>> from mlxtend.plotting import heatmap
>>> cm = np.corrcoef(df.values.T)
>>> hm = heatmap(cm, row_names=df.columns, column_names=df.columns)
>>> plt.tight_layout()
>>> plt.show() 
```

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.4. A correlation matrix of the selected variables
</figcaption>

* 선형 회귀 모형을 적합시키기 위해 목표 변수인 SalePrice와 높은 상관 관계가 있는 특징에 관심이 있음.
* 이전 상관 행렬을 살펴보면, SalePrice가 Gr Liv Area 변수(0.71)와 가장 큰 상관관계를 나타낸다는 것을 알 수 있는데, 이는 탐색적 변수가 다음 섹션에서 단순 선형 회귀 모델의 개념을 도입하는 데 좋은 선택으로 보임.


## 3. Implementing an ordinary least squares linear regression model

* 다음 하위 섹션에서는 훈련 예제에 대한 수직 거리 제곱합(잔차 또는 오차)의 합을 최소화하는 선형 회귀선의 매개 변수를 추정하기 위해 일반 최소 제곱법(OLS) 방법을 사용하여 이 퍼즐의 누락된 조각을 채울 것임. 


#### Solving regression for regression parameters with gradient descent

* 아달린에서 이 손실 함수는 평균 제곱 오차(MSE)로, 우리가 OLS에 사용하는 손실 함수와 동일

$L(w,b)=\frac{1}{2n}\sum_{i=1}^{n}(y^{(i)}-\hat{y}^{(i)})^2$

* 여기서, $\hat{y}$는 예측값 $\hat{y}=w^{T}x+b$임. 
* 본질적으로 OLS 회귀는 임계함수가 없는 아달라인으로 이해될 수 있으므로 클래스 라벨 0과 1 대신 연속적인 목표값을 구함.
* 이를 증명하기 위해 2장에서 아달린의 GD 구현을 취하고 임계값 함수를 제거하여 첫 번째 선형 회귀 모델을 구현

```python
class LinearRegressionGD:
 def __init__(self, eta=0.01, n_iter=50, random_state=1):
 self.eta = eta
 self.n_iter = n_iter
 self.random_state = random_state
 def fit(self, X, y):
 rgen = np.random.RandomState(self.random_state)
 self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1])
 self.b_ = np.array([0.])
 self.losses_ = []
 for i in range(self.n_iter):
 output = self.net_input(X)
 errors = (y - output)
 self.w_ += self.eta * 2.0 * X.T.dot(errors) / X.shape[0]
 self.b_ += self.eta * 2.0 * errors.mean()
 loss = (errors**2).mean()
 self.losses_.append(loss)
 return self
 def net_input(self, X):
 return np.dot(X, self.w_) + self.b_
 def predict(self, X):
 return self.net_input(X)
```

* 선형 회귀 분석을 보려면실제 GD 회귀 분석에서, 에임스 주택 데이터 세트의 Gr Living Area(지상 위 거주 구역의 크기, 평방 피트) 기능을 설명 변수로 사용하고 세일 가격을 예측할 수 있는 모델을 훈련시킴. 
* 또한 GD 알고리듬의 더 나은 수렴을 위해 변수를 표준화
* 코드는 다음과 같음.

```python
>>> X = df[['Gr Liv Area']].values
>>> y = df['SalePrice'].values
>>> from sklearn.preprocessing import StandardScaler
>>> sc_x = StandardScaler()
>>> sc_y = StandardScaler()
>>> X_std = sc_x.fit_transform(X)
>>> y_std = sc_y.fit_transform(y[:, np.newaxis]).flatten()
>>> lr = LinearRegressionGD(eta=0.1)
>>> lr.fit(X_std, y_std)
```

* np.new 축을 사용하여 평활을 사용하여 y_std에 대한 해결 방법을 확인
* skickit-learn의 대부분의 데이터 전처리 클래스는 데이터가 2차원 배열에 저장될 것으로 예상
* 이전 코드 예제에서 y[:, np.newaxis]에서 np.newaxis를 사용하면 배열에 새로운 차원이 추가됨. 
* 그런 다음 StandardScaler가 스케일링된 변수를 반환한 후 편의상 flat() 방법을 사용하여 원래 1차원 배열 표현으로 다시 변환진행

* 알고리듬이 손실 최소(여기서, 전역 손실 최소)로 수렴되는지 확인하기 위해 GD와 같은 최적화 알고리듬을 사용할 때 항상 훈련 데이터 세트에 대한 에폭(완전한 반복) 수의 함수로 손실을 표시하는 것이 좋음.

```python
>>> plt.plot(range(1, lr.n_iter+1), lr.losses_)
>>> plt.ylabel('MSE')
>>> plt.xlabel('Epoch')
>>> plt.show()
```

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.5. The loss function versus the number of epochs
</figcaption>

* 다음으로, 선형 회귀선이 훈련 데이터에 얼마나 적합한지 시각


<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.6. A linear regression plot of sale prices versus living area size
</figcaption>

* 이런 관측도 일리가 있지만, 생활권 규모가 집값을 잘 설명하지 못하는 경우가 많다는 점도 데이터로 알 수 있음.
* 이 장의 뒷부분에서는 회귀 모델의 성능을 정량화하는 방법에 대해 논의
* 몇 가지 특이치를 관찰할 수 있음. 예를 들어, 6보다 큰 표준화된 생활 영역에 해당하는 세 개의 데이터 지점에 대한 것이 뒷장에서 특이치 처리에 대해 논의함. 


#### Estimating the coefficient of a regression model via scikit-learn
* 이전 섹션에서 회귀 분석을 위한 작업 모델을 구현했지만 실제 응용 프로그램에서 보다 효율적인 구현에 관심이 있을 수 있음.
* 예를 들어, 회귀를 위한 Scikit-learn의 많은 추정기는 SciPy(scipy.linalg.lstsq)에서 최소 제곱 구현을 사용하며, 이는 차례로 선형 대수 패키지(Linear 
Algebra Package, PACK)를 기반으로 고도로 최적화된 코드 최적화를 사용함.
* sickit-learn의 선형 회귀 구현은 (S)GD 기반 최적화를 사용하지 않기 때문에 표준화 단계를 건너뛸 수 있기 때문에 표준화되지 않은 변수에서도 (더 잘) 작동함.

```python
>>> from sklearn.linear_model import LinearRegression
>>> slr = LinearRegression()
>>> slr.fit(X, y)
>>> y_pred = slr.predict(X)
>>> print(f'Slope: {slr.coef_[0]:.3f}')
Slope: 111.666
>>> print(f'Intercept: {slr.intercept_:.3f}')
Intercept: 13342.979
```

* 표준화되지 않은 Gr Liv Area 및 SalePrice 변수가 장착된 skickit-learn의 선형 회귀 모델은 형상이 표준화되지 않았기 때문에 서로 다른 모델 계수를 산출하였음.
* 그러나 Gr Liv Area에 대한 SalePrice를 그려서 GD 구현과 비교해보면, 우리는 질적으로 그것이 데이터에 비슷하게 잘 맞는다는 것을 알 수 있음.


<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.7. A linear regression plot using scikit-learn
</figcaption>

* 예를 들어, 그림 7을 통해 반적인 결과가 GD 구현과 동일하게 보인다는 것을 알 수 있음. 


## 4. Fitting a robust regression model using RANSAC
* 선형 회귀 모형은 특이치의 존재에 의해 크게 영향을 받을 수 있음.
* 특정 상황에서는 데이터의 매우 작은 부분 집합이 추정된 모델 계수에 큰 영향을 미칠 수 있음.
* 특이치를 탐지하는 데 많은 통계 테스트를 사용할 수 있지만, 이러한 테스트는 이 문서의 범위를 벗어남.
* 특이치를 버리는 것에 대한 대안으로, 우리는 데이터의 부분 집합인 소위 특이치에 회귀 모델을 맞추는 <mark style='background-color: #fff5b1'>RANSAC(RANdom Sample Consensus) 알고리듬</mark> 사용하는 강력한 회귀 방법을 살펴봄.

* 반복적 RANSAC 알고리즘을 다음과 같이 요약
* 랜덤한 수의 예제를 선택하고 모형을 적합시킴.
* 적합 모형에 대해 다른 모든 데이터 점을 검증하고 사용자가 제공한 공차에 속하는 점을 특이치에 추가함.
* 모든 인라이더를 사용하여 모델을 다시 장착함.
* 적합 모형 대 특이치의 오차를 추정함.
* 성능이 특정 사용자 정의 임계값을 충족하거나 고정된 반복 횟수에 도달한 경우 알고리즘을 종료하고, 그렇지 않으면 1단계로 돌아감. 

```python
>>> from sklearn.linear_model import RANSACRegressor
>>> ransac = RANSACRegressor(
... LinearRegression(), 
... max_trials=100, # default value
... min_samples=0.95, 
... residual_threshold=None, # default value 
... random_state=123)
>>> ransac.fit(X, y)
>>> inlier_mask = ransac.inlier_mask_
>>> outlier_mask = np.logical_not(inlier_mask)
>>> line_X = np.arange(3, 10, 1)
>>> line_y_ransac = ransac.predict(line_X[:, np.newaxis])
>>> plt.scatter(X[inlier_mask], y[inlier_mask],
... c='steelblue', edgecolor='white',
... marker='o', label='Inliers')
>>> plt.scatter(X[outlier_mask], y[outlier_mask],
... c='limegreen', edgecolor='white',
... marker='s', label='Outliers')
>>> plt.plot(line_X, line_y_ransac, color='black', lw=2)
>>> plt.xlabel('Living area above ground in square feet')
>>> plt.ylabel('Sale price in U.S. dollars')
>>> plt.legend(loc='upper left')
>>> plt.tight_layout()
>>> plt.show()
```
 
* 위 코드 셋팅은 RANSACRegressor의 최대 반복 횟수를 100으로 설정하고 min_sig=0.95를 사용하여 무작위로 선택한 훈련 예제의 최소 수를 데이터 세트의 95% 이상으로 설정함.
* skickit-learn은 (residual_threshold=Framework를 통해) MAD 추정치를 사용하여 내부 임계값을 선택, 여기서 MAD는 목표 값의 중위수 절대 편차 y를 나타냄.
* 그러나, 내부 임계값에 대한 적절한 값의 선택은 문제에 따라 다르며, 이는 RANSAC의 단점 중 하나임.
* RANSAC 모형을 적합시킨 후 적합된 RANSAC 선형 회귀 모형에서 특이치와 특이치를 구해 선형 적합치와 함께 표시함. 


<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.8. Inliers and outliers identified via a RANSAC linear regression model
</figcaption>

* 다음 코드를 실행하여 모델의 기울기와 절편을 인쇄할 때, 선형 회귀선은 RANSAC를 사용하지 않고 이전 섹션에서 얻은 적합치와 약간 다를 것임. 

```python
>>> print(f'Slope: {ransac.estimator_.coef_[0]:.3f}')
Slope: 106.348
>>> print(f'Intercept: {ransac.estimator_.intercept_:.3f}')
Intercept: 20190.093
```

* resident_threshold 매개 변수를 없음으로 설정했기 때문에 RANSAC는 MAD를 사용하여 특이치와 특이치를 플래그링하기 위한 임계값을 계산하였음.
* 이 데이터 세트에 대한 MAD는 다음과 같이 계산

```python
>>> def mean_absolute_deviation(data):
... return np.mean(np.abs(data - np.mean(data)))
>>> mean_absolute_deviation(y)
58269.561754979375
```

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.9. Inliers and outliers determined by a RANSAC linear regression model with a larger residual threshold
</figcaption>

* 따라서 더 적은 데이터 포인트를 특이치로 식별하려면 이전 MAD보다 큰 resident_threshold 값을 선택할 수 있음.
* 그림 9은 잔차 임계값이 65,000인 RANSAC 선형 회귀 모델의 특이치와 특이치를 보여줌. 
* RANSAC를 사용하여 이 데이터 세트에서 특이치의 잠재적 영향을 줄였지만, 이 접근 방식이 보이지 않는 데이터에 대한 예측 성능에 긍정적인 영향을 미칠지는 알 수 없음. 

## 5. Evaluating the performance of linear regression models

* 데이터 세트를 별도의 교육 및 테스트 데이터 세트로 분할하고, 여기서 전자를 사용하여 모델을 맞추고 후자를 사용하여 보이지 않는 데이터에 대한 성능을 평가하여 일반화 성능을 추정하려함. 
* 단순 회귀 모델을 진행하는 대신, 이제 데이터 세트의 다섯 가지 기능을 모두 사용하고 다중 회귀 모델을 교육할 것임. 
* 우리 모델은 여러 개의 설명 변수를 사용하기 때문에, 우리는 2차원 그림에서 선형 회귀선(또는 정확하게는 초평면)을 시각화할 수 없지만, 예측 값 대비 Residual  값과 예측 값 사이의 차이 또는 수직 거리)를 그려 회귀 모델을 진단할 수 있음.
* 그림은 회귀 모형을 진단하는 데 일반적으로 사용되는 그래픽 도구이며 비선형성과 특이치를 탐지하고 오차가 랜덤하게 분포되어 있는지 확인하는 데 도움이 될 수 있음. 
* 다음 코드를 사용하여 예측 반응에서 실제 목표 변수를 빼는 Residual 그림을 그림. 


<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.10 Residual plots of our data
</figcaption>

* 완벽한 예측의 경우 Residual가 정확히 0이 될 수 있으며, 이는 현실적이고 실용적인 적용에서는 결코 발생하지 않음.
* 그러나 좋은 회귀 모형의 경우 오차가 랜덤하게 분포되고 잔차가 중심선을 중심으로 랜덤하게 분산될 것으로 예상됨.
* Residual가 그림에서 패턴을 보면 이전 잔차 그림에서 볼 수 있듯이 잔차로 누출된 일부 설명 정보를 모형이 캡처할 수 없다는 것을 의미
* 또한 Residual가 그림을 사용하여 특이치를 탐지할 수도 있으며, 특이치는 중심선에서 편차가 큰 점으로 표시됨.

$MSE = \frac{1}{n}\sum_{i=1}^{n}(y^{(i)}-\hat{y}^{(i)})^{2}$

* 6장에서 논의한 교차 검증과 모델 선택을 위해 MSE를 사용할 수 있음. 
* 분류 정확도와 마찬가지로, MSE도 샘플 크기, n에 따라 정규화하며 이를 통해 서로 다른 표본 크기(예: 학습 곡선의 맥락에서)에서도 비교할 수 있음.
* 다음으로 교육 및 테스트 예측의 MSE를 계산함. 

```python
>>> from sklearn.metrics import mean_squared_error
>>> mse_train = mean_squared_error(y_train, y_train_pred)
>>> mse_test = mean_squared_error(y_test, y_test_pred)
>>> print(f'MSE train: {mse_train:.2f}')
MSE train: 1497216245.85
>>> print(f'MSE test: {mse_test:.2f}')
MSE test: 1516565821.00
```

* 훈련 데이터 세트의 MSE가 테스트 세트보다 크다는 것을 알 수 있는데, 이 경우는 모델이 훈련 데이터를 과대적합하고 있음을 나타내는 지표임.
* 원래 단위 척도(여기서는 달러 제곱 대신 달러)에서 오류를 표시하는 것이 더 직관적일 수 있으므로 루트 평균 제곱 오차라고 불리는 MSE의 제곱근을 계산하거나 잘못된 예측을 약간 덜 강조하는 평균 절대 오차(MAE)를 선택할 수 있음.

$MAE = \frac{1}{n}\sum_{i=1}^{n}|y^{(i)}-\hat{y}^{(i)}|$

```python
>>> from sklearn.metrics import mean_absolute_error
>>> mae_train = mean_absolute_error(y_train, y_train_pred)
>>> mae_test = mean_absolute_error(y_test, y_test_pred)
>>> print(f'MAE train: {mae_train:.2f}')
MAE train: 25983.03
>>> print(f'MAE test: {mae_test:.2f}')
MAE test: 24921.29
```

* 테스트 세트 MAE를 기준으로 모델이 평균적으로 약 25,000달러의 오류가 생기는 것을 확인할 수 있음. 
* MAE와 MSE의 해석은 데이터 세트 및 기능 확장에 따라 달라짐. 
* 예를 들어, 판매 가격이 1,000의 배수(K 접미사가 있는)로 표시되는 경우 동일한 모델이 확장되지 않은 기능으로 작동하는 모델에 비해 낮은 MAE를 산출함. 


## 6. Using regularized methods for regression

* 정규화는 추가 정보를 추가하여 복잡성에 대한 패널티를 유도하기 위해 모델의 매개 변수 값을 축소하여 과적합 문제를 해결하기 위한 한 가지 접근법임. 
* 정규화된 선형 회귀에 대한 가장 인기 있는 접근법은 소위 능선 회귀, 최소 절대 수축 및 선택 연산자(least absolute shrinkage and selection operator, LASSO) 및 elastic net이 있음.
* 능선 회귀(Ridge regression)는 가중치의 제곱 합을 MSE 손실 함수에 단순히 추가하는 L2 벌칙(penalized) 모델임.

$L(W)_{Ridge}=\sum_{i=1}^{n}(y^{(i)}-\hat{y}^{(i)})^{2} + \lambda |w|_{2}^{2}$

* L2항의 정의는 다음과 같음. 

$\lambda\left\|W\right\|_{2}^{2}=\lambda\sum_{i=1}^{m}w_{j}^{2}$ 

$L(W)_{Lasso}=\sum_{i=1}^{n}(y^{(i)}-\hat{y}^{(i)})^{2} + \lambda |w|_{1}$

* 여기서 LASSO에 대한 L1 패널티는 다음과 같이 모델 가중치의 절대 크기의 합으로 정의 

$\lambda\left\|W\right\|_{1}=\lambda\sum_{i=1}^{m}|w_{j}|$ 


## 7. Turning a linear regression model into a curve – polynomial regression

* 이전 섹션에서는 설명 변수와 반응 변수 사이의 선형 관계를 가정하였음. 
* 선형성 가정의 위반을 설명하는 한 가지 방법은 다항식을 추가하여 다항식 회귀 모델을 사용하는 것임. 

$y=w_{1}x+w_{2}x^{2}+...+w_{d}x^{d}+b$

* 다음 하위 섹션에서는 이러한 다항식 항을 기존 데이터 세트에 편리하게 추가하고 다항식 회귀 모델을 적합시킬 수 있는 방법에 대해 논의함.


#### Adding polynomial terms using scikit-learn

* 이제 skickit-learn에서 다항식 특징 변압기 클래스를 사용하여 하나의 설명 변수가 있는 간단한 회귀 문제에 2차 항(d = 2)을 추가하는 방법을 배움.
* 그런 다음 다음 다음 단계를 수행하여 다항식을 선형 적합치와 비교함.

```python
# 2차 다항식 항 추가
>>> from sklearn.preprocessing import PolynomialFeatures
>>> X = np.array([ 258.0, 270.0, 294.0, 320.0, 342.0,
... 368.0, 396.0, 446.0, 480.0, 586.0])\
... [:, np.newaxis]
>>> y = np.array([ 236.4, 234.4, 252.8, 298.6, 314.2,
... 342.2, 360.8, 368.0, 391.2, 390.8])
>>> lr = LinearRegression()
>>> pr = LinearRegression()
>>> quadratic = PolynomialFeatures(degree=2)
>>> X_quad = quadratic.fit_transform(X)

# 비교를 위해 단순 선형 회귀 모형 적합
>>> lr.fit(X, y)
>>> X_fit = np.arange(250, 600, 10)[:, np.newaxis]
>>> y_lin_fit = lr.predict(X_fit)

# 다항식 회귀 분석을 위해 변환된 형상에 다중 회귀 모형 적합
>>> pr.fit(X_quad, y)
>>> y_quad_fit = pr.predict(quadratic.fit_transform(X_fit))

# 결과 plot 
>>> plt.scatter(X, y, label='Training points')
>>> plt.plot(X_fit, y_lin_fit,
... label='Linear fit', linestyle='--')
>>> plt.plot(X_fit, y_quad_fit,
... label='Quadratic fit')
>>> plt.xlabel('Explanatory variable')
>>> plt.ylabel('Predicted or known target values')
>>> plt.legend(loc='upper left')
>>> plt.tight_layout()
>>> plt.show()
``` 

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.11 A comparison of a linear and quadratic model
</figcaption>

* 결과 그림에서 다항식 적합이 선형 적합보다 반응 변수와 설명 변수 사이의 관계를 훨씬 더 잘 캡처한다는 것을 알 수 있음. 
* 다음으로 MSE및 $R^{2}$ 평가 지표를 계산함. 

```python
>>> y_lin_pred = lr.predict(X)
>>> y_quad_pred = pr.predict(X_quad)
>>> mse_lin = mean_squared_error(y, y_lin_pred)
>>> mse_quad = mean_squared_error(y, y_quad_pred)
>>> print(f'Training MSE linear: {mse_lin:.3f}'
 f', quadratic: {mse_quad:.3f}')
Training MSE linear: 569.780, quadratic: 61.330
>>> r2_lin = r2_score(y, y_lin_pred)
>>> r2_quad = r2_score(y, y_quad_pred)
>>> print(f'Training R^2 linear: {r2_lin:.3f}'
 f', quadratic: {r2_quad:.3f}')
Training R^2 linear: 0.832, quadratic: 0.982
``` 

* MSE는 570(선형 적합)에서 61(선형 적합)로 감소, 또한 결정 계수는 이 특정 장난감 문제에서 선형 적합(R2 = 0.832)과 반대로 2차 모델의 더 가까운 적합(R2 = 0.982)을 반영함.


## 8. Modeling nonlinear relationships in the Ames Housing dataset

* 다음 코드를 실행하여 2차(사분위) 다항식과 3차(입방체) 다항식을 사용하여 분양가와 지상 생활권 사이의 관계를 모델링하고 선형 적합치와 비교

```python
>>> X = df[['Gr Liv Area']].values
>>> y = df['SalePrice'].values
>>> X = X[(df['Gr Liv Area'] < 4000)]
>>> y = y[(df['Gr Liv Area'] < 4000)]
>>> regr = LinearRegression()
>>> # create quadratic and cubic features
>>> quadratic = PolynomialFeatures(degree=2)
>>> cubic = PolynomialFeatures(degree=3)
>>> X_quad = quadratic.fit_transform(X)
>>> X_cubic = cubic.fit_transform(X)
>>> # fit to features
>>> X_fit = np.arange(X.min()-1, X.max()+2, 1)[:, np.newaxis]
>>> regr = regr.fit(X, y)
>>> y_lin_fit = regr.predict(X_fit)
>>> linear_r2 = r2_score(y, regr.predict(X))
>>> regr = regr.fit(X_quad, y)
>>> y_quad_fit = regr.predict(quadratic.fit_transform(X_fit))
>>> quadratic_r2 = r2_score(y, regr.predict(X_quad))
>>> regr = regr.fit(X_cubic, y)
>>> y_cubic_fit = regr.predict(cubic.fit_transform(X_fit))
>>> cubic_r2 = r2_score(y, regr.predict(X_cubic))
>>> # plot results
>>> plt.scatter(X, y, label='Training points', color='lightgray')
>>> plt.plot(X_fit, y_lin_fit, 
... label=f'Linear (d=1), $R^2$={linear_r2:.2f}',
... color='blue', 
... lw=2, 
... linestyle=':')
>>> plt.plot(X_fit, y_quad_fit, 
... label=f'Quadratic (d=2), $R^2$={quadratic_r2:.2f}',
... color='red', 
... lw=2,
... linestyle='-')
>>> plt.plot(X_fit, y_cubic_fit, 
... label=f'Cubic (d=3), $R^2$={cubic_r2:.2f}',
... color='green', 
... lw=2,
... linestyle='--')
>>> plt.xlabel('Living area above ground in square feet')
>>> plt.ylabel('Sale price in U.S. dollars')
>>> plt.legend(loc='upper left')
>>> plt.show()
``` 

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.12 A comparison of different curves fitted to the sale price and living area data
</figcaption>

* 전체 품질을 살펴봄. 전체 품질 변수는 주택의 전반적인 품질과 마감 품질을 평가하며 1부터 10까지의 척도로 제공됨.

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.13 A linear, quadratic, and cubic fit on the sale price and house quality data
</figcaption>

* 그림 13을 통해 2차 적합치와 3차 적합치는 선형 적합치보다 분양가와 주택의 전반적인 품질 간의 관계를 더 잘 포착함.
* 그러나 다항식 피쳐를 점점 더 추가하면 모형의 복잡성이 증가하므로 과적합 확률이 증가함.
* 따라서 실제로 일반화 성능을 추정하기 위해 항상 별도의 테스트 데이터 세트에서 모델의 성능을 평가하는 것이 권장됨. 


## 9. Dealing with nonlinear relationships using random forests

* 여러 의사 결정 트리의 앙상블인 랜덤 포레스트는 앞에서 논의한 글로벌 선형 및 다항식 회귀 모델과 달리 조각별 선형 함수의 합으로 이해할 수 있음.
* 결정 트리 알고리즘을 통해 입력 공간을 더 많은 사람이 되는 더 작은 영역으로 세분화함. 


#### Random forest regression

* 3장에서 배웠듯이 랜덤 포레스트 알고리즘은 여러 의사 결정 트리를 결합한 앙상블 기법임. 
* 랜덤 포리스트는 일반적으로 랜덤성으로 인해 개별 의사 결정 트리보다 일반화 성능이 우수하므로 모형의 분산을 줄이는 데 도움이 됨.
* 랜덤 포리스트의 또 다른 장점은 데이터 세트의 특이치에 덜 민감하고 많은 매개 변수 조정이 필요하지 않음.
* 회귀를 위한 기본 랜덤 포레스트 알고리즘은 3장에서 논의한 분류를 위한 랜덤 포레스트 알고리즘과 거의 동일함. 
* 유일한 차이점은 MSE 기준을 사용하여 개별 의사 결정 트리를 성장시키고, 예측 목표 변수는 모든 의사 결정 트리에 대한 평균 예측으로 계산됨.

* 이제 Ames Housing 데이터 세트의 모든 기능을 사용하여 예제 중 70%에 랜덤 포레스트 회귀 모델을 맞추고 나머지 30%에 대해 선형 회귀 모델의 성능 평가 섹션에서 수행한 것처럼 성능을 평가함. 다음과 같이 구현됨.

```python
>>> target = 'SalePrice'
>>> features = df.columns[df.columns != target]
>>> X = df[features].values
>>> y = df[target].values
>>> X_train, X_test, y_train, y_test = train_test_split(
... X, y, test_size=0.3, random_state=123)
>>> from sklearn.ensemble import RandomForestRegressor
>>> forest = RandomForestRegressor(
... n_estimators=1000, 
... criterion='squared_error', 
... random_state=1, 
... n_jobs=-1)
>>> forest.fit(X_train, y_train)
>>> y_train_pred = forest.predict(X_train)
>>> y_test_pred = forest.predict(X_test)
>>> mae_train = mean_absolute_error(y_train, y_train_pred)
>>> mae_test = mean_absolute_error(y_test, y_test_pred)
>>> print(f'MAE train: {mae_train:.2f}')
MAE train: 8305.18
>>> print(f'MAE test: {mae_test:.2f}')
MAE test: 20821.77
>>> r2_train = r2_score(y_train, y_train_pred)
>>> r2_test =r2_score(y_test, y_test_pred)
>>> print(f'R^2 train: {r2_train:.2f}')
R^2 train: 0.98
>>> print(f'R^2 test: {r2_test:.2f}')
R^2 test: 0.85
``` 

* 결과를 통해 랜덤 포레스트가 훈련 데이터를 과대적합시키는 경향이 있음을 알 수 있음.
* 그러나 여전히 대상 변수와 설명 변수 사이의 관계를 비교적 잘 설명할 수 있음.(TEST 데이터 세트에서 $R^{2}$ = 0.85)
* 이전 섹션인 동일한 데이터 세트에 적합했던 선형 회귀 모델의 성능 평가의 선형 모델은 과대적합은 적었지만 테스트 세트에서 성능이 더 좋지 않았음.($R^{2}$=0.75)

```python
>>> x_max = np.max([np.max(y_train_pred), np.max(y_test_pred)])
>>> x_min = np.min([np.min(y_train_pred), np.min(y_test_pred)])
>>> fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 3), sharey=True)
>>> ax1.scatter(y_test_pred, y_test_pred - y_test,
... c='limegreen', marker='s', edgecolor='white',
... label='Test data')
>>> ax2.scatter(y_train_pred, y_train_pred - y_train,
... c='steelblue', marker='o', edgecolor='white',
... label='Training data')
>>> ax1.set_ylabel('Residuals')
>>> for ax in (ax1, ax2):
... ax.set_xlabel('Predicted values')
... ax.legend(loc='upper left')
... ax.hlines(y=0, xmin=x_min-100, xmax=x_max+100,
... color='black', lw=2)
>>> plt.tight_layout()
>>> plt.show()
``` 

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.14 The residuals of the random forest regression
</figcaption>

* residuals 분포가 0 중심점을 중심으로 완전히 랜덤하지 않은 것으로 나타나 모형이 모든 탐색 정보를 캡처할 수 없음을 나타냄. 그러나 residuals 그림은 이 장의 앞부분에서 표시한 선형 모형의 residuals 그림에 비해 크게 개선되었음을 나타냄.
* 이상적인 경우, 모델 오류는 랜덤이거나 예측 불가능해야 함. 즉, 예측의 오차는 설명 변수에 포함된 어떤 정보와도 관련이 없어야 하며, 오히려 실제 분포나 패턴의 무작위성을 반영 해야함.



