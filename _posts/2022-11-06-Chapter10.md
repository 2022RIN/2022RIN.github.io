---
layout: single
title:  "Chapter10. Working with Unlabeled Data – Clustering Analysis"
use_math: true
---

* 이전 챕터에서는 지도 학습 기술을 사용하여 이미 답이 알려진 데이터를 사용하여 기계 학습 모델을 구축했음.
* 이 챕터에서는 기어를 바꾸고 우리가 정답을 미리 알지 못하는 데이터의 숨겨진 구조를 발견할 수 있는 비지도 학습 기법의 범주인 클러스터 분석을 탐구
* 군집화의 목표는 데이터에서 자연스러운 그룹화를 찾아 동일한 군집의 항목이 서로 다른 군집의 항목보다 서로 더 비슷하도록 하는 것임.

## 1. Grouping objects by similarity using k-means

#### k-means clustering using scikit-learn

* **k-평균(k-means) 알고리즘**은 구현하기 쉬우며 다른 클러스터링 알고리즘에 비해 계산이 매우 효율적임. 이는 프로토타입 기반 클러스터링 범주에 속함.
* 이 장 뒷부분에서 계층적 및 밀도 기반 클러스터링이라는 두 가지 다른 클러스터링 범주에 대해 설명함.
* 프로토타입 기반 군집화는 각 군집이 프로토타입으로 표현되는 것을 의미하며, 일반적으로 연속 형상을 가진 유사한 점의 중심(평균)이거나 범주형 형상의 경우 메이드 (가장 대표적인 점 또는 특정 군집에 속하는 다른 모든 점까지의 거리를 최소화하는 점)임.
* k-평균은 군집을 식별하는데 매우 뛰어나지만 군집화 알고리즘은 군집 k, a priori의 수를 지정해야 한다는 단점이 있음.(k를 잘못 선택하면 클러스터링 성능이 저하될 수 있음.)

* 시각화를 위해 다음의 간단한 2차원 데이터 세트를 사용하여 다음 예제를 살펴봄.

```python
>>> from sklearn.datasets import make_blobs
>>> X, y = make_blobs(n_samples=150,
... n_features=2,
... centers=3,
... cluster_std=0.5,
... shuffle=True,
... random_state=0)
>>> import matplotlib.pyplot as plt
>>> plt.scatter(X[:, 0],
... X[:, 1],
... c='white',
... marker='o',
... edgecolor='black',
... s=50)
>>> plt.xlabel('Feature 1')
>>> plt.ylabel('Feature 2')
>>> plt.grid()
>>> plt.tight_layout()
>>> plt.show()
```


* 방금 생성한 데이터 세트는 밀도가 높은 세 개의 영역으로 대략 그룹화된 무작위로 생성된 150개의 점으로 구성되어 있으며, 이는 2차원 산점도를 통해 시각화됨.

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.1. A scatterplot of our unlabeled dataset
</figcaption>


* 목표는 다음 네 단계로 요약된 k-평균 알고리듬을 사용하여 달성할 수 있는 특징 유사성을 기반으로 예제를 그룹화하는 것임.
* 1) 예제에서 $k$개의 중심체를 초기 군집 중심으로 랜덤하게 선택
* 2) 각 예제를 가장 가까운 중심점에 할당함. \mu^{j}, j\in \{1,...,k\}
* 3) 중심선을 지정된 예제 가운데로 이동함.
* 4) 클러스터 할당이 변경되지 않거나 사용자 정의 허용 오차 또는 최대 반복 횟수에 도달할 때까지 2단계와 3단계를 반복함.

* 물체들 사이의 유사석 측정은 m차원 공간에서 x와 y의 두 점 사이의 제곱 유클리드 거리로 계산함.

$d(x, y)^{2} = \sum_{j=1}^{m}(x_{j}-y_{j})^2=\begin{Vmatrix}x-y\end{Vmatrix}_{2}^{2}$


* 여기서, 인덱스 j는 예제 입력 x 및 y의 j번째 차원(특성 열)을 나타냄.
* 이 섹션의 나머지 부분에서는 위첨자 i와 j를 사용하여 각각 예제의 인덱스(데이터 레코드)와 클러스터 인덱스를 참조함.
* 유클리드 거리 메트릭을 기반으로, 우리는 k-평균 알고리즘을 클러스터 내 제곱 오차 합(SSE)을 최소화하기 위한 반복적인 접근 방식인 단순한 최적화 문제로 설명하며 이를 클러스터 관성이라함. 
* 이전 방정식에서 인덱스 j는 예제 입력 x 및 y의 j번째 차원(특징 열)을 나타냅니다. 

$SSE = \sum_{i=1}^{n}\sum_{j=1}^{k}w^{(i,j)}=\begin{Vmatrix}x^{(i)}-\mu^{(j)}\end{Vmatrix}_{2}^{2}$

* 여기서, $\mu^{(j)}$는 $x(i)$가 군집 $j$에 있으면 군집 $j. w(i, j) = 1$을 나타내는 점(중심점)이 되고, 그렇지 않으면 $0$이 됨.
* 이제 skickit-learn의 클러스터 모듈의 KMeans 클래스를 사용하여 예제 데이터 세트에 적용


```python
>>> from sklearn.cluster import KMeans
>>> km = KMeans(n_clusters=3,
... init='random',
... n_init=10,
... max_iter=300,
... tol=1e-04,
... random_state=0)
>>> y_km = km.fit_predict(X)
```

* 이제 k-평균이 데이터 세트에서 식별된 클러스터를 클러스터 중심과 함께 시각화함.
* 이 속성은 적합된 KMeans 개체의 cluster_centers_ 속성 아래에 저장됨.

```python
>>> plt.scatter(X[y_km == 0, 0],
... X[y_km == 0, 1],
... s=50, c='lightgreen',
... marker='s', edgecolor='black',
... label='Cluster 1')
>>> plt.scatter(X[y_km == 1, 0],
... X[y_km == 1, 1],
... s=50, c='orange',
... marker='o', edgecolor='black',
... label='Cluster 2')
>>> plt.scatter(X[y_km == 2, 0],
... X[y_km == 2, 1],
... s=50, c='lightblue',
... marker='v', edgecolor='black',
... label='Cluster 3')
>>> plt.scatter(km.cluster_centers_[:, 0],
... km.cluster_centers_[:, 1],
... s=250, marker='*',
... c='red', edgecolor='black',
... label='Centroids')
>>> plt.xlabel('Feature 1')
>>> plt.ylabel('Feature 2')
>>> plt.legend(scatterpoints=1)
>>> plt.grid()
>>> plt.tight_layout()
>>> plt.show()
```

* 그림에서 k-평균이 각 구의 중심에 세 개의 중심체를 배치한 것을 볼 수 있으며, 이 데이터 세트를 고려할 때 합리적인 그룹처럼 판단됨.


<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.2. The k-means clusters and their centroids
</figcaption>


* 여전히 클러스터 k, a priori의 수를 지정해야 하는 단점이 있음. 특히 시각화할 수 없는 고차원 데이터 세트를 사용하는 경우 실제 애플리케이션에서 선택할 클러스터 수가 항상 명확하지는 않을 수 있음.
* k-평균의 다른 특성은 군집이 겹치지 않고 계층적이지 않다는 것이며, 각 군집에 적어도 하나의 항목이 있다고 가정함.


