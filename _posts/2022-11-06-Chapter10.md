---
layout: single
title:  "Chapter10. Working with Unlabeled Data – Clustering Analysis"
use_math: true
---

* 이전 챕터에서는 지도 학습 기술을 사용하여 이미 답이 알려진 데이터를 사용하여 기계 학습 모델을 구축했음.
* 이 챕터에서는 기어를 바꾸고 우리가 정답을 미리 알지 못하는 데이터의 숨겨진 구조를 발견할 수 있는 비지도 학습 기법의 범주인 클러스터 분석을 탐구
* 군집화의 목표는 데이터에서 자연스러운 그룹화를 찾아 동일한 군집의 항목이 서로 다른 군집의 항목보다 서로 더 비슷하도록 하는 것임.

## 1. Grouping objects by similarity using k-means

#### k-means clustering using scikit-learn

* **k-평균(k-means) 알고리즘**은 구현하기 쉬우며 다른 클러스터링 알고리즘에 비해 계산이 매우 효율적임. 이는 프로토타입 기반 클러스터링 범주에 속함.
* 이 장 뒷부분에서 계층적 및 밀도 기반 클러스터링이라는 두 가지 다른 클러스터링 범주에 대해 설명함.
* 프로토타입 기반 군집화는 각 군집이 프로토타입으로 표현되는 것을 의미하며, 일반적으로 연속 형상을 가진 유사한 점의 중심(평균)이거나 범주형 형상의 경우 메이드 (가장 대표적인 점 또는 특정 군집에 속하는 다른 모든 점까지의 거리를 최소화하는 점)임.
* k-평균은 군집을 식별하는데 매우 뛰어나지만 군집화 알고리즘은 군집 k, a priori의 수를 지정해야 한다는 단점이 있음.(k를 잘못 선택하면 클러스터링 성능이 저하될 수 있음.)

* 시각화를 위해 다음의 간단한 2차원 데이터 세트를 사용하여 다음 예제를 살펴봄.

```python
>>> from sklearn.datasets import make_blobs
>>> X, y = make_blobs(n_samples=150,
... n_features=2,
... centers=3,
... cluster_std=0.5,
... shuffle=True,
... random_state=0)
>>> import matplotlib.pyplot as plt
>>> plt.scatter(X[:, 0],
... X[:, 1],
... c='white',
... marker='o',
... edgecolor='black',
... s=50)
>>> plt.xlabel('Feature 1')
>>> plt.ylabel('Feature 2')
>>> plt.grid()
>>> plt.tight_layout()
>>> plt.show()
```


* 방금 생성한 데이터 세트는 밀도가 높은 세 개의 영역으로 대략 그룹화된 무작위로 생성된 150개의 점으로 구성되어 있으며, 이는 2차원 산점도를 통해 시각화됨.

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.1. A scatterplot of our unlabeled dataset
</figcaption>


* 목표는 다음 네 단계로 요약된 k-평균 알고리듬을 사용하여 달성할 수 있는 특징 유사성을 기반으로 예제를 그룹화하는 것임.
* 1) 예제에서 $k$개의 중심체를 초기 군집 중심으로 랜덤하게 선택
* 2) 각 예제를 가장 가까운 중심점에 할당함. \mu^{j}, j\in \{1,...,k\}
* 3) 중심선을 지정된 예제 가운데로 이동함.
* 4) 클러스터 할당이 변경되지 않거나 사용자 정의 허용 오차 또는 최대 반복 횟수에 도달할 때까지 2단계와 3단계를 반복함.

* 물체들 사이의 유사석 측정은 m차원 공간에서 x와 y의 두 점 사이의 제곱 유클리드 거리로 계산함.

$d(x, y)^{2} = \sum_{j=1}^{m}(x_{j}-y_{j})^2=\begin{Vmatrix}x-y\end{Vmatrix}_{2}^{2}$


* 여기서, 인덱스 j는 예제 입력 x 및 y의 j번째 차원(특성 열)을 나타냄.
* 이 섹션의 나머지 부분에서는 위첨자 i와 j를 사용하여 각각 예제의 인덱스(데이터 레코드)와 클러스터 인덱스를 참조함.
* 유클리드 거리 메트릭을 기반으로, 우리는 k-평균 알고리즘을 클러스터 내 제곱 오차 합(SSE)을 최소화하기 위한 반복적인 접근 방식인 단순한 최적화 문제로 설명하며 이를 클러스터 관성이라함. 
* 이전 방정식에서 인덱스 j는 예제 입력 x 및 y의 j번째 차원(특징 열)을 나타냅니다. 

$SSE = \sum_{i=1}^{n}\sum_{j=1}^{k}w^{(i,j)}=\begin{Vmatrix}x^{(i)}-\mu^{(j)}\end{Vmatrix}_{2}^{2}$

* 여기서, $\mu^{(j)}$는 $x(i)$가 군집 $j$에 있으면 군집 $j. w(i, j) = 1$을 나타내는 점(중심점)이 되고, 그렇지 않으면 $0$이 됨.
* 이제 skickit-learn의 클러스터 모듈의 KMeans 클래스를 사용하여 예제 데이터 세트에 적용


```python
>>> from sklearn.cluster import KMeans
>>> km = KMeans(n_clusters=3,
... init='random',
... n_init=10,
... max_iter=300,
... tol=1e-04,
... random_state=0)
>>> y_km = km.fit_predict(X)
```

* 이제 k-평균이 데이터 세트에서 식별된 클러스터를 클러스터 중심과 함께 시각화함.
* 이 속성은 적합된 KMeans 개체의 cluster_centers_ 속성 아래에 저장됨.

```python
>>> plt.scatter(X[y_km == 0, 0],
... X[y_km == 0, 1],
... s=50, c='lightgreen',
... marker='s', edgecolor='black',
... label='Cluster 1')
>>> plt.scatter(X[y_km == 1, 0],
... X[y_km == 1, 1],
... s=50, c='orange',
... marker='o', edgecolor='black',
... label='Cluster 2')
>>> plt.scatter(X[y_km == 2, 0],
... X[y_km == 2, 1],
... s=50, c='lightblue',
... marker='v', edgecolor='black',
... label='Cluster 3')
>>> plt.scatter(km.cluster_centers_[:, 0],
... km.cluster_centers_[:, 1],
... s=250, marker='*',
... c='red', edgecolor='black',
... label='Centroids')
>>> plt.xlabel('Feature 1')
>>> plt.ylabel('Feature 2')
>>> plt.legend(scatterpoints=1)
>>> plt.grid()
>>> plt.tight_layout()
>>> plt.show()
```

* 그림에서 k-평균이 각 구의 중심에 세 개의 중심체를 배치한 것을 볼 수 있으며, 이 데이터 세트를 고려할 때 합리적인 그룹처럼 판단됨.


<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.2. The k-means clusters and their centroids
</figcaption>


* 여전히 클러스터 k, a priori의 수를 지정해야 하는 단점이 있음. 특히 시각화할 수 없는 고차원 데이터 세트를 사용하는 경우 실제 애플리케이션에서 선택할 클러스터 수가 항상 명확하지는 않을 수 있음.
* k-평균의 다른 특성은 군집이 겹치지 않고 계층적이지 않다는 것이며, 각 군집에 적어도 하나의 항목이 있다고 가정함.


#### A smarter way of placing the initial cluster centroids using k-means++

* 초기 중심체를 잘못 선택하면 클러스터가 잘 동작하지않거나 수렴이 느려질 수 있는 랜덤 시드를 사용하여 초기 중심체를 배치하는 고전 방식의 k-평균 알고리즘에 대해 논의함. 이 문제를 해결하기 위해 데이터 세트에서 k-평균 알고리즘을 여러번 실행하고 SSE 측면에서 가장 좋은 성능 모델을 선택하는 것임.
* 또 다른 전략은 k-평균++ 알고리즘을 통해 초기 중심체를 서로 멀리 배치하는 것인데, 이는 고전적인 k-평균보다 더 좋은 결과가 나타나며 일관된 결과를 가져옴.

## 2. Hard versus soft clustering

* 하드 클러스터링(hard clustering)은 앞부분에서 논의한 k-평균 및 k-평균++ 알고리즘에서와 같이 데이터 세트의 각 예제가 정확히 하나의 클러스터에 할당되는 알고리즘 제품군을 의미함.
* 대조적으로, 소프트 클러스터링(soft clustering)(퍼지 클러스터링(fuzzy clustering)이라고도 함)을 위한 알고리즘은 하나 이상의 클러스터에 예를 할당함.
* 소프트 클러스터링의 일반적인 예는 퍼지 C-평균(FCM,fuzzy C-means) 알고리즘(소프트 k-평균 또는 퍼지 k-평균이라고도 함)임. 
* FCM 절차는 k-평균과 매우 유사함. 그러나 하드 클러스터 할당을 각 클러스터에 속하는 각 포인트에 대한 확률로 대체함. 

* 여기서 각 값은 $[0, 1]$ 범위에 속하며 각 군집 중심선의 구성원 자격을 나타냄.
* 주어진 예제의 구성원 자격의 합은 1임. k-평균 알고리즘과 마찬가지로 FCM 알고리즘은 다음과 같이 요약됨.
* 1) $k$개의 중심 수를 지정하고 각 점에 대해 랜덤하게 군집 구성원 자격을 할당
* 2) 클러스터 중심부를 계산, $\mu^{(j)}, j\in\{1,...,k\} $
* 3) 각 지점의 클러스터 구성원 자격을 업데이트함.
* 4) 멤버십 계수가 변경되지 않거나 사용자 정의 공차 또는 최대 반복 횟수에 도달할 때까지 2단계와 3단계를 반복됨. 

* FCM의 목표 함수(약칭 Jm)는 k-평균을 최소화하는 클러스터 내 SSE와 매우 유사함. 

$J_{m} = \sum_{i=1}^{n}\sum_{j=1}^{k}w^{(i,j)^{m}}=\begin{Vmatrix}x^{(i)}-\mu^{(j)}\end{Vmatrix}_{2}^{2}$

* $m$ 값이 클수록 군집 구성원 $w(i, j)$가 작아져 군집이 더 흐릿해짐. 클러스터 구성원 자격 확률 자체는 다음과 같이 계산됨. 

**** fig 3 *****

* 예를 들어, 앞의 k-평균 예시와 같이 세 개의 군집 중심을 선택했다면, 다음과 같이 군집 $\mu^{(j)}$ 에 속하는 $x^{(i)}$의 멤버쉽을 계산됨.

**** fig 4 *****

**** fig 5 *****


* 군집 자체의 중심 $\mu^{(j)}$는 각 예제가 해당 군집에 속하는 정도에 따라 가중된 모든 예제의 평균으로 계산됨$(w^{(i,j)^{m}})$.

#### Using the elbow method to find the optimal number of clusters

* 비지도 학습의 주요 과제 중 하나는 결정적인(definitive) 답을 알 수 없다는 것임.
* 데이터 세트에는 지도 모델의 성능을 평가하기 위해 6장 "모델 평가 및 하이퍼파라미터 조정을 위한 모범 사례 학습"에서 사용한 기술을 적용할 수 있는 실측 자료 클래스 레이블이 없음.
* 따라서 클러스터링 품질을 정량화하려면 클러스터 내 SSE(왜곡)와 같은 고유한 메트릭을 사용하여 다양한 k-평균 클러스터링 모델의 성능을 비교해야함.
* KMeans 모델을 장착한 후 이미 **inertia_** 속성을 통해 액세스할 수 있으므로 skickit-learn을 사용할 때 클러스터 내 SSE를 명시적으로 계산할 필요가 없음. 


```python
>>> print(f'Distortion: {km.inertia_:.2f}')
Distortion: 72.48
```

* 클러스터 내 SSE를 기반으로, 우리는 주어진 작업에 대한 최적의 클러스터 수 k를 추정하기 위해 소위 엘보(elbow) 방법이라는 그래픽 도구를 사용할 수 있음.
* k가 증가하면 왜곡이 감소한다고 말할 수 있음. 이는 예제가 할당된 중심선에 더 가깝기 때문 엘보 방법의 이면에 있는 아이디어는 왜곡이 가장 빠르게 증가하기 시작하는 k의 값을 식별하는 것임. k의 다른 값에 대한 왜곡을 플롯하면 더 명확해질 것

```python
>>> distortions = []
>>> for i in range(1, 11):
... km = KMeans(n_clusters=i,
... init='k-means++',
... n_init=10,
... max_iter=300,
... random_state=0)
... km.fit(X)
... distortions.append(km.inertia_)
>>> plt.plot(range(1,11), distortions, marker='o')
>>> plt.xlabel('Number of clusters')
>>> plt.ylabel('Distortion')
>>> plt.tight_layout()
>>> plt.show()
```


* 아래 그림과 같이 엘보는 k = 3에 위치하므로, 이는 k = 3이 실제로 이 데이터 세트에 적합한 선택이라는 것을 뒷받침하는 증거임.

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.3. Finding the optimal number of clusters using the elbow method
</figcaption>


## 3. Quantifying the quality of clustering via silhouette plots

* 클러스터링 품질을 평가하기 위한 또 다른 메트릭은 실루엣 분석(silhouette analysis)이며, 이는 이 장의 뒷부분에서 논의될 k-평균 이외의 클러스터링 알고리즘에도 적용될 수 있음.
* 실루엣 분석을 사용하여 군집의 예제의 측도를 표시할 수 있음. 데이터 세트에서 단일 예제의 실루엣 계수(silhouette coefficient)를 계산하기 위해 다음 세 단계를 적용할 수 있음.
* 1) 군집 응집력 $a^{(i)}$를 예제 $x^{(i)}$와 동일한 군집의 다른 모든 점 사이의 평균 거리로 계산
* 2) 예제 $x^{(i)}$와 가장 가까운 군집의 모든 예제 사이의 평균 거리로 다음 가장 가까운 군집으로부터의 군집 분리 $b^{(i)}$를 계산
* 3) 아래 표시된 것처럼 클러스터 응집력과 분리 사이의 차이를 둘 중 큰 값으로 나눈 실루엣 $s^{(i)}$를 계산

$S^{(i)}=\frac{b^{(i)}-a^{(i)}}{max\{b^{(i)},a^{(i)}\}}$

* 실루엣 계수는 –1에서 1까지의 범위로 제한됨. 
* 앞의 방정식을 바탕으로 군집 분리와 응집력이 같으면 실루엣 계수가 0임을 알 수 있음. $(b^{(i)} = a^{(i)})$ 또한, $b^{(i)}$는 예제가 다른 클러스터와 얼마나 다른지 정량화하고 $a^{(i)}$는 자체 클러스터의 다른 예제와 얼마나 유사한지 알려주기 때문에, $b^{(i)}>>a^{(i)}$ 일 경우 이상적인 실루엣 계수 1에 근접
* 실루엣 계수는 skickit-learn's metric module에서 실루엣_sample로 사용할 수 있으며, 필요에 따라 편의를 위해 실루엣_scores 함수를 사용함.
* 실루엣_scores 함수는 모든 예제의 평균 실루엣 계수를 계산함. 이는 numpy. mean(실루엣_samples(...))과 같으며, 다음 코드를 실행하여 k = 3인 k-분산 군집화에 대한 실루엣 계수의 그림을 만듦.

```python
>>> km = KMeans(n_clusters=3,
... init='k-means++',
... n_init=10,
... max_iter=300,
... tol=1e-04,
... random_state=0)
>>> y_km = km.fit_predict(X)
>>> import numpy as np
>>> from matplotlib import cm
>>> from sklearn.metrics import silhouette_samples
>>> cluster_labels = np.unique(y_km)
>>> n_clusters = cluster_labels.shape[0]
>>> silhouette_vals = silhouette_samples(
... X, y_km, metric='euclidean'
... )
>>> y_ax_lower, y_ax_upper = 0, 0
>>> yticks = []
>>> for i, c in enumerate(cluster_labels):
... c_silhouette_vals = silhouette_vals[y_km == c]
... c_silhouette_vals.sort()
... y_ax_upper += len(c_silhouette_vals)
... color = cm.jet(float(i) / n_clusters)
... plt.barh(range(y_ax_lower, y_ax_upper),
... c_silhouette_vals,
... height=1.0,
... edgecolor='none',
... color=color)
... yticks.append((y_ax_lower + y_ax_upper) / 2.)
... y_ax_lower += len(c_silhouette_vals)
>>> silhouette_avg = np.mean(silhouette_vals)
>>> plt.axvline(silhouette_avg,
... color="red",
... linestyle="--")
>>> plt.yticks(yticks, cluster_labels + 1)
>>> plt.ylabel('Cluster')
>>> plt.xlabel('Silhouette coefficient')
>>> plt.tight_layout()
>>> plt.show()
```

* 실루엣 그림에 대한 육안 검사를 통해 여러 군집의 크기를 신속하게 면밀히 조사하고 특이치를 포함하는 군집을 식별할 수 있음.

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.4. A silhouette plot for a good example of clustering
</figcaption>

* 그러나 이전 실루엣 그림에서 볼 수 있듯이 실루엣 계수는 0에 가깝지 않으며 평균 실루엣 점수(이 경우 좋은 군집화를 나타내는 지표)에서 거의 동일하게 멀리 떨어져 있음. 또한 클러스터링의 장점을 요약하기 위해 평균 실루엣 계수를 그림(점선)에 추가하였음.

* 상대적으로 불량한 군집화에 대한 실루엣 그림이 어떻게 보이는지 보려면 두 개의 중심선만 사용하여 k-평균 알고리즘을 시드함. 

```python
>>> km = KMeans(n_clusters=2,
... init='k-means++',
... n_init=10,
... max_iter=300,
... tol=1e-04,
... random_state=0)
>>> y_km = km.fit_predict(X)
>>> plt.scatter(X[y_km == 0, 0],
... X[y_km == 0, 1],
... s=50, c='lightgreen',
... edgecolor='black',
... marker='s',
... label='Cluster 1')
>>> plt.scatter(X[y_km == 1, 0],
... X[y_km == 1, 1],
... s=50,
... c='orange',
... edgecolor='black',
... marker='o',
... label='Cluster 2')
>>> plt.scatter(km.cluster_centers_[:, 0],
... km.cluster_centers_[:, 1],
... s=250,
... marker='*',
... c='red',
... label='Centroids')
>>> plt.xlabel('Feature 1')
>>> plt.ylabel('Feature 2')
>>> plt.legend()
>>> plt.grid()
>>> plt.tight_layout()
>>> plt.show()
```


* 아래 그림과 같이 중심선 중 하나는 세 개의 구형 그룹 중 두 개 사이에 있음.
* 입력 데이터의 군집화가 완전히 엉망으로 보이지는 않지만, 차선의 군집화임.

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.5. A suboptimal example of clustering
</figcaption>

* 일반적으로 우리는 더 높은 차원의 데이터로 작업하기 때문에 실제 문제의 2차원 산점도에서 데이터 세트를 시각화할 여유가 없음. 다음으로는 결과를 평가하기 위한 실루엣 그림을 플롯함. 

```python
>>> cluster_labels = np.unique(y_km)
>>> n_clusters = cluster_labels.shape[0]
>>> silhouette_vals = silhouette_samples(
... X, y_km, metric='euclidean'
... )
>>> y_ax_lower, y_ax_upper = 0, 0
>>> yticks = []
>>> for i, c in enumerate(cluster_labels):
... c_silhouette_vals = silhouette_vals[y_km == c]
... c_silhouette_vals.sort()
... y_ax_upper += len(c_silhouette_vals)
... color = cm.jet(float(i) / n_clusters)
... plt.barh(range(y_ax_lower, y_ax_upper),
... c_silhouette_vals,
... height=1.0,
... edgecolor='none',
... color=color)
... yticks.append((y_ax_lower + y_ax_upper) / 2.)
... y_ax_lower += len(c_silhouette_vals)
>>> silhouette_avg = np.mean(silhouette_vals)
>>> plt.axvline(silhouette_avg, color="red", linestyle="--")
>>> plt.yticks(yticks, cluster_labels + 1)
>>> plt.ylabel('Cluster')
>>> plt.xlabel('Silhouette coefficient')
>>> plt.tight_layout()
>>> plt.show()
```

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.6. A silhouette plot for a suboptimal example of clustering
</figcaption>

* 위 그림에서 이제 실루엣의 길이와 폭이 눈에 띄게 다르며, 이는 상대적으로 좋지 않거나 최소한 차선의 군집화를 나타내는 증거임.
* 이제 클러스터링의 작동 방식을 잘 이해한 후 다음 섹션에서는 계층적 클러스터링을 대안적 접근 방식으로 소개함.


## 4. Organizing clusters as a hierarchical tree

* 이 섹션에서는 프로토타입 기반 클러스터링에 대한 대체 접근 방식인 계층형 클러스터링에 대해 살펴봄.
* 계층적 클러스터링 알고리듬의 한 가지 장점은 덴드로그램(이항 계층적 클러스터링의 시각화)을 표시할 수 있다는 것이며, 이는 의미 있는 분류법을 만들어 결과를 해석하는 데 도움이 될 수 있음. 이러한 계층적 접근 방식의 또 다른 장점은 클러스터 수를 미리 지정할 필요가 없다는 것을 의미함.
* 계층적 클러스터링에 대한 두 가지 주요 접근 방식은 응집적 및 분열적 계층 클러스터링(agglomerative and divisive)임. 분할 계층적 클러스터링에서는 전체 데이터 세트를 포함하는 하나의 클러스터부터 시작하여 각 클러스터가 하나의 예만 포함할 때까지 클러스터를 더 작은 클러스터로 반복적으로 분할함.
* 이 섹션에서는 이와 반대되는 접근 방식을 취하는 응집적 클러스터링에 초점을 맞출 것임. 각 예제를 개별 클러스터로 시작하여 하나의 클러스터만 남을 때까지 가장 가까운 클러스터 쌍을 병합함. 


#### Grouping clusters in a bottom-up fashion

* 응집적 계층적 클러스터링을 위한 두 가지 표준 알고리듬은 단일 연결과 완전한 연결임.
* 단일 연결을 사용하여 각 클러스터 쌍에 대해 가장 유사한 멤버 간의 거리를 계산하고 가장 유사한 멤버 간의 거리가 가장 작은 두 클러스터를 병합함.
* 전체 연결 접근법은 단일 연결과 유사하지만 각 클러스터 쌍에서 가장 유사한 구성원을 비교하는 대신 가장 다른 구성원을 비교하여 병합을 수행하며 이는 아래 그림에 나와있음. 


<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.7. The complete linkage approach
</figcaption>


* 이 섹션에서는 전체 연결 방식을 사용한 응집 클러스터링에 초점을 맞춤. 계층적 전체 연결 군집은 다음 단계로 요약할 수 있는 반복적인 절차를 의미함.
* 1) 모든 예제의 쌍별 거리 행렬을 계산
* 2) 각 데이터 점을 단일 톤 클러스터로 나타냄.
* 3) 가장 유사한(distant) 구성원 간의 거리를 기준으로 가장 가까운 두 군집을 병합함.
* 4) 클러스터 링크 매트릭스를 업데이트함.
* 5) 단일 클러스터 2-4단계를 반복

* 다음으로 거리 행렬을 계산하는 방법에 대해 설명함(1단계). 
* 먼저 사용할 랜덤 데이터 샘플을 생성하고 행은 서로 다른 관측치(ID 0-4)를 나타내고 열은 해당 예제의 서로 다른 특성(X, Y, Z)을 나타냄.

```python
>>> import pandas as pd
>>> import numpy as np
>>> np.random.seed(123)
>>> variables = ['X', 'Y', 'Z']
>>> labels = ['ID_0', 'ID_1', 'ID_2', 'ID_3', 'ID_4']
>>> X = np.random.random_sample([5, 3])*10
>>> df = pd.DataFrame(X, columns=variables, index=labels)
>>> df
```

* 이전 코드를 실행한 후 랜덤하게 생성된 예제가 포함된 다음 DataFrame이 표시됨.

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.8. A randomly generated data sample
</figcaption>

#### Performing hierarchical clustering on a distance matrix

* 계층적 클러스터링 알고리즘에 대한 입력으로 거리 행렬을 계산하기 위해 SciPy의 **spatial.distance** 하위 모듈의 **pdist** 함수를 사용


```python
>>> from scipy.spatial.distance import pdist, squareform
>>> row_dist = pd.DataFrame(squareform(
... pdist(df, metric='euclidean')),
... columns=labels, index=labels)
>>> row_dist
```

* 이전 코드를 사용하여, 우리는 특성 X, Y 및 Z를 기반으로 데이터 세트의 각 입력 예제 쌍 사이의 유클리드 거리를 계산하였음
* 쌍방향 거리의 대칭 행렬을 생성하기 위해 정사각형 함수에 입력으로 pdist에 의해 반환된 축약 거리 행렬을 제공했음.



<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.9. The calculated pair-wise distances of our data
</figcaption>


* 다음으로 SciPy의 클러스터의 연결 기능을 사용하여 전체 연결 집계를 클러스터에 적용함.

```python
>>> from scipy.cluster.hierarchy import linkage
>>> help(linkage)
[...]
Parameters:
 y : ndarray
 A condensed or redundant distance matrix. A condensed
 distance matrix is a flat array containing the upper
 triangular of the distance matrix. This is the form
 that pdist returns. Alternatively, a collection of m
 observation vectors in n dimensions may be passed as
 an m by n array.
 
 method : str, optional
 The linkage algorithm to use. See the Linkage Methods
 section below for full descriptions.
 
 metric : str, optional
 The distance metric to use. See the distance.pdist
 function for a list of valid distance metrics.
 
 Returns:
 Z : ndarray
 The hierarchical clustering encoded as a linkage matrix.
[...]
```

* 함수 설명에 기초하여, 우리는 pdist 함수의 응축된 거리 행렬(상단의 삼각형)을 입력 속성으로 사용할 수 있음.
* 또는 초기 데이터 배열을 제공하고 링크에서 함수 인수로 '유클리드' 메트릭을 사용할 수 있음. 
* 그러나 앞에서 정의한 사각형의 거리 행렬은 예상과 다른 거리 값을 산출하기 때문에 사용할 수 없음. 요약하면 다음과 같은 세 가지 시나리오가 존재함. 

* **Incorrect approach**: 다음 코드 조각에 표시된 것과 같이 사각형의 거리 행렬을 사용하면 잘못된 결과가 발생

```python
>>> row_clusters = linkage(row_dist,
... method='complete',
... metric='euclidean')
```

* **Correct approach**: 다음 코드 예시와 같이 축약된 거리 행렬을 사용하면 올바른 연결 행렬이 생성

```python
>>> row_clusters = linkage(pdist(df, metric='euclidean'),
... method='complete')
```

* **Correct approach**: 다음 코드 조각에 나와 있는 완전한 입력 예제 행렬(이른바 설계 행렬)을 사용하면 이전 접근 방식과 유사한 올바른 연결 행렬로 이어짐.

```python
>>> row_clusters = linkage(df.values,
... method='complete',
... metric='euclidean')
```

* 클러스터링 결과를 자세히 살펴보기 위해 다음과 같이 결과를 판다 데이터 프레임(주피터 노트북에서 가장 잘 볼 수 있음)으로 변환할 수 있음.


```python
>>> pd.DataFrame(row_clusters,
... columns=['row label 1',
... 'row label 2',
... 'distance',
... 'no. of items in clust.'],
... index=[f'cluster {(i + 1)}' for i in
... range(row_clusters.shape[0])])
```


* 어래 그림 같이 연결 매트릭스는 각 행이 하나의 병합을 나타내는 여러 행으로 구성됨. 첫 번째 열과 두 번째 열은 각 클러스터에서 가장 서로 다른 멤버를 나타내며 세 번째 열은 해당 멤버 간의 거리를 보고함.
* 마지막 열은 각 클러스터의 멤버 수를 반환: 

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.10. The linkage matrix
</figcaption>


* 이제 연결 행렬을 계산했으므로 덴드로그램의 형태로 결과를 시각화할 수 있음.

```python
>>> from scipy.cluster.hierarchy import dendrogram
>>> # make dendrogram black (part 1/2)
>>> # from scipy.cluster.hierarchy import set_link_color_palette
>>> # set_link_color_palette(['black'])
>>> row_dendr = dendrogram(
... row_clusters,
... labels=labels,
... # make dendrogram black (part 2/2)
... # color_threshold=np.inf
... )
>>> plt.tight_layout()
>>> plt.ylabel('Euclidean distance')
>>> plt.show()
```

* 색상표는 덴드로그램의 거리 임계값에 대해 순환되는 Matplotlib 색상 목록에서 파생됨. 
* 예를 들어 덴드로그램을 검은색으로 표시하려면 이전 코드에 삽입된 각 섹션의 주석을 해제할 수 있음. 

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.11. A dendrogram of our data
</figcaption>

* 이러한 덴드로그램은 응집적 계층적 클러스터링 동안 형성된 여러 클러스터를 요약함.
* 예를 들어, ID_0과 ID_4와 ID_1과 ID_2가 유클리드 거리 메트릭을 기반으로 가장 유사한 예임을 알 수 있음.

## 5. Attaching dendrograms to a heat map

* 실제 응용 프로그램에서 계층적 클러스터링 덴드로그램은 종종 열 지도와 함께 사용되며, 이를 통해 훈련 예를 포함하는 데이터 배열 또는 행렬의 개별 값을 색상 코드로 나타낼 수 있음. 
* 이 섹션에서는 덴드로그램을 열 지도 그림에 부착하고 열 지도에 있는 행을 그에 따라 정렬하는 방법에 대해 설명함.
* 다만 덴드로그램을 히트맵에 부착하는 것은 조금 까다로울 수 있으므로 다음 절차를 단계별로 진행함.

* 1) 새로운 도형 객체를 생성하고 add_axes 속성을 통해 덴드로그램의 x축 위치, y축 위치, 너비 및 높이를 정의함. 또한 덴드로그램을 시계 반대 방향으로 90도 회전시킵니다. 코드는 다음과 같음.

```python
>>> fig = plt.figure(figsize=(8, 8), facecolor='white')
>>> axd = fig.add_axes([0.09, 0.1, 0.2, 0.6])
>>> row_dendr = dendrogram(row_clusters,
... orientation='left')
>>> # note: for matplotlib < v1.5.1, please use
>>> # orientation='right'
```

* 2) 다음으로, 덴드로그램 개체에서 액세스할 수 있는 클러스터링 레이블에 따라 초기 데이터 프레임의 데이터를 재정렬합니다. 코드는 다음과 같음.

```python
>>> df_rowclust = df.iloc[row_dendr['leaves'][::-1]]
```

* 3) 이제 정렬된 DataFrame에서 열 지도를 작성하고 덴드로그램 옆에 배치함.

```python
>>> axm = fig.add_axes([0.23, 0.1, 0.6, 0.6])
>>> cax = axm.matshow(df_rowclust,
... interpolation='nearest',
... cmap='hot_r')
```

* 4) 마지막으로, 우리는 축 눈금을 제거하고 축 가시를 숨김으로써 덴드로그램의 미학을 수정 또한 색상 막대를 추가하고 형상 및 데이터 레코드 이름을 각각 x축 눈금 레이블과 y축 눈금 레이블에 할당함. 

```python
>>> axd.set_xticks([])
>>> axd.set_yticks([])
>>> for i in axd.spines.values():
... i.set_visible(False)
>>> fig.colorbar(cax)
>>> axm.set_xticklabels([''] + list(df_rowclust.columns))
>>> axm.set_yticklabels([''] + list(df_rowclust.index))
>>> plt.show()
```

* 이전 단계를 수행한 후에는 덴드로그램이 부착된 상태로 열 지도가 표시되어야함.


<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.12. A heat map and dendrogram of our data
</figcaption>

* 보시다시피 열 지도에서 행의 순서는 덴드로그램의 예제 군집을 반영함. 간단한 덴드로그램 외에도 히트 맵에 있는 각 예제와 기능의 색상 코드 값은 데이터 세트의 멋진 요약을 제공함.

#### Applying agglomerative clustering via scikit-learn

* 이전 부분에서는 SciPy를 사용하여 응집적 계층적 클러스터링을 수행하는 방법을 살펴보았습니다.
* 그러나 sickit-learn에는 반환할 클러스터 수를 선택할 수 있는 Aggregative Clustering 구현 방법이 존재함.
* 이 기능은 계층적 클러스터 트리를 정리하려는 경우에 유용합니다. n_cluster 매개 변수를 3으로 설정함으로써, 우리는 이제 이전과 같은 유클리드 거리 메트릭을 기반으로 하는 완전한 연결 접근법을 사용하여 입력 예제를 세 그룹으로 클러스터링할 것임.

```python
>>> from sklearn.cluster import AgglomerativeClustering
>>> ac = AgglomerativeClustering(n_clusters=3,
... affinity='euclidean',
... linkage='complete')
>>> labels = ac.fit_predict(X)
>>> print(f'Cluster labels: {labels}')
Cluster labels: [1 0 0 2 1]
```

```python
>>> ac = AgglomerativeClustering(n_clusters=2,
... affinity='euclidean',
... linkage='complete')
>>> labels = ac.fit_predict(X)
>>> print(f'Cluster labels: {labels}')
Cluster labels: [0 1 1 0 0]
```

## 6. Locating regions of high density via DBSCAN

* 이 장에서는 다양한 클러스터링 알고리즘을 다룰 수는 없지만, 클러스터링에 대한 한 가지 접근 방식을 더 포함함. 
* 즉, k-평균과 같은 구형 클러스터에 대한 가정을 하지 않고 데이터 세트를 필요한 계층으로 분할하지 않는 DBSCAN(Density-Based Spatial Clustering with Noise)을 포함함.
* 수동 컷오프 지점을 읽음. 이름에서 알 수 있듯이 밀도 기반 클러스터링은 점들의 밀집된 영역을 기반으로 클러스터 레이블을 할당함. DBSCAN에서 밀도의 개념은 지정된 반지름 내 점수 $\varepsilon $로 정의함.
* DBSCAN 알고리즘에 따라 다음 기준을 사용하여 각 예(데이터 포인트)에 특수 레이블이 지정됨.
* DBSCAN의 결과를 보다 잘 이해하기 위해 구현으로 이동하기 전에 방금 배운 핵심 지점, 경계 지점 및 노이즈 지점에 대한 내용을 아래 그림에 요약함.

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.13. Core, noise, and border points for DBSCAN
</figcaption>



* 보다 예시적인 예를 위해 k-평균 클러스터링, 계층 클러스터링 및 DBSCAN을 비교할 수 있는 반달 모양의 구조 데이터 세트를 새로 생성: 
```python
>>> from sklearn.datasets import make_moons
>>> X, y = make_moons(n_samples=200,
... noise=0.05,
... random_state=0)
>>> plt.scatter(X[:, 0], X[:, 1])
>>> plt.xlabel('Feature 1')
>>> plt.ylabel('Feature 2')
>>> plt.tight_layout()
>>> plt.show()
```

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.14. A two-feature half-moon-shaped dataset
</figcaption>



<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.15. A two-feature half-moon-shaped dataset
</figcaption>

* 가시화된 클러스터링 결과를 바탕으로 k-평균 알고리즘이 두 클러스터를 분리할 수 없었음을 알 수 있으며, 또한 계층적 클러스터링 알고리즘은 이러한 복잡한 형태에 의해 도전함.


* DBSCAN 알고리즘은 DBSCAN의 장점 중 하나인 임의 모양의 클러스터링 데이터를 강조하는 반달 모양을 성공적으로 감지할 수 있음.

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.16. DBSCAN clustering on the half-moon-shaped dataset
</figcaption>




