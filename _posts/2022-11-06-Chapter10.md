---
layout: single
title:  "Chapter10. Working with Unlabeled Data – Clustering Analysis"
use_math: true
---

* 이전 챕터에서는 지도 학습 기술을 사용하여 이미 답이 알려진 데이터를 사용하여 기계 학습 모델을 구축했음.
* 이 챕터에서는 기어를 바꾸고 우리가 정답을 미리 알지 못하는 데이터의 숨겨진 구조를 발견할 수 있는 비지도 학습 기법의 범주인 클러스터 분석을 탐구
* 군집화의 목표는 데이터에서 자연스러운 그룹화를 찾아 동일한 군집의 항목이 서로 다른 군집의 항목보다 서로 더 비슷하도록 하는 것임.

## 1. Grouping objects by similarity using k-means

#### k-means clustering using scikit-learn

* **k-평균(k-means) 알고리즘**은 구현하기 쉬우며 다른 클러스터링 알고리즘에 비해 계산이 매우 효율적임. 이는 프로토타입 기반 클러스터링 범주에 속함.
* 이 장 뒷부분에서 계층적 및 밀도 기반 클러스터링이라는 두 가지 다른 클러스터링 범주에 대해 설명함.
* 프로토타입 기반 군집화는 각 군집이 프로토타입으로 표현되는 것을 의미하며, 일반적으로 연속 형상을 가진 유사한 점의 중심(평균)이거나 범주형 형상의 경우 메이드 (가장 대표적인 점 또는 특정 군집에 속하는 다른 모든 점까지의 거리를 최소화하는 점)임.
* k-평균은 군집을 식별하는데 매우 뛰어나지만 군집화 알고리즘은 군집 k, a priori의 수를 지정해야 한다는 단점이 있음.(k를 잘못 선택하면 클러스터링 성능이 저하될 수 있음.)

* 시각화를 위해 다음의 간단한 2차원 데이터 세트를 사용하여 다음 예제를 살펴봄.

```python
>>> from sklearn.datasets import make_blobs
>>> X, y = make_blobs(n_samples=150,
... n_features=2,
... centers=3,
... cluster_std=0.5,
... shuffle=True,
... random_state=0)
>>> import matplotlib.pyplot as plt
>>> plt.scatter(X[:, 0],
... X[:, 1],
... c='white',
... marker='o',
... edgecolor='black',
... s=50)
>>> plt.xlabel('Feature 1')
>>> plt.ylabel('Feature 2')
>>> plt.grid()
>>> plt.tight_layout()
>>> plt.show()
```


* 방금 생성한 데이터 세트는 밀도가 높은 세 개의 영역으로 대략 그룹화된 무작위로 생성된 150개의 점으로 구성되어 있으며, 이는 2차원 산점도를 통해 시각화됨.

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.1. A scatterplot of our unlabeled dataset
</figcaption>


* 목표는 다음 네 단계로 요약된 k-평균 알고리듬을 사용하여 달성할 수 있는 특징 유사성을 기반으로 예제를 그룹화하는 것임.
* 1) 예제에서 $k$개의 중심체를 초기 군집 중심으로 랜덤하게 선택
* 2) 각 예제를 가장 가까운 중심점에 할당함. \mu^{j}, j\in \{1,...,k\}
* 3) 중심선을 지정된 예제 가운데로 이동함.
* 4) 클러스터 할당이 변경되지 않거나 사용자 정의 허용 오차 또는 최대 반복 횟수에 도달할 때까지 2단계와 3단계를 반복함.

* 물체들 사이의 유사석 측정은 m차원 공간에서 x와 y의 두 점 사이의 제곱 유클리드 거리로 계산함.

$d(x, y)^{2} = \sum_{j=1}^{m}(x_{j}-y_{j})^2=\begin{Vmatrix}x-y\end{Vmatrix}_{2}^{2}$


* 여기서, 인덱스 j는 예제 입력 x 및 y의 j번째 차원(특성 열)을 나타냄.
* 이 섹션의 나머지 부분에서는 위첨자 i와 j를 사용하여 각각 예제의 인덱스(데이터 레코드)와 클러스터 인덱스를 참조함.
* 유클리드 거리 메트릭을 기반으로, 우리는 k-평균 알고리즘을 클러스터 내 제곱 오차 합(SSE)을 최소화하기 위한 반복적인 접근 방식인 단순한 최적화 문제로 설명하며 이를 클러스터 관성이라함. 
* 이전 방정식에서 인덱스 j는 예제 입력 x 및 y의 j번째 차원(특징 열)을 나타냅니다. 

$SSE = \sum_{i=1}^{n}\sum_{j=1}^{k}w^{(i,j)}=\begin{Vmatrix}x^{(i)}-\mu^{(j)}\end{Vmatrix}_{2}^{2}$

* 여기서, $\mu^{(j)}$는 $x(i)$가 군집 $j$에 있으면 군집 $j. w(i, j) = 1$을 나타내는 점(중심점)이 되고, 그렇지 않으면 $0$이 됨.
* 이제 skickit-learn의 클러스터 모듈의 KMeans 클래스를 사용하여 예제 데이터 세트에 적용


```python
>>> from sklearn.cluster import KMeans
>>> km = KMeans(n_clusters=3,
... init='random',
... n_init=10,
... max_iter=300,
... tol=1e-04,
... random_state=0)
>>> y_km = km.fit_predict(X)
```

* 이제 k-평균이 데이터 세트에서 식별된 클러스터를 클러스터 중심과 함께 시각화함.
* 이 속성은 적합된 KMeans 개체의 cluster_centers_ 속성 아래에 저장됨.

```python
>>> plt.scatter(X[y_km == 0, 0],
... X[y_km == 0, 1],
... s=50, c='lightgreen',
... marker='s', edgecolor='black',
... label='Cluster 1')
>>> plt.scatter(X[y_km == 1, 0],
... X[y_km == 1, 1],
... s=50, c='orange',
... marker='o', edgecolor='black',
... label='Cluster 2')
>>> plt.scatter(X[y_km == 2, 0],
... X[y_km == 2, 1],
... s=50, c='lightblue',
... marker='v', edgecolor='black',
... label='Cluster 3')
>>> plt.scatter(km.cluster_centers_[:, 0],
... km.cluster_centers_[:, 1],
... s=250, marker='*',
... c='red', edgecolor='black',
... label='Centroids')
>>> plt.xlabel('Feature 1')
>>> plt.ylabel('Feature 2')
>>> plt.legend(scatterpoints=1)
>>> plt.grid()
>>> plt.tight_layout()
>>> plt.show()
```

* 그림에서 k-평균이 각 구의 중심에 세 개의 중심체를 배치한 것을 볼 수 있으며, 이 데이터 세트를 고려할 때 합리적인 그룹처럼 판단됨.


<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.2. The k-means clusters and their centroids
</figcaption>


* 여전히 클러스터 k, a priori의 수를 지정해야 하는 단점이 있음. 특히 시각화할 수 없는 고차원 데이터 세트를 사용하는 경우 실제 애플리케이션에서 선택할 클러스터 수가 항상 명확하지는 않을 수 있음.
* k-평균의 다른 특성은 군집이 겹치지 않고 계층적이지 않다는 것이며, 각 군집에 적어도 하나의 항목이 있다고 가정함.


#### A smarter way of placing the initial cluster centroids using k-means++

* 초기 중심체를 잘못 선택하면 클러스터가 잘 동작하지않거나 수렴이 느려질 수 있는 랜덤 시드를 사용하여 초기 중심체를 배치하는 고전 방식의 k-평균 알고리즘에 대해 논의함. 이 문제를 해결하기 위해 데이터 세트에서 k-평균 알고리즘을 여러번 실행하고 SSE 측면에서 가장 좋은 성능 모델을 선택하는 것임.
* 또 다른 전략은 k-평균++ 알고리즘을 통해 초기 중심체를 서로 멀리 배치하는 것인데, 이는 고전적인 k-평균보다 더 좋은 결과가 나타나며 일관된 결과를 가져옴.

## 2. Hard versus soft clustering

* 하드 클러스터링(hard clustering)은 앞부분에서 논의한 k-평균 및 k-평균++ 알고리즘에서와 같이 데이터 세트의 각 예제가 정확히 하나의 클러스터에 할당되는 알고리즘 제품군을 의미함.
* 대조적으로, 소프트 클러스터링(soft clustering)(퍼지 클러스터링(fuzzy clustering)이라고도 함)을 위한 알고리즘은 하나 이상의 클러스터에 예를 할당함.
* 소프트 클러스터링의 일반적인 예는 퍼지 C-평균(FCM,fuzzy C-means) 알고리즘(소프트 k-평균 또는 퍼지 k-평균이라고도 함)임. 
* FCM 절차는 k-평균과 매우 유사함. 그러나 하드 클러스터 할당을 각 클러스터에 속하는 각 포인트에 대한 확률로 대체함. 

* 여기서 각 값은 $[0, 1]$ 범위에 속하며 각 군집 중심선의 구성원 자격을 나타냄.
* 주어진 예제의 구성원 자격의 합은 1임. k-평균 알고리즘과 마찬가지로 FCM 알고리즘은 다음과 같이 요약됨.
* 1) $k$개의 중심 수를 지정하고 각 점에 대해 랜덤하게 군집 구성원 자격을 할당
* 2) 클러스터 중심부를 계산, $\mu^{(j)}, j\in\{1,...,k\} $
* 3) 각 지점의 클러스터 구성원 자격을 업데이트함.
* 4) 멤버십 계수가 변경되지 않거나 사용자 정의 공차 또는 최대 반복 횟수에 도달할 때까지 2단계와 3단계를 반복됨. 

* FCM의 목표 함수(약칭 Jm)는 k-평균을 최소화하는 클러스터 내 SSE와 매우 유사함. 

$J_{m} = \sum_{i=1}^{n}\sum_{j=1}^{k}w^{(i,j)^{m}}=\begin{Vmatrix}x^{(i)}-\mu^{(j)}\end{Vmatrix}_{2}^{2}$

* $m$ 값이 클수록 군집 구성원 $w(i, j)$가 작아져 군집이 더 흐릿해짐. 클러스터 구성원 자격 확률 자체는 다음과 같이 계산됨. 

**** fig 3 *****

* 예를 들어, 앞의 k-평균 예시와 같이 세 개의 군집 중심을 선택했다면, 다음과 같이 군집 $\mu^{(j)}$ 에 속하는 $x^{(i)}$의 멤버쉽을 계산됨.

**** fig 4 *****

**** fig 5 *****


* 군집 자체의 중심 $\mu^{(j)}$는 각 예제가 해당 군집에 속하는 정도에 따라 가중된 모든 예제의 평균으로 계산됨$(w^{(i,j)^{m}})$.

#### Using the elbow method to find the optimal number of clusters

* 비지도 학습의 주요 과제 중 하나는 결정적인(definitive) 답을 알 수 없다는 것임.
* 데이터 세트에는 지도 모델의 성능을 평가하기 위해 6장 "모델 평가 및 하이퍼파라미터 조정을 위한 모범 사례 학습"에서 사용한 기술을 적용할 수 있는 실측 자료 클래스 레이블이 없음.
* 따라서 클러스터링 품질을 정량화하려면 클러스터 내 SSE(왜곡)와 같은 고유한 메트릭을 사용하여 다양한 k-평균 클러스터링 모델의 성능을 비교해야함.
* KMeans 모델을 장착한 후 이미 **inertia_** 속성을 통해 액세스할 수 있으므로 skickit-learn을 사용할 때 클러스터 내 SSE를 명시적으로 계산할 필요가 없음. 


```python
>>> print(f'Distortion: {km.inertia_:.2f}')
Distortion: 72.48
```

* 클러스터 내 SSE를 기반으로, 우리는 주어진 작업에 대한 최적의 클러스터 수 k를 추정하기 위해 소위 엘보(elbow) 방법이라는 그래픽 도구를 사용할 수 있음.
* k가 증가하면 왜곡이 감소한다고 말할 수 있음. 이는 예제가 할당된 중심선에 더 가깝기 때문 엘보 방법의 이면에 있는 아이디어는 왜곡이 가장 빠르게 증가하기 시작하는 k의 값을 식별하는 것임. k의 다른 값에 대한 왜곡을 플롯하면 더 명확해질 것

```python
>>> distortions = []
>>> for i in range(1, 11):
... km = KMeans(n_clusters=i,
... init='k-means++',
... n_init=10,
... max_iter=300,
... random_state=0)
... km.fit(X)
... distortions.append(km.inertia_)
>>> plt.plot(range(1,11), distortions, marker='o')
>>> plt.xlabel('Number of clusters')
>>> plt.ylabel('Distortion')
>>> plt.tight_layout()
>>> plt.show()
```


* 아래 그림과 같이 엘보는 k = 3에 위치하므로, 이는 k = 3이 실제로 이 데이터 세트에 적합한 선택이라는 것을 뒷받침하는 증거임.

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.3. Finding the optimal number of clusters using the elbow method
</figcaption>


## 3. Quantifying the quality of clustering via silhouette plots

* 클러스터링 품질을 평가하기 위한 또 다른 메트릭은 실루엣 분석(silhouette analysis)이며, 이는 이 장의 뒷부분에서 논의될 k-평균 이외의 클러스터링 알고리즘에도 적용될 수 있음.
* 실루엣 분석을 사용하여 군집의 예제의 측도를 표시할 수 있음. 데이터 세트에서 단일 예제의 실루엣 계수(silhouette coefficient)를 계산하기 위해 다음 세 단계를 적용할 수 있음.
* 1) 군집 응집력 $a^{(i)}$를 예제 $x^{(i)}$와 동일한 군집의 다른 모든 점 사이의 평균 거리로 계산
* 2) 예제 $x^{(i)}$와 가장 가까운 군집의 모든 예제 사이의 평균 거리로 다음 가장 가까운 군집으로부터의 군집 분리 $b^{(i)}$를 계산
* 3) 아래 표시된 것처럼 클러스터 응집력과 분리 사이의 차이를 둘 중 큰 값으로 나눈 실루엣 $s^{(i)}$를 계산

$S^{(i)}=\frac{b^{(i)}-a^{(i)}}{max\{b^{(i)},a^{(i)}\}}$

* 실루엣 계수는 –1에서 1까지의 범위로 제한됨. 
* 앞의 방정식을 바탕으로 군집 분리와 응집력이 같으면 실루엣 계수가 0임을 알 수 있음. $(b^{(i)} = a^{(i)})$ 또한, $b^{(i)}$는 예제가 다른 클러스터와 얼마나 다른지 정량화하고 $a^{(i)}$는 자체 클러스터의 다른 예제와 얼마나 유사한지 알려주기 때문에, $b^{(i)}>>a^{(i)}$ 일 경우 이상적인 실루엣 계수 1에 근접
* 실루엣 계수는 skickit-learn's metric module에서 실루엣_sample로 사용할 수 있으며, 필요에 따라 편의를 위해 실루엣_scores 함수를 사용함.
* 실루엣_scores 함수는 모든 예제의 평균 실루엣 계수를 계산함. 이는 numpy. mean(실루엣_samples(...))과 같으며, 다음 코드를 실행하여 k = 3인 k-분산 군집화에 대한 실루엣 계수의 그림을 만듦.

```python
>>> km = KMeans(n_clusters=3,
... init='k-means++',
... n_init=10,
... max_iter=300,
... tol=1e-04,
... random_state=0)
>>> y_km = km.fit_predict(X)
>>> import numpy as np
>>> from matplotlib import cm
>>> from sklearn.metrics import silhouette_samples
>>> cluster_labels = np.unique(y_km)
>>> n_clusters = cluster_labels.shape[0]
>>> silhouette_vals = silhouette_samples(
... X, y_km, metric='euclidean'
... )
>>> y_ax_lower, y_ax_upper = 0, 0
>>> yticks = []
>>> for i, c in enumerate(cluster_labels):
... c_silhouette_vals = silhouette_vals[y_km == c]
... c_silhouette_vals.sort()
... y_ax_upper += len(c_silhouette_vals)
... color = cm.jet(float(i) / n_clusters)
... plt.barh(range(y_ax_lower, y_ax_upper),
... c_silhouette_vals,
... height=1.0,
... edgecolor='none',
... color=color)
... yticks.append((y_ax_lower + y_ax_upper) / 2.)
... y_ax_lower += len(c_silhouette_vals)
>>> silhouette_avg = np.mean(silhouette_vals)
>>> plt.axvline(silhouette_avg,
... color="red",
... linestyle="--")
>>> plt.yticks(yticks, cluster_labels + 1)
>>> plt.ylabel('Cluster')
>>> plt.xlabel('Silhouette coefficient')
>>> plt.tight_layout()
>>> plt.show()
```






