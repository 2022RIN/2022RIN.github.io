---
layout: single
title:  "Chapter 5. Compressing Data via Dimensionality Reduction"
use_math: true
---

## 1. Unsupervised dimensionality reduction via principal component analysis
* **feature selection(특성 선택)과 feature extraction(특성 추출)의 공통점**
    * 데이터 특성의 수를 줄이기 위해 사용되는 기술임 **(차원 축소)**. 
  
* **특성 선택과 특성 추출의 차이점**
    * 특성 선택(feature selection): sequential back-ward selection(SBS)와 같이 알고리즘을 사용 시 기존의 특성 중에 특성을 선택함. (Chapter 4의 후반부 내용)
    * 특성 추출(feature extraction): 특성에서 얻은 정보를 토대로하여 데이터를 <mark style='background-color: #fff5b1'> 새로운 특성 공간으로 변환(결합,투영)함. </mark> 
*  특성 추출 
    * 차원 축소(dimensionality reduction)맥락에서 특성 추출은 관련 정보를 유지하는 것을 목표로 **데이터 압축**에 대한 접근 방식
    * 학습 알고리듬의 저장 공간 또는 계산 효율성을 향상시키는 데 사용될 뿐만 아니라 차원의 저주를 줄임으로써 **예측 성능을 향상시키는 효과가 있음.**
    
#### * The main steps in principal component analysis
* 이 섹션에서는 비지도 선형 변환 기술인 <mark style='background-color: #fff5b1'> 주성분 분석(principal component analysis ;PCA) </mark> 에 대해 논의하며, 특성 추출 및 차원 축소에 대해 논의함.

* **주성분 분석(principal component analysis ;PCA)**
    * PCA는 **특성 간 상관 관계**를 기반으로 데이터 패턴을 식별하는 데 도움을 줌. 
    * PCA는 고차원 데이터에서 **최대 분산의 방향**을 찾는 것을 목표로 하며, 새로운 부분 공간에 데이터를 원래의 데이터 또는 더 작은 차원으로 투영함. 

![1](https://user-images.githubusercontent.com/113302607/195550970-af9b7346-5d7c-4991-9824-ddd38ce71de4.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.1. Using PCA to find the directions of maximum variance in a dataset
</figcaption>

* Fig. 1.에서 $x1$과 $x2$는 원본 특성 축을 의미하며 **PC 1**과 **PC 2**는 주성분을 나타내며, 두 직교 좌표는 서로 직각을 이루는 형태를 가짐.
    * 이는 벡터의 특성으로 보아 **두 직교좌표가 상관관계가 없음을 의미함.** 
* PCA를 사용하여 차원을 축소하기 위해서는, 원본 특성 차원 $d$보다 차원이 작은 새로운 특성 부분공간 $k$차원의 변환형렬 $W$를 구성하며 관련 식은 아래와 같음. 

$$
\lim_{x\to 0}{\frac{e^x-1}{2x}}
\overset{\left[\frac{0}{0}\right]}{\underset{\mathrm{H}}{=}}
\lim_{x\to 0}{\frac{e^x}{2}}={\frac{1}{2}}
$$


$$x=[x_1,x_2,...,x_d], x \in \mathbb{R}^k$$

$$
xW = z
$$

* 변환 행렬에 의해 다음과 같이 변환된 출력 벡터는 다음과 같음.

$$
z=[z_1,z_2,...,z_d], z \in \mathbb{R}^k 
$$

* 원래의 $d$차원 데이터를 $k$차원의 부분 공간으로 변형함을 통해(앞서 언급했듯, $K$는 원본 $d$보다 작은 차원을 가짐.) 첫 번째 주성분이 가장 큰 분산을 가짐. 
    * 모든 주성분이 다른 주성분과 상관관계가 없다는 제약 하에 가장 큰 분산을 가지는 것임. 

* **PCA 알고리즘 단계를 정리하면 다음과 같음.** 
    1. d차원 데이터 세트를 표준화
    2. 공분산(covariance) 행렬을 구성
    3. 공분산 행렬을 고유 벡터(eigenvector)와 고윳값(eigenvalue)으로 분해
    4. 해당 고유 벡터의 순위를 매기기 위해 고윳값을 내림차순으로 정렬
    5. 가장 큰 고윳값을 가지는 k개의 고유 벡터를 선택(여기서, k는 특성 부분공간 차원을 의미($k \le d$))
    6. 가장큰 $k$개의 고유 벡터로 투영(projection)행렬 $W$를 구성
    7. 투영행렬 $W$를 사용하여 $d$차원 입력 데이터 세트 $X$를 변환하여 새로운 $k$차원 형상 부분 공간을 얻음.

#### * Extracting the principal components step by step
*  이번 섹션에서는, PCA의 4가지 단계를 다루고 파이썬으로 구현함.
    1. 데이터 표준화
    2. 공분산 행렬 구성
    3. 공분산 행렬의 고유값 및 고유 벡터 추출
    4. 고유 벡터 순위 카운트를 위한 고윳값 정렬
    
```python
from sklearn.model_selection import train_test_split 
X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values
X_train, X_test, y_train, y_test = \
train_test_split(X, y, test_size=0.3,stratify=y,random_state=0)
# 특성 표준화 
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train_std = sc.fit_transform(X_train)
X_test_std = sc.transform(X_test)
```

* **(데이터 표준화 전처리 과정임)** 위 코드를 통해 와인 데이터 셋을 읽어들인 후, 훈련과 테스트 데이터 세트로(각각 70%, 30%)나누어 단위 분산으로 표준화 함.
* 다음으로 공분산 행렬(2개의 확률변수의 상관관계를 나타냄.)을 구성함. 
* $d$는 데이터 집합의 차원 수인 대칭 $dxd$ 차원 공분산 행렬은 서로 다른 특징 사이의 pairwise(쌍) 공분산을 저장함.
* 두 특성 사이의 공분산은 다음 방정식을 통해 계산됨.

$$
\sigma_{jk} = \frac{1}{n} \sum_{i=1}^{n}(x_{j}^{(i)}-\mu_j)(x_{k}^{(i)}-\mu_k) 
$$

* 여기서, $\mu_{j}$와 $\mu_{k}$는 각각 특성 $j$와 $k$의 표본 평균을 의미함.
* 이전 단계에서 데이터 세트 전처리 과정을 통해 평균은 0인 상태임.(=정규분포)
* 공분산이 양인 경우 특성이 함께 증가 또는 감소함을 나타내며, 음의 경우 특성이 반대로 달라짐을 의미함.
* 예를들어, 3개의 특성으로 이루어진 공분산 행렬은 다음과 같이 나타남. 

$$ 
\begin{bmatrix}
\sigma_{1}^{2} & \sigma_{12} & \sigma_{13}\\ 
\sigma_{21} & \sigma_{2}^{2} & \sigma_{23}\\ 
\sigma_{31} & \sigma_{32} & \sigma_{2}^{2}
\end{bmatrix} 
$$

* 행렬의 고유 벡터는 주성분(최대 분산 방향)을 나타내며, 해당 고유의 값은 크기를 나타냄.
* 와인 데이터 세트의 경우, 13X13 차원 공분산 행렬에서 13개의 고유벡터와 고유의 값을 얻음.
* 세 번째 단계로 공분산 행렬의 고유 쌍을 구함.
* 선형대 수학에서, 고유 벡터 $v$는 다음 조건을 만족함. 

$$
\Sigma v = \lambda v 
$$

* 여기서, $\lambda$는 스칼라로, 고유값을 의미함.
* 고유 벡터와 고유값을 수동으로 계산한는 것은 복잡하기 때문에, 와인 데이터셋에서 공분산 행렬의 고유 쌍을 얻기위해 **Numpy**의 **linalg.eig** 함수를 사용함.

```python
import numpy as np
cov_mat = np.cov(X_train_std.T)
eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)
print('\nEigenvalues \n', eigen_vals)
```

![2](https://user-images.githubusercontent.com/113302607/195551437-57b32065-072b-47fe-bf9c-fad348f57d96.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.2. result
</figcaption>

* **numpy.cov** 함수를 통해 훈련 데이터의 공분산 행렬을 구하고, **linalg.eig** 함수를 사용하여 고윳값을 구함.
* 이를 통해 와인 데이터 세트에서 13개의 고윳값이 저장된 행렬을 얻음. 

#### * Total and explained variance
* 데이터 세트를 새로운 하위 특성 공간으로 압축하는 것을 목표로하기 때문에 대부분의 정보(분산)를 포함하고 있는 고유 벡터(주성분)의 일부만을 선택함.
    * 고유 벡터는 고윳값 순서에 따라 가장 큰 k개를 의미 
* 고윳값은 고유 벡터의 크기를 정하기 때문에, 크기를 줄여 고윳값을 정렬해야함. 
* explained variance은 고유 벡터 k 수집 이전에 주어진 데이터의 분산 설명 비율을 표시함. 

$$ Explained variance ratio = \frac{\lambda_{j}}{\sum_{j=1}^{d}\lambda_{j}} $$

* 비율은 전체 고윳값 중 원하는 고윳값 $\lambda_{j}$의 비율을 의미함. 
* numpy의 **cumsum** 함수로 분산 누적 총합을 계산하는 코드와 결과는 다음과 같음. 

```python
tot = sum(eigen_vals)
var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]
cum_var_exp = np.cumsum(var_exp)
import matplotlib.pyplot as plt
plt.bar(range(1,14), var_exp, align='center', label='Individual explained variance')
plt.step(range(1,14), cum_var_exp, where='mid', label='Cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal component index')
plt.legend(loc='best')
plt.tight_layout()
plt.show()
```

![3](https://user-images.githubusercontent.com/113302607/195551551-bc66a120-7b45-4dc7-953c-20cd78e163f3.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.3. The proportion of the total variance captured by the principal components
</figcaption>

* Fig. 3.를 통해 첫 번째 주성분만 분산의 약 40%를 차지하는 것을 확인할 수 있으며, 두 번째 까지의 구성 요소까지 포함하면 약 60% 까지 차지함을 확인할 수 있음 

#### * Feature transformation
* 이번 단계에서는 와인 데이터 세트를 새로운 주성분 축으로 변환하는 단계를 설명함. 
    1. 가장 큰 고윳값에 해당하는 k개의 고유 벡터를 선택함. 
    2. 선택된 고유 벡터로 투영행렬 W를 구성함.
    3. 투영행렬 W를 사용하여 하위 k 차원 특성 부분 공간을 얻음.
* 다음 코드를 통해 고윳값의 순서를 줄여 고유 쌍을 내림차 순으로 정렬함.

```python
eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i]) for i in range(len(eigen_vals))]
eigen_pairs.sort(key=lambda k: k[0], reverse=True)
```

* 분산의 60%를 캡처하기 위해, 가장 큰 두 고윳값에 해당하는 고유 벡터를 수집함.
* Fig. 3.에서와 같이 이 두 가지 세트로 총 분산의 60%를 캡처할 수 있음. 
* 결과 실행을 통해 13X2 차원의 투영행렬 $W$를 생성함.

```python
w = np.hstack((eigen_pairs[0][1][:, np.newaxis], eigen_pairs[1][1][:, np.newaxis]))
print('Matrix W:\n', w)
```

![4](https://user-images.githubusercontent.com/113302607/195551679-c9349b1d-e3fa-4d08-83fc-d966f99cb871.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.4. result
</figcaption>

* 위 투영행렬을 사용하여 샘플 $x$를 PCA 부분 공간으로 변환하여 두 가지 특성으로 이루어진 2차원 샘플 벡터 $x^{'}$를 얻음.

$$ x^{'} = xW $$

```python
X_train_std[0].dot(w)
X_train_pca = X_train_std.dot(w)
colors = ['r', 'b', 'g']
markers = ['o', 's', '^']
for l, c, m in zip(np.unique(y_train), colors, markers): plt.scatter(X_train_pca[y_train==l, 0], X_train_pca[y_train==l, 1], c=c, label=f'Class {l}', marker=m)
plt.xlabel('PC 1')
plt.ylabel('PC 2')
plt.legend(loc='lower left')
plt.tight_layout()
plt.show()
```

![5](https://user-images.githubusercontent.com/113302607/195551953-2b75f87d-ad65-4ebf-b36b-a273121d58f7.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.5. Data records from the Wine dataset projected onto a 2D feature space via PCA
</figcaption>

* Fig. 5.를 통해 데이터는 두 번째 주성분(y축)에 비해 첫 번째 주성분(x축)을 따아 더 넓게 분산되어 있으며, Fig. 3. 그래프의 분산 비율과 일치함.

#### * Principal component analysis in scikit-learn
* 이번 단계에서는 sickit-learn에서 구현된 PCA 클래스 구현 방법에 대해 논의함. 

```python
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
lr = LogisticRegression(multi_class='ovr', random_state=1, solver='lbfgs')
X_train_pca = pca.fit_transform(X_train_std)
X_test_pca = pca.transform(X_test_std)
lr.fit(X_train_pca, y_train)
plot_decision_regions(X_train_pca, y_test, classifier=lr)
plt.xlabel('PC 1')
plt.ylabel('PC 2')
plt.legend(loc='lower left')
plt.tight_layout()
plt.show()
```

* 위 코드를 실행하면, 두 개의 주성분 축으로 축소됨. 

![6](https://user-images.githubusercontent.com/113302607/195552150-7cde9ebf-3888-4217-bf1e-4c3dd787be98.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.6. Test datapoints with logistic regression decision regions in the PCA-based feature space
</figcaption>

## 2. Supervised data compression via linear discriminant analysis
* **선형 판별 분석(linear discriminant analysis ;LDA)** 은 비정규화된 모델에서 과대적합 정도를 줄이고 계산의 효율성을 높이기 위한 특성 추출 기법임.
* LDA는 PCA의 개념과 매우 유사하지만 PCA는 데이터 세트에서 최대 분산의 직교 성분 축을 찾는 것을 목표로 하지만, LDA는 클래스 구분을 최적으로 할 수 있는 특성 하위 공간을 찾는 것을 목표로 함. 

#### * Principal component analysis versus linear discriminant analysis
* PCA와 LDA의 공통점
    * 데이터 세트 차원의 수를 줄이는데 사용되는 선형 변환 기술임. 
  
* PCA와 LDA의 차이점
    * PCA는 비지도, LDA는 지도학습임.
*  이러한 이유로 LDA가 클래스 분류에 우수한 기술이라고 생각할 수 있음. 
는 두 가지 클래스 문제에 대한 LDA 개념을 요약한 것입니다. 클래스 1의 예는 원으로 표시되고 클래스 2의 예는 십자가로 표시됩니다.

![7](https://user-images.githubusercontent.com/113302607/195552333-b7f60aef-e169-4d61-ba6d-c6f5d6492e9c.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.7. The concept of LDA for a two-class problem
</figcaption>

* Fig. 7.은 두 가지 클래스 문제에 대한 LDA의 개념을 요약한 그림으로 클래스 1의 예는 원으로, 클래스 2는 십자가 모양으로 표시됨.
* x축(LD 1)에 표시된 것처럼 선형 판별 벡터는 두 클래스를 잘 분리함.
* 하지만, y축(LD 2)의 선형 판별 벡터는 분산을 잡지만 클래스 판별 정보가 없기 때문에 좋은 선형 판별기는 아님.
* LDA의 한 가지 가정은 데이터가 정규 분포를 따르며, 클래스가 동일한 공분산 행렬을 가지며 훈련예제는 통계적으로 서로 독립적이라고 가정함. 

#### * The inner workings of linear discriminant analysis
* **LDA 알고리즘 단계를 정리하면 다음과 같음.** 
    1. $d$차원 데이터 세트를 표준화
    2. 각 클래스에 대해 $d$차원 평균 벡터를 계산
    3. 클래스 간 산포 행렬 $S_{B}$와 클래스 내 산포 행렬 $S_{W}$를 구성합니다.
    4. 행렬의 고유 벡터와 해당 고유값 계산, $S_{W}^{-1}S_{B}S_{B}$
    5. 해당 고유 벡터의 순위를 매기기 위해 고윳값을 내림차순으로 정렬합니다.
    6. $k$개의 가장 큰 고유값에 해당하는 $k$개의 고유 벡터를 선택하여 $d×k$차원 변환 행렬 $W$를 구성합니다.
    7. 변환 행렬 $W$를 사용하여 예제들을 새 형상 부분 공간에 투영한다.

#### * Computing the scatter matrices
* 이 단계에서, 클래스 내 산포 행렬과 클래스 간 산포 행렬을 각각 구성하는 데 사용할 평균 벡터의 계산을 진행함.
* 각 평균 벡터 $mi$는 클래스 $i$의 예에 대한 평균 특성 값 $\mu_{m}$을 저장함.

$$ m_{i}=\frac{1}{n}\sum_{x\in D_{i}}^{c}x_{m} $$

* 결과 평균 벡터는 다음과 같음.

$$ m={i}=\begin{bmatrix}
\mu_{i},alcohol\\ 
\mu_{i},malic acid\\ 
\vdots\\
\mu_{i},proline\\ 
\end{bmatrix}^{T} $$

* 이러한 평균 벡터는 다음 코드에 의해 계산될 수 있으며, 여기서 세 개의 레이블 각각에 대해 하나의 평균 벡터를 계산함.

```python
np.set_printoptions(precision=4)
mean_vecs = []
for label in range(1,4): mean_vecs.append(np.mean( X_train_std[y_train==label], axis=0))
print(f'MV {label}: {mean_vecs[label - 1]}\n')
```

* 평균 벡터를 사용하여 클래스 내 산포 행렬을 계산할 수 있음, $S_{W}$:

$$ S_{W}=\sum_{i=1}^{c}S_{i} $$
* 이것은 각 개별 클래스 $i$의 개별 산포 행렬 $S_{i}$를 합하여 계산됨.
$$ S_{i}=\sum_{i=1}\in (x-m_{i})(x-m_{i})^{T} $$

```python
d = 13 # 특성 번호 
S_W = np.zeros((d, d))
for label, mv in zip(range(1, 4), mean_vecs): 
class_scatter = np.zeros((d, d))
for row in X_train_std[y_train == label]:
row, mv = row.reshape(d, 1), mv.reshape(d, 1)
class_scatter += (row - mv).dot((row - mv).T)
S_W += class_scatter
print('Within-class scatter matrix: 'f'{S_W.shape[0]}x{S_W.shape[1]}')
```

* 산포 행렬 계산 시 훈련 세트와 클래스 레이블이 균등하게 분포되었다고 가정, 하지만 클래스 레이블 개수 출력을 통해 가정이 다름을 확인할 수 있음.
* 따라서, 개별 산포 행렬을 전체에 더하기 전, 스케일을 조정해 줌.
* 산포행렬 클래스를 샘플 개수로 나누면 산포 행렬을 계산한느 공분산 행렬과 같은 계산임을 알 수 있으며, 이를 통해 공분산 행렬은 산포 행렬의 정규화된 공식임을 알 수 있음. 

$$ \Sigma_{i}=\frac{1}{n_{i}}S_{i}=\frac{1}{n_{i}}\sum_{i=1}\in (x-m_{i})(x-m_{i})^{T} $$

* 스케일링된 클래스 내 산란 행렬을 계산하는 코드는 다음과 같음.

```python
d = 13 # 특성 번호 
S_W = np.zeros((d, d))
for label,mv in zip(range(1, 4), mean_vecs): class_scatter = np.cov(X_train_std[y_train==label].T)
S_W += class_scatter
print('Scaled within-class scatter matrix: 'f'{S_W.shape[0]}x{S_W.shape[1]}')
```

* 스케일링된 클래스 내 산란 행렬(또는 공분산 행렬)을 계산한 후 다음 단계로 이동하여 클래스 간 산란 행렬 $S_{B}$를 계산할 수 있음.

$$ S_{i}=\sum_{i=1}^{c}\in (m_{i}-m)(m_{i}-m)^{T} $$

```python
mean_overall = np.mean(X_train_std, axis=0)
mean_overall = mean_overall.reshape(d, 1)
d = 13 # number of features
S_B = np.zeros((d, d))
for i, mean_vec in enumerate(mean_vecs): n = X_train_std[y_train == i + 1, :].shape[0] mean_vec = mean_vec.reshape(d, 1) # make column vector
S_B += n * (mean_vec - mean_overall).dot((mean_vec - mean_overall).T)
print('Between-class scatter matrix: 'f'{S_B.shape[0]}x{S_B.shape[1]}')
```

* 위 식에서 $m$은 모든 클래스 $c$의 예를 포함하여 계산되는 전체의 평균을 의미함.

#### * Selecting linear discriminants for the new feature subspace
* LDA의 나머지 단계는 PCA의 단계와 유사함. 그러나 공분산 행렬에 대해 고유 분해를 수행하는 대신 행렬의 일반화된 고윳값 문제를 해결에 목적을 둠. $S_{W}^{-1}S_{B}S_{B}$

```python
eigen_vals, eigen_vecs =\np.linalg.eig(np.linalg.inv(S_W).dot(S_B))
eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:,i]) for i in range(len(eigen_vals))]
eigen_pairs = sorted(eigen_pairs, key=lambda k: k[0], reverse=True)
print('Eigenvalues in descending order:\n') 
for eigen_val in eigen_pairs:
print(eigen_val[0])
```

![8](https://user-images.githubusercontent.com/113302607/195552419-8f615e18-af2f-4722-8016-769398491203.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.8. result
</figcaption>

* LDA에서 선형 판별 벡터는 최대 $c-1$개 이며, 여기서 $c$는 클래스 레이블의 개수를 의미함.
* 선형 판별 벡터로 찾은 클래스 판별 정보를 측정하기 위해, 고윳값 내림차순으로 선형 판별 벡터를 그리면 다음과 같음.

![9](https://user-images.githubusercontent.com/113302607/195552498-78b3027a-bf36-4120-a72f-1a73400c2b6a.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.9. The top two discriminants capture 100 percent of the useful information
</figcaption>

* Fig. 9. 와 같이, 첨은 두 개의 선형 판별기는 와인 훈련 데이터 세트에서 유용한 정보량의 100%를 캡처함. 
* 이제 두 가지의 판별 벡터를 열로 만들어 변환 행렬 %W%로 만들어 줌. 

![10](https://user-images.githubusercontent.com/113302607/195552655-139831bf-1422-4358-816a-1155c59c917b.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.10. result
</figcaption>

#### * Projecting examples onto the new feature space
* 생성한 행렬$W$를 사용하여 교육데이터 세트를 변환함. 

$$ X^{'}=XW $$

```python
X_train_lda = X_train_std.dot(w)
colors = ['r', 'b', 'g']
markers = ['o', 's', '^']
for l, c, m in zip(np.unique(y_train), colors, markers):
plt.scatter(X_train_lda[y_train==l, 0],
X_train_lda[y_train==l, 1] * (-1),
c=c, label= f'Class {l}', marker=m)
plt.xlabel('LD 1')
plt.ylabel('LD 2')
plt.legend(loc='lower right')
plt.tight_layout()
plt.show()
```

![11](https://user-images.githubusercontent.com/113302607/195552788-96cd7a63-aea3-47c3-a071-bca05fb2445e.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.11. Wine classes perfectly separable after projecting the data onto the first two discriminants
</figcaption>

* Fig. 11. 에서 볼 수 있듯 세 가지 와인 클래스는 이제 새로운 기능 하위 공간에서 완벽하게 선형으로 분리됨.

#### * LDA via scikit-learn
* 이번 단계에서는 sickit-learn에서 구현된 LDA 클래스 구현 방법에 대해 논의함. 

```python
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
lda = LDA(n_components=2)
X_test_lda = lda.transform(X_test_std)
plot_decision_regions(X_test_lda, y_test, classifier=lr)
plt.xlabel('LD 1')
plt.ylabel('LD 2')
plt.legend(loc='lower left')
plt.tight_layout()
plt.show()
```

![12](https://user-images.githubusercontent.com/113302607/195552952-66429c08-c131-4c7f-b4bf-50ddc43e1e29.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.12. The logistic regression model works perfectly on the test data
</figcaption>

* Fig. 12. 를 통해, 로지스틱 회귀 분석 분류기는 원래의 13가지 와인 특성 대신 2차원 특징 부분 공간만을 사용하여 테스트 데이터 세트의 예제를 분류하기 위한 완벽한 정확도 점수를 얻을 수 있음.**

## 3. Nonlinear dimensionality reduction and visualization
* 이전 섹션에서 특성 추출을 위해 PCA 및 LDA와 같은 선형 변환 기술에 대해 설명하였음. 이 섹션에서는 비선형 차원 축소 기술을 고려하는 것이 왜 가치가 있는지 논의함.
* 또한 t-SNE(st-distributed schorical neighbor embedding)을 적용하여 2차원 특징 공간에 손으로 쓴 이미지의 이미지를 그릴 수 있는지 볼 것임.
