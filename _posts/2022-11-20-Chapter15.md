---
layout: single
title:  "Chapter15. Modeling Sequential Data Using Recurrent Neural Networks"
use_math: true
---
#### Topic
* 이 장에서는 **반복 신경망(RNN)** 을 탐구하고 순차 데이터 모델링에 적용.

* 1) 순차 데이터 소개
* 2) 시퀀스 모델링을 위한 RNN
* 3) 긴 단기 메모리 
* 4) 시간에 따른 잘린 역전파
* 5) PyTorch에서 시퀀스 모델링을 위한 다층 RNN 구현
* 6) 프로젝트 1: IMDb 영화 리뷰 데이터 세트의 RNN 정서 분석
* 7) 프로젝트 2: RNN 문자 수준 언어 모델. LSTM 셀을 사용한 g, Jules Verne의 The Mysterious Island의 텍스트 데이터 사용
* 8) 폭발적인 그레이디언트를 피하기 위해 그라디언트 클리핑 사용


## 1. Introducing sequential data: 순차 데이터 소개 

#### Modeling sequential data – order matters

* 다른 유형의 데이터와 비교하여 시퀀스를 고유하게 만드는 것은 시퀀스의 요소가 특정 순서로 나타나며 서로 독립적이지 않음. 지도 학습을 위한 일반적인 기계 학습 알고리듬은 입력이 독립적이고 동일하게 분포된(IID) 데이터라고 가정하며, 이는 훈련 예제가 상호 독립적이며 기본 분포가 동일하다는 것을 의미함. 이와 관련하여, 상호 독립성 가정에 기초하여, 훈련 예제가 모델에 제공되는 순서는 무관함.
* 예를 들어, n개의 훈련 예제, $x^{(1)}, x^{(2)}, ..., x^{(n)}$로 구성된 샘플이 있다면, 머신 러닝 알고리듬을 훈련하기 위해 데이터를 사용하는 순서는 중요하지 않음.
* 이 시나리오의 예로는 이전에 작업했던 아이리스 데이터 세트가 있음. 아이리스 데이터 세트에서 각 꽃은 독립적으로 측정되었으며, 한 꽃의 측정값은 다른 꽃의 측정값에 영향을 미치지 않음.
* 예를 들어, n개의 교육 예제의 샘플이 있다고 가정, 여기서 각 훈련 예제는 특정 날의 특정 주식의 시장 가치를 나타냄. 과제가 향후 3일간의 주식시장 가치를 예측하는 것이라면, 이러한 훈련 사례를 무작위 순서로 활용하기보다는 추세를 도출하기 위해 날짜순으로 이전 주가를 고려하는 것이 타당할 것임.

#### Sequential data versus time series data: 순차 데이터 대 시계열 데이터

* 시계열 데이터는 각 예제가 시간의 차원과 연관된 특수한 유형의 순차 데이터임. 시계열 데이터에서 표본은 연속적인 타임스탬프에서 추출되므로 시간 차원이 데이터 점 간의 순서를 결정함.
* 예를 들어, 주가와 음성 또는 음성 기록은 시계열 데이터임. 반면에 모든 순차 데이터에 시간 차원이 있는 것은 아님. 예를 들어 텍스트 데이터 또는 DNA 시퀀스에서 예제는 순서가 지정되지만 텍스트 또는 DNA는 시계열 데이터로 적합하지 않음.

#### Representing sequences

* 순차적 데이터에서 데이터 포인트 간의 순서가 중요하다는 것을 확인했으며, 다음으로 기계 학습 모델에서 이 순서 정보를 활용하는 방법을 찾아야 함.
* 시퀀스를 $x^{(1)},x^{(2)},…,x^{(T)}$로 표현 위첨자 인덱스는 인스턴스의 순서를 나타내며 시퀀스의 길이는 T임. 시퀀스의 합리적인 예를 위해 각 예제 점 x(t)가 특정 시간 t에 속하는 시계열 데이터를 고려
* 아래 그림은 입력 형상(x's)과 대상 레이블(y's)이 모두 시간 축에 따른 순서를 자연스럽게 따르는 시계열 데이터의 예를 보여줌. 따라서 x와 y는 모두 시퀀스

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.1. An example of time series data
</figcaption>

* 이미지 데이터를 위한 MLP(Multilayer Perceptron) 및 CNN과 같이 지금까지 다룬 표준 NN 모델은 훈련 예제가 서로 독립적이므로 주문 정보를 통합하지 않는다고 가정

#### The different categories of sequence modeling

* 시퀀스 모델링에는 언어 번역(예: 텍스트를 영어에서 독일어로 번역), 이미지 캡션 및 텍스트 생성과 같은 많은 흥미로운 응용 프로그램이 있음.
* 그러나 적절한 아키텍처와 접근 방식을 선택하려면 이러한 서로 다른 시퀀스 모델링 작업을 이해하고 구별할 수 있어야함.
* 아래 그림은 Andrej Karpathy의 2015년 순환 신경망의 불합리한 효과에 대한 훌륭한 기사의 설명을 바탕으로 입력 및 출력 데이터의 관계 범주에 따라 달라지는 가장 일반적인 시퀀스 모델링 작업을 요약

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.2. The most common sequencing tasks
</figcaption>

* 출력이 시퀀스인 경우 모델링 작업은 다음 범주 중 하나에 속할 가능성이 높음. 
* Many-to-one: 입력 데이터는 시퀸스지만 출력은 시퀸스가 아닌 고정 크기 벡터 또는 스칼라, 예를 들어, 감정 분석에서 입력은 텍스트 기반(예: 영화 리뷰)이고 출력은 클래스 레이블(예: 검토자가 영화를 좋아했는지 여부를 나타내는 레이블)
* One-to-many: 입력 데이터는 시퀀스가 아닌 표준 형식이지만 출력은 시퀀스, 이 범주의 예로는 이미지 캡션이 있음. 입력은 이미지이고 출력은 해당 이미지의 내용을 요약하는 영어 구문임.
* Many-to-many: 입력 및 출력 배열은 모두 시퀀스, 이 범주는 입력 및 출력의 동기화 여부에 따라 더 나눌 수 있음. 동기화된 다대다 모델링 작업의 예로는 비디오 분류가 있으며, 여기서 비디오의 각 프레임에 레이블이 지정함.


## RNNs for modeling sequences

#### Understanding the dataflow in RNNs

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.3. The dataflow of a standard feedforward NN and an RNN
</figcaption>

* 위 그림은 비교를 위해 표준 피드포워드 NN과 RNN의 데이터 흐름을 나란히 보여줌.
* 이 두 네트워크 모두 숨겨진 계층이 하나뿐임이 표현에서는 단위가 표시되지 않지만 입력 계층(x), 숨겨진 계층(h) 및 출력 계층(o)이 많은 단위를 포함하는 벡터라고 가정.
* 아래 그림은 하나의 숨겨진 레이어(위)가 있는 RNN과 두 개의 숨겨진 레이어(아래)가 있는 RNN을 보여줌.

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.4. Examples of an RNN with one and two hidden layers
</figcaption>


#### Computing activations in an RNN

* 단순성을 위해, 우리는 단일 숨겨진 레이어만 고려함. 그러나 동일한 개념이 다층 RNN에 적용됨.
* 단일 레이어 RNN의 다른 가중치 행렬은 다음과 같음.

* • $W_{xh}$: 입력 x(t)와 숨겨진 레이어 h 사이의 가중치 행렬 
* • $W_{hh}$: 반복 에지와 관련된 가중치 행렬 
* • $W_{ho}$: 숨겨진 도면층과 출력 도면층 사이의 가중치 행렬

* 가중치 행렬은 아래 그림과 같음.


<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.5. Applying weights to a single-layer RNN
</figcaption>


* 아래 그림은 두 가지 공식을 사용하여 이러한 활성화를 계산하는 과정을 보여줌.

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.6. Computing the activations
</figcaption>


#### Hidden recurrence versus output recurrence: 숨겨진 반복 대 출력 반복

* 반복 연결이 출력 계층에서 나오는 대체 모델이 있다는 점에 유의, 이 경우 이전 시간 단계의 출력 계층에서 발생하는 순 활성화는 두 가지 방법 중 하나로 추가할 수 있음.
* • 현재 시간 단계의 숨겨진 계층에 $h^{t}$ (아래 그림에서 출력-숨겨진 반복으로 표시됨) 
* • 현재 시간 단계의 출력 계층에 $o^{t}$ (아래 그림에서 출력-출력 반복으로 표시됨)


<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.7. Different recurrent connection models
</figcaption>

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.8. Problems in computing the gradients of the loss function
</figcaption>

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.9. The structure of an LSTM cell
</figcaption>

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.10. A breakdown of how embedding works
</figcaption>

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.11. Character-level language modeling
</figcaption>

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.12. Character and integer mappings
</figcaption>

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.13. Predicting the next character for a text sequence
</figcaption>

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.14. Generating next text based on this multiclass classification approach
</figcaption>

