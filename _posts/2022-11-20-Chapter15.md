---
layout: single
title:  "Chapter15. Modeling Sequential Data Using Recurrent Neural Networks"
use_math: true
---
#### Topic
* 이 장에서는 **반복 신경망(RNN)** 을 탐구하고 순차 데이터 모델링에 적용.

* 1) 순차 데이터 소개
* 2) 시퀀스 모델링을 위한 RNN
* 3) 긴 단기 메모리 
* 4) 시간에 따른 잘린 역전파
* 5) PyTorch에서 시퀀스 모델링을 위한 다층 RNN 구현
* 6) 프로젝트 1: IMDb 영화 리뷰 데이터 세트의 RNN 정서 분석
* 7) 프로젝트 2: RNN 문자 수준 언어 모델. LSTM 셀을 사용한 g, Jules Verne의 The Mysterious Island의 텍스트 데이터 사용
* 8) 폭발적인 그레이디언트를 피하기 위해 그라디언트 클리핑 사용


## 1. Introducing sequential data: 순차 데이터 소개 

#### Modeling sequential data – order matters

* 다른 유형의 데이터와 비교하여 시퀀스를 고유하게 만드는 것은 시퀀스의 요소가 특정 순서로 나타나며 서로 독립적이지 않음. 지도 학습을 위한 일반적인 기계 학습 알고리듬은 입력이 독립적이고 동일하게 분포된(IID) 데이터라고 가정하며, 이는 훈련 예제가 상호 독립적이며 기본 분포가 동일하다는 것을 의미함. 이와 관련하여, 상호 독립성 가정에 기초하여, 훈련 예제가 모델에 제공되는 순서는 무관함.
* 예를 들어, n개의 훈련 예제, $x^{(1)}, x^{(2)}, ..., x^{(n)}$로 구성된 샘플이 있다면, 머신 러닝 알고리듬을 훈련하기 위해 데이터를 사용하는 순서는 중요하지 않음.
* 이 시나리오의 예로는 이전에 작업했던 아이리스 데이터 세트가 있음. 아이리스 데이터 세트에서 각 꽃은 독립적으로 측정되었으며, 한 꽃의 측정값은 다른 꽃의 측정값에 영향을 미치지 않음.
* 예를 들어, n개의 교육 예제의 샘플이 있다고 가정, 여기서 각 훈련 예제는 특정 날의 특정 주식의 시장 가치를 나타냄. 과제가 향후 3일간의 주식시장 가치를 예측하는 것이라면, 이러한 훈련 사례를 무작위 순서로 활용하기보다는 추세를 도출하기 위해 날짜순으로 이전 주가를 고려하는 것이 타당할 것임.

#### Sequential data versus time series data: 순차 데이터 대 시계열 데이터

* 시계열 데이터는 각 예제가 시간의 차원과 연관된 특수한 유형의 순차 데이터임. 시계열 데이터에서 표본은 연속적인 타임스탬프에서 추출되므로 시간 차원이 데이터 점 간의 순서를 결정함.
* 예를 들어, 주가와 음성 또는 음성 기록은 시계열 데이터임. 반면에 모든 순차 데이터에 시간 차원이 있는 것은 아님. 예를 들어 텍스트 데이터 또는 DNA 시퀀스에서 예제는 순서가 지정되지만 텍스트 또는 DNA는 시계열 데이터로 적합하지 않음.

#### Representing sequences

* 순차적 데이터에서 데이터 포인트 간의 순서가 중요하다는 것을 확인했으며, 다음으로 기계 학습 모델에서 이 순서 정보를 활용하는 방법을 찾아야 함.
* 시퀀스를 $x^{(1)},x^{(2)},…,x^{(T)}$로 표현 위첨자 인덱스는 인스턴스의 순서를 나타내며 시퀀스의 길이는 T임. 시퀀스의 합리적인 예를 위해 각 예제 점 x(t)가 특정 시간 t에 속하는 시계열 데이터를 고려
* 아래 그림은 입력 형상(x's)과 대상 레이블(y's)이 모두 시간 축에 따른 순서를 자연스럽게 따르는 시계열 데이터의 예를 보여줌. 따라서 x와 y는 모두 시퀀스


![1](https://user-images.githubusercontent.com/113302607/202909882-d9beeacf-1225-41f0-ab84-907ca137dd9c.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.1. An example of time series data
</figcaption>

* 이미지 데이터를 위한 MLP(Multilayer Perceptron) 및 CNN과 같이 지금까지 다룬 표준 NN 모델은 훈련 예제가 서로 독립적이므로 주문 정보를 통합하지 않는다고 가정

#### The different categories of sequence modeling

* 시퀀스 모델링에는 언어 번역(예: 텍스트를 영어에서 독일어로 번역), 이미지 캡션 및 텍스트 생성과 같은 많은 흥미로운 응용 프로그램이 있음.
* 그러나 적절한 아키텍처와 접근 방식을 선택하려면 이러한 서로 다른 시퀀스 모델링 작업을 이해하고 구별할 수 있어야함.
* 아래 그림은 Andrej Karpathy의 2015년 순환 신경망의 불합리한 효과에 대한 훌륭한 기사의 설명을 바탕으로 입력 및 출력 데이터의 관계 범주에 따라 달라지는 가장 일반적인 시퀀스 모델링 작업을 요약

![2](https://user-images.githubusercontent.com/113302607/202909896-69ec1081-b25a-456c-a371-83ceba061ad3.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.2. The most common sequencing tasks
</figcaption>

* 출력이 시퀀스인 경우 모델링 작업은 다음 범주 중 하나에 속할 가능성이 높음. 
* Many-to-one: 입력 데이터는 시퀸스지만 출력은 시퀸스가 아닌 고정 크기 벡터 또는 스칼라, 예를 들어, 감정 분석에서 입력은 텍스트 기반(예: 영화 리뷰)이고 출력은 클래스 레이블(예: 검토자가 영화를 좋아했는지 여부를 나타내는 레이블)
* One-to-many: 입력 데이터는 시퀀스가 아닌 표준 형식이지만 출력은 시퀀스, 이 범주의 예로는 이미지 캡션이 있음. 입력은 이미지이고 출력은 해당 이미지의 내용을 요약하는 영어 구문임.
* Many-to-many: 입력 및 출력 배열은 모두 시퀀스, 이 범주는 입력 및 출력의 동기화 여부에 따라 더 나눌 수 있음. 동기화된 다대다 모델링 작업의 예로는 비디오 분류가 있으며, 여기서 비디오의 각 프레임에 레이블이 지정함.


## RNNs for modeling sequences

#### Understanding the dataflow in RNNs

![3](https://user-images.githubusercontent.com/113302607/202909903-438f0054-0c95-45c4-b71f-1f207928af77.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.3. The dataflow of a standard feedforward NN and an RNN
</figcaption>

* 위 그림은 비교를 위해 표준 피드포워드 NN과 RNN의 데이터 흐름을 나란히 보여줌.
* 이 두 네트워크 모두 숨겨진 계층이 하나뿐임이 표현에서는 단위가 표시되지 않지만 입력 계층(x), 숨겨진 계층(h) 및 출력 계층(o)이 많은 단위를 포함하는 벡터라고 가정.
* 아래 그림은 하나의 숨겨진 레이어(위)가 있는 RNN과 두 개의 숨겨진 레이어(아래)가 있는 RNN을 보여줌.

![4](https://user-images.githubusercontent.com/113302607/202909906-e60ffde1-ca9b-4329-a3d3-f6c960e9bd08.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.4. Examples of an RNN with one and two hidden layers
</figcaption>


#### Computing activations in an RNN

* 단순성을 위해, 우리는 단일 숨겨진 레이어만 고려함. 그러나 동일한 개념이 다층 RNN에 적용됨.
* 단일 레이어 RNN의 다른 가중치 행렬은 다음과 같음.

* • $W_{xh}$: 입력 x(t)와 숨겨진 레이어 h 사이의 가중치 행렬 
* • $W_{hh}$: 반복 에지와 관련된 가중치 행렬 
* • $W_{ho}$: 숨겨진 도면층과 출력 도면층 사이의 가중치 행렬

* 가중치 행렬은 아래 그림과 같음.

![5](https://user-images.githubusercontent.com/113302607/202909912-3de91b20-7103-442b-b5c5-6a85c9fec76a.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.5. Applying weights to a single-layer RNN
</figcaption>


* 아래 그림은 두 가지 공식을 사용하여 이러한 활성화를 계산하는 과정을 보여줌.

![6](https://user-images.githubusercontent.com/113302607/202909920-e92c8769-0767-408f-a6a7-10852e4d743e.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.6. Computing the activations
</figcaption>


#### Hidden recurrence versus output recurrence: 숨겨진 반복 대 출력 반복

* 반복 연결이 출력 계층에서 나오는 대체 모델이 있다는 점에 유의, 이 경우 이전 시간 단계의 출력 계층에서 발생하는 순 활성화는 두 가지 방법 중 하나로 추가할 수 있음.
* • 현재 시간 단계의 숨겨진 계층에 $h^{t}$ (아래 그림에서 출력-숨겨진 반복으로 표시됨) 
* • 현재 시간 단계의 출력 계층에 $o^{t}$ (아래 그림에서 출력-출력 반복으로 표시됨)

![7](https://user-images.githubusercontent.com/113302607/202909929-c988f929-4b83-4aef-8a0c-25163a9f1eea.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.7. Different recurrent connection models
</figcaption>

* 위 그림과 같이 이러한 아키텍처들 사이의 차이는 반복되는 연결에서 명확하게 확인할 수 있음. 이러한 아키텍처들 사이의 차이는 반복되는 연결에서 명확하게 확인할 수 있음. 우리의 표기법에 따라, 반복 연결과 관련된 가중치는 $W_{hh}$에 의해 숨겨진 반복에 대해, $W_{oh}$에 의해 숨겨진 반복에 대해, $W_{oo}$에 의해 출력된 반복에 대해 표시될 것임. 문헌의 일부 논문에서는 반복 연결과 관련된 가중치도 $W_{rec}$로 표시됨.
* torch.nn 모듈을 사용하여 RNN을 통해 반복 레이어를 정의할 수 있으며, 이는 숨겨진 반복과 유사함. 다음 코드에서는 RNN에서 반복 레이어를 생성하고 길이 3의 입력 시퀀스에 대해 포워드 패스를 수행하여 출력을 계산할 것임. 또한 순방향 패스를 수동으로 계산하고 결과를 RNN의 결과와 비교

```python
>>> import torch
>>> import torch.nn as nn
>>> torch.manual_seed(1)
>>> rnn_layer = nn.RNN(input_size=5, hidden_size=2,
... num_layers=1, batch_first=True)
>>> w_xh = rnn_layer.weight_ih_l0
>>> w_hh = rnn_layer.weight_hh_l0
>>> b_xh = rnn_layer.bias_ih_l0
>>> b_hh = rnn_layer.bias_hh_l0
>>> print('W_xh shape:', w_xh.shape)
>>> print('W_hh shape:', w_hh.shape)
>>> print('b_xh shape:', b_xh.shape)
>>> print('b_hh shape:', b_hh.shape)
W_xh shape: torch.Size([2, 5])
W_hh shape: torch.Size([2, 2])
b_xh shape: torch.Size([2])
b_hh shape: torch.Size([2])
```

* 이 레이어의 입력 형태는 (batch_size, sequence_length, 5)이며, 여기서 첫 번째 차원은 배치 차원(batch_first=True로 설정)이고, 두 번째 차원은 시퀀스에 해당하고, 마지막 차원은 형상에 해당
* 길이가 3인 입력 시퀀스에 대해 출력 시퀀스를 수행하면 출력 시퀀스가 $o^(0), o^(1), o^(2)$가 됨.
* 또한 RNN은 기본적으로 하나의 레이어를 사용하며, num_layers를 설정하여 여러 개의 RNN 레이어를 함께 쌓아서 스택된 RNN을 형성할 수 있음.

```python
>>> x_seq = torch.tensor([[1.0]*5, [2.0]*5, [3.0]*5]).float()
>>> ## output of the simple RNN:
>>> output, hn = rnn_layer(torch.reshape(x_seq, (1, 3, 5)))
>>> ## manually computing the output:
>>> out_man = []
>>> for t in range(3):
... xt = torch.reshape(x_seq[t], (1, 5))
... print(f'Time step {t} =>')
... print(' Input :', xt.numpy())
... 
... ht = torch.matmul(xt, torch.transpose(w_xh, 0, 1)) + b_hh
... print(' Hidden :', ht.detach().numpy()
...
... if t > 0:
... prev_h = out_man[t-1]
... else:
... prev_h = torch.zeros((ht.shape))
... ot = ht + torch.matmul(prev_h, torch.transpose(w_hh, 0, 1)) \
... + b_hh
... ot = torch.tanh(ot)
... out_man.append(ot)
... print(' Output (manual) :', ot.detach().numpy())
... print(' RNN output :', output[:, t].detach().numpy())
... print()
Time step 0 =>
 Input : [[1. 1. 1. 1. 1.]]
 Hidden : [[-0.4701929 0.5863904]]
 Output (manual) : [[-0.3519801 0.52525216]]
 RNN output : [[-0.3519801 0.52525216]]
Time step 1 =>
 Input : [[2. 2. 2. 2. 2.]]
 Hidden : [[-0.88883156 1.2364397 ]]
 Output (manual) : [[-0.68424344 0.76074266]]
 RNN output : [[-0.68424344 0.76074266]]
Time step 2 =>
 Input : [[3. 3. 3. 3. 3.]]
 Hidden : [[-1.3074701 1.886489 ]]
 Output (manual) : [[-0.8649416 0.90466356]]
 RNN output : [[-0.8649416 0.90466356]]
```

#### The challenges of learning long-range interactions: 장기적 상호작용 학습의 어려움

* 곱셈 인자인 $\frac{\partial h^{t}}{\partial h^{k}}$ 로 인해 손실함수의 기울기를 계산할 때 **소위 소멸 및 폭발(vanishing and exploding)** 기울기 문제가 발생함.

* 이러한 문제는 아래 그림의 예로 설명되며, 단순성을 위해 숨겨진 장치가 하나만 있는 RNN을 고려함.
* 기본적으로,  $\frac{\partial h^{t}}{\partial h^{k}}$ 는 $t-k$의 곱셈을 가지고있음. 따라서 가중치 w, 그 자체로 $t-k$곱셈을 하면 $wt-k$ 인자가 됨. 
* 반면에, 반복되는 모서리의 가중치가  $|w| \in 1$ 이라면, $t-k$가 클 경우 $wt-k$는 매우 커짐.
* 큰 $t-k$는 장거리 의존성을 의미함. $|w|=1$을 보장함으로써 소멸되거나 폭발하는 그레디언트를 피하는 순진한 해결책에 도달할 수 있음을 알 수 있음. 
* 실제로 이 문제에 대한 솔루션 세 가지는 다음과 같음.
* 경사 클리핑
* 시간에 따른 역전파(TBPTT)
* LSTM

![8](https://user-images.githubusercontent.com/113302607/202909936-7d2b0cb1-abd8-41f8-98c1-114bc3d23078.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.8. Problems in computing the gradients of the loss function
</figcaption>

#### Long short-term memory cells: 장기 단기 메모리 셀
앞서 언급한 바와 같이, LSTM은 소멸 그레이디언트 문제(S에 의한 장기 단기 메모리)를 극복하기 위해 처음 도입되었다. Hochreiter와 J. Schmidhuber, 신경 계산, 9(8): 1735-1780, 1997). LSTM의 구성 요소는 메모리 셀이며, 기본적으로 표준 RNN의 숨겨진 계층을 나타내거나 대체한다. 각 메모리 셀에는, 우리가 논의한 바와 같이 소실되고 폭발하는 기울기 문제를 극복하기 위해 바람직한 무게 w = 1을 갖는 반복 에지가 있다. 이러한 반복 에지와 연관된 값을 집합적으로 셀 상태라고 한다. 현대 LSTM 셀의 펼쳐진 구조는 그림 15.9에 나와 있습니다.

* LSTM은 소멸 그레이디언트 문제(S에 의한 장기 단기 메모리)를 극복하기 위해 처음 도입되었음.
* LSTM의 구성 요소는 메모리 셀이며, 기본적으로 표준 RNN의 숨겨진 계층을 나타내거나 대체함.
* 각 메모리 셀에는, 우리가 논의한 바와 같이 소실되고 폭발하는 기울기 문제를 극복하기 위해 바람직한 가중치 w = 1을 갖는 반복 에지가 존재함.
* 이러한 반복 에지와 연관된 값을 집합적으로 셀 상태라고 함. 
* 현대 LSTM 셀의 펼처진 구조는 아래 그림과 같음.  

![9](https://user-images.githubusercontent.com/113302607/202909941-c3fa7963-0146-4998-a97a-edf7b928f130.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.9. The structure of an LSTM cell
</figcaption>

* 이전 시간 단계인 $C^{(t–1)}$의 셀 상태는 가중치 계수를 직접 곱하지 않고 현재 시간 단계인 $C^{(t)}$에서 셀 상태를 얻도록 수정됨.
* 이 메모리 셀의 정보 흐름은 여기서 설명할 여러 계산 단위(종종 게이트라 불림)에 의해 제어됨.

## 2. Implementing RNNs for sequence modeling in PyTorch: PyTorch에서 시퀀스 모델링을 위한 RNN 구현

이제 RNN 뒤에 있는 기본 이론을 다루었으므로, 이 장의 더 실용적인 부분, 즉 PyTorch에서 RNN을 구현할 준비가 되었다. 이 장의 나머지 부분에서, 우리는 RNN을 두 가지 일반적인 문제 작업에 적용할 것이다: 1. 감정 분석 2. 언어 모델링 우리가 다음 페이지에서 함께 살펴볼 이 두 프로젝트는 모두 흥미롭지만 꽤 관련이 있다. 따라서 코드를 한 번에 제공하는 대신 구현을 여러 단계로 나누고 코드에 대해 자세히 논의할 것이다. 전체적인 개요를 확인하고 토론에 들어가기 전에 모든 코드를 한 번에 확인하려면 먼저 코드 구현을 살펴보십시오.

* 이번 섹션에서는 PyTorch에서 RNN을 구현함. 이 장의 나머지 부분에서 RNN을 두 가지 일반적인 문제 작업에 적용할 것임. 
* 1. 감정 분석 
* 2. 언어 모델링

#### Project one – predicting the sentiment of IMDb movie reviews

* 이 섹션 및 하위 섹션에서는 다대일 아키텍처를 사용하여 감정 분석을 위한 다층 RNN을 구현할 것임. 다음 섹션에서는 언어 모델링 애플리케이션을 위한 many-to-one RNN을 구현할 것임. 선택된 예는 RNN의 주요 개념을 소개하기 위해 의도적으로 단순하지만, 언어 모델링은 광범위한 응용 프로그램을 가져 컴퓨터가 인간과 직접 대화하고 상호작용할 수 있음. 

#### Preparing the movie review data

* 8장에서는 검토 데이터 세트를 사전 처리하고 정리하였음. 똑같은 절차를 진행함. 먼저 필요한 모듈을 가져오고 토치 텍스트(pip 설치 토치 텍스트를 통해 설치 예정, 2021년 말 현재 버전 0.10.0이 사용됨)에서 데이터를 읽음. 
* 각 세트에는 25,000개의 샘플이 있음. 그리고 데이터 세트의 각 샘플은 우리가 예측하고자 하는 대상 레이블을 나타내는 감정 레이블(부정적인 감정을 의미하고 긍정적인 감정을 의미함)과 영화 리뷰 텍스트(입력 기능)의 두 가지 요소로 구성됨. 이러한 영화 리뷰의 텍스트 구성 요소는 단어의 시퀀스이며, RNN 모델은 각 시퀀스를 긍정적(1) 또는 부정적(0) 리뷰로 분류 

```python
>>> from torchtext.datasets import IMDB
>>> train_dataset = IMDB(split='train')
>>> test_dataset = IMDB(split='test')
```

* 그러나 RNN 모델에 데이터를 공급하기 전에 몇 가지 전처리 단계를 적용해야함. 
* 1. 훈련 데이터 세트를 별도의 훈련 및 검증 파티션으로 분할함. 
* 2. 훈련 데이터 세트에서 고유한 단어 식별
* 3. 각 고유 단어를 고유 정수에 매핑하고 검토 텍스트를 인코딩된 정수(각 고유 단어의 색인)로 인코딩
* 4. 모델에 대한 입력으로 데이터 세트를 미니 배치로 나눔.

```python
>>> ## Step 1: create the datasets
>>> from torch.utils.data.dataset import random_split
>>> torch.manual_seed(1)
>>> train_dataset, valid_dataset = random_split(
... list(train_dataset), [20000, 5000])
```


#### Embedding layers for sentence encoding: 문장 인코딩을 위한 레이어 포함
이전 단계에서 데이터를 준비하는 동안 동일한 길이의 시퀀스를 생성했습니다. 이 시퀀스의 요소는 고유 단어의 인덱스에 해당하는 정수였다. 이러한 단어 인덱스는 여러 가지 다른 방식으로 입력 기능으로 변환될 수 있다. 한 가지 간단한 방법은 지수를 0과 1의 벡터로 변환하기 위해 원핫 인코딩을 적용하는 것이다. 그런 다음, 각 단어는 전체 데이터 세트의 고유 단어 수가 크기인 벡터에 매핑됩니다. 고유한 단어의 수(어휘의 크기)가 104 – 105의 순서일 수 있다는 점을 고려할 때, 이러한 기능에 대해 훈련된 모델은 차원성의 저주로 어려움을 겪을 수 있다. 게다가, 이러한 특징들은 하나를 제외하고는 모두 0이기 때문에 매우 희박하다.

* 이전 단계에서 데이터를 준비하는 동안 동일한 길이의 시퀀스를 생성했음. 
* 이 시퀀스 요소는 고유 단어의 인덱스에 해당하는 정수였음. 이러한 단어 인덱스는 여러 가지 다른 방식으로 입력 기능으로 변환될 수 있음. 
* 한 가지 간단한 방법은 지수를 0과 1의 벡터로 변환하기 위해 원핫 인코딩을 적용하는 것임.
* 그런 다음, 각 단어는 전체 데이터 세트의 고유 단어 수가 크기인 벡터에 매핑됨. 고유한 단어의 수(어휘의 크기)가 $10^{4} – 10^{5}$의 순서일 수 있다는 점을 고려할 때, 이러한 기능에 대해 훈련된 모델은 차원성의 저주로 어려움을 겪을 수 있음.
* 다른 접근법은 단어를 실제 값 요소(꼭 정수일 필요는 없음)가 있는 고정된 크기의 벡터에 매핑하는 것임.
* 하나의 핫 인코딩된 벡터와 대조적으로, 유한한 크기의 벡터를 이용할 수 있음.(예를 들어,[-1,1])
* 이것이 임베딩 뒤에 있는 아이디어임. 이는 데이터 세트의 단어를 표현하기 위해 두드러진 특징을 자동으로 학습하기 위해 사용할 수 있는 기능 학습 기술임.
* 고유 단어 수, nwords를 고려할 때, 우리는 전체 어휘를 입력 특징으로 나타내기 위해 고유 단어 수(embedding_dim << nwords)보다 훨씬 작은 임베딩 벡터의 크기(일명 임베딩 차원)를 선택할 수 있음.

* 원핫 인코딩에 비해 임베딩의 장점은 다음과 같음.
* • 차원성의 저주의 영향을 줄이기 위한 특징 공간의 차원성 감소 
* • NN의 임베딩 계층이 최적화(또는 학습)될 수 있기 때문에 두드러진 특징의 추출
* 다음의 도식적 표현은 토큰 인덱스를 훈련 가능한 임베딩 매트릭스에 매핑함으로써 임베딩이 어떻게 작동하는지 보여줌.

![10](https://user-images.githubusercontent.com/113302607/202909945-8acc7a5f-7170-4810-a894-3d67fcfe72e7.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.10. A breakdown of how embedding works
</figcaption>

* n+2 크기의 토큰 집합이 주어지면(n은 토큰 집합의 크기, 인덱스 0은 패딩 자리 표시자를 위해 예약되어있고 1은 토큰 집합에 없는 단어를 위해 예약되어 있음.) 크기의 임베딩 행렬(n + 2) × 임베딩_dim은 이 행렬의 각 행이 토큰과 관련된 숫자 특징을 나타내는 곳에 생성
* 따라서, 정수 인덱스 i가 임베딩에 대한 입력으로 제공될 때, 그것은 인덱스 i에서 행렬의 해당 행을 찾고 숫자 특징을 반환할 것이다. 임베딩 매트릭스는 NN 모델의 입력 계층 역할을 함.
* 실제로 임베딩 계층을 만드는 것은 단순히 **nn.Embedding** 을 사용하여 수행할 수 있음. 

#### Project two – predicting the sentiment of IMDb movie reviews
언어 모델링은 기계가 영어 문장을 생성하는 것과 같은 인간 언어 관련 작업을 수행할 수 있게 해주는 흥미로운 응용 프로그램이다. 이 분야의 흥미로운 연구 중 하나는 일리야 수츠케버, 제임스 마르텐스, 제프리 E의 순환 신경망을 사용한 텍스트 생성이다. 힌튼, 2011년 제28회 국제 기계 학습 회의(ICML-11)의 프로시딩 (hinton://pdfs. semanticscholar.org/93c2/0e38c85b69fc2d2eb314b3c1217913f7db11.pdf)). 지금부터 구축할 모델에서 입력은 텍스트 문서이며, 우리의 목표는 입력 문서와 스타일이 유사한 새로운 텍스트를 생성할 수 있는 모델을 개발하는 것이다. 이러한 입력의 예로는 특정 프로그래밍 언어의 책 또는 컴퓨터 프로그램이 있다. 문자 수준 언어 모델링에서 입력은 한 번에 하나의 문자를 네트워크에 공급하는 일련의 문자로 분해된다. 네트워크는 다음 문자를 예측하기 위해 이전에 본 문자의 메모리와 함께 각각의 새로운 문자를 처리할 것이다.

* 언어 모델링은 기계가 영어 문장을 생성하는 것과 같은 인간 언어 관련 작업을 수행할 수 있게 해주는 응용 프로그램임.
* 목표는 입력 문서와 스타일이 유사한 새로운 텍스트를 생성할 수 있는 모델을 개발하는 것임.
* 이러한 입력의 예로는 특정 프로그래밍 언어의 책 또는 컴퓨터 프로그램이 있음.
* 문자 수준 언어 모델링에서 입력은 한 번에 하나의 문자를 네트워크에 공급하는 일련의 문자로 분해됨. 네트워크는 다음 문자를 예측하기 위해 이전에 본 문자의 메모리와 함께 각각의 새로운 문자를 처리할 것임.
* 아래 그림은 문자 수준 언어 모델링의 예를 보여줌(EOS는 "순서의 끝").
* 이 구현을 데이터 준비, RNN 모델 구축, 새 텍스트 생성을 위한 다음 문자 예측 및 샘플링의 세 가지 단계로 나눌 수 있음.

![11](https://user-images.githubusercontent.com/113302607/202909949-819f8ecd-352d-4630-8e4a-e34341d7b207.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.11. Character-level language modeling
</figcaption>

## 2. Preprocessing the dataset: 데이터 집합 전처리 

* 이 섹션에서는 문자 수준의 언어 모델링을 위한 데이터를 준비
* 다음 코드를 사용하여 다운로드한 파일에서 직접 텍스트를 읽고 처음과 끝 부분(구텐베르크 프로젝트에 대한 특정 설명 포함)을 제거하고 이 텍스트에서 관찰되는 고유 문자 집합을 나타내는 Python 변수 char_set를 만들 것임.

```python
>>> import numpy as np
>>> ## Reading and processing text
>>> with open('1268-0.txt', 'r', encoding="utf8") as fp:
... text=fp.read()
>>> start_indx = text.find('THE MYSTERIOUS ISLAND')
>>> end_indx = text.find('End of the Project Gutenberg')
>>> text = text[start_indx:end_indx]
>>> char_set = set(text)
>>> print('Total Length:', len(text))
Total Length: 1112350
>>> print('Unique Characters:', len(char_set))
Unique Characters: 80
```

* 텍스트를 다운로드하여 전처리한 후, 총 1,112,350자와 80자의 고유 문자로 구성된 시퀀스가 생성됨.
* 대부분의 NN 라이브러리와 RNN 구현은 문자열 형식의 입력 데이터를 처리할 수 없으므로 텍스트를 숫자 형식으로 변환해야함.
* 이를 위해, 우리는 각 문자를 정수 char2int에 매핑하는 간단한 Python 사전을 만들고 모델의 결과를 텍스트로 다시 변환하려면 역방향 매핑이 필요함.
* 정수 키를 문자 값과 연결하는 사전을 사용하여 역으로 수행할 수 있지만, NumPy 배열을 사용하고 배열을 인덱싱하여 해당 고유 문자에 인덱스를 매핑하는 것이 더 효율적 
* 아래 그림은 문자를 정수로 변환하는 예를 보여주고, "안녕하세요"와 "세계"라는 단어에 대해서는 그 반대의 예를 보여줌.

![12](https://user-images.githubusercontent.com/113302607/202910006-af100523-c41b-4a9c-8ee6-7802b0515390.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.12. Character and integer mappings
</figcaption>

* 이전 그림과 같이 문자를 정수로 매핑하기 위한 사전 구축 및 NumPy 배열 인덱싱을 통한 역매핑은 다음과 같음.

```python
>>> chars_sorted = sorted(char_set)
>>> char2int = {ch:i for i,ch in enumerate(chars_sorted)}
>>> char_array = np.array(chars_sorted)
>>> text_encoded = np.array(
... [char2int[ch] for ch in text],
... dtype=np.int32
... )
>>> print('Text encoded shape:', text_encoded.shape)
Text encoded shape: (1112350,)
>>> print(text[:15], '== Encoding ==>', text_encoded[:15])
>>> print(text_encoded[15:21], '== Reverse ==>',
... ''.join(char_array[text_encoded[15:21]]))
THE MYSTERIOUS == Encoding ==> [44 32 29 1 37 48 43 44 29 42 33 39 45 43 1]
[33 43 36 25 38 28] == Reverse ==> ISLAND
```

* text_encoded NumPy 배열은 텍스트의 모든 문자에 대한 인코딩된 값을 포함함. 이 배열에서 처음 5개 문자의 매핑을 인쇄함.

```python
>>> for ex in text_encoded[:5]:
... print('{} -> {}'.format(ex, char_array[ex]))
44 -> T
32 -> H
29 -> E
1 -> 
37 -> M
```

* 텍스트 생성 작업의 경우 문제를 분류 작업으로 공식화할 수 있음. 아래 그림과 같이 불완전한 텍스트 문자 시퀀스 집합이 있다고 가정함.
* 아래그림에서 왼쪽 상자에 표시된 시퀀스를 입력으로 간주할 수 있음. 
* 새로운 텍스트를 생성하기 위해, 우리의 목표는 입력 시퀀스가 불완전한 텍스트를 나타내는 주어진 입력 시퀀스의 다음 문자를 예측할 수 있는 모델을 설계하는 것임.
* 예를 들어, "딥 러닝"을 본 후 모델은 "i"를 다음 문자로 예측해야함. 80개의 고유한 문자를 가지고 있다는 것을 감안할 때, 이 문제는 다중 클래스 분류 작업이됨.

![13](https://user-images.githubusercontent.com/113302607/202910021-4f61542a-3a89-4024-bdcf-eeed336177cd.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.13. Predicting the next character for a text sequence
</figcaption>


* 길이 1의 시퀀스(즉, 하나의 문자)로 시작하여 아래 그림에 설명된 것처럼 이 다중 클래스 분류 접근법을 기반으로 새로운 텍스트를 반복적으로 생성할 수 있음.

![14](https://user-images.githubusercontent.com/113302607/202910036-9341d7ae-654b-40c7-b3d3-9249cbbb6ce4.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.14. Generating next text based on this multiclass classification approach
</figcaption>




