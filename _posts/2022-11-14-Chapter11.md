---
layout: single
title:  "Chapter11/12/13"
use_math: true
---
* **Chapter11. Implementing a Multilayer Artificial Neural Network from Scratch**
* **Chapter12. Parallelizing Neural Network Training with PyTorch**
* **Chapter13. Going Deeper – The Mechanics of PyTorch**


# Chapter11. Implementing a Multilayer Artificial Neural Network from Scratch

* 딥 러닝은 여러 계층을 가진 인공 신경망(NN)을 효율적으로 훈련하는 것과 관련된 기계 학습의 하위 분야임.
* 인공 NN의 기본 개념을 학습하며, 이미지 및 텍스트 분석에 특히 적합한 고급 파이썬 기반 딥 러닝 라이브러리와 심층 신경망(DNN) 아키텍처를 소개


## 1. Introducing the multilayer neural network architecture

* 이 섹션에서는 여러 단일 뉴런을 다층 feedforward NN에 연결하는 방법에 대해 배움, 완전히 연결된 네트워크의 특별한 유형은 MLP라 부름.


<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.1. A two-layer MLP
</figcaption>

* 위 그림의 MLP는 데이터 입력 옆 숨겨진 레이어와 출력 레이어가 하나씩 존재함. 
* 숨김 계층의 장치는 입력 기능에 완전히 연결되고 출력 계층은 숨김 계층에 완전히 연결됨.
* 이러한 네트워크에 둘 이상의 숨겨진 레이어가 있는 경우, 우리는 그것을 심층 NN이라고 부름.

## 2. Activating a neural network via forward propagation

* 이 섹션에서는 MLP 모델의 출력을 계산하기 위한 순방향 전파 과정에 대해 설명
* MLP 학습 절차를 세 가지 간단한 단계로 요약하면 다음과 같음. 


* 1) 입력 계층에서 시작하여 네트워크를 통해 훈련 데이터의 패턴을 전달하여 출력을 생성
* 2) 네트워크의 출력에 기초하여, 우리는 나중에 설명할 손실 함수를 사용하여 최소화하고자 하는 손실을 계산
* 3) 손실을 역전파하고, 네트워크의 각 가중치 및 바이어스 단위에 대한 도함수를 찾고, 모델을 업데이트

* 마지막으로, 우리는 이 세 가지 단계를 여러 시대에 걸쳐 반복하고 MLP의 가중치 및 바이어스 매개 변수를 학습한 후, 전방 전파를 사용하여 네트워크 출력을 계산하고 임계값 함수를 적용하여 이전 섹션에서 설명한 원-핫 표현에서 예측된 클래스 레이블을 얻음.

* 은닉층의 각 단위는 입력층의 모든 단위와 연결되어 있기 때문에, 먼저 다음과 같이 은닉층의 활성화 단위를 계산함.
$$z_{1}^{(h)}=x_{1}^{(in)}w_{1,1}^{(h)}+x_{2}^{(in)}w_{1,2}^{(h)}+...+x_{m}^{(in)}w_{1,m}^{(h)}$$
$$a_{1}^{(h)}=\sigma(z_{1}^{(h)})$$

* 여기서, $z_{1}^{(h)}$는 순 입력을 의미하고 $\sigma$는 활성화 함수를 의미함. 
* 이미지 분류와 같은 복잡한 문제를 해결하기 위해 MLP 모델에서 비선형 활성화 기능을 필요로함. 
* 예를 들어 Scikit-Learn을 사용한 기계 학습 분류기 둘러보기 3장의 로지스틱 회귀에 대한 섹션에서 기억하는 Sigmoid(로지스틱) 활성화 기능이 필요

$$\sigma(z) =  \frac{1}{1+e^{-z}} $$


<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.2. The sigmoid activation function
</figcaption>

* S자형 함수는 위 그림과 같이 0에서 1까지의 범위의 로지스틱 분포에 순 입력 z를 매핑하는 S자형 곡선으로, z = 0에서 y축을 절단함.
* MLP는 feedforward artificial NN(피드포워드 인공NN)의 전형적인 예임. 
* 피드포워드라는 용어는 각 계층이 루프 없이 다음 계층에 대한 입력 역할을 한다는 사실을 의미
* 다층 퍼셉트론이라는 용어는 이 네트워크 아키텍처에서 인공 뉴런은 퍼셉트론이 아닌 일반적으로 시그모이드 단위이기 때문에 약간 혼란스럽게 들릴 수 있음.
* MLP의 뉴런을 0과 1 사이의 연속적인 범위에서 값을 반환하는 로지스틱 회귀 단위로 생각할 수 있음.

## 3. Classifying handwritten digits

#### Obtaining and preparing the MNIST dataset

* MNIST 데이터 세트는 미국 국립표준기술원(NIST)의 두 데이터 세트에서 구성,교육 데이터 세트는 250명의 다른 사람들의 손으로 쓴 숫자, 50 퍼센트의 고등학생, 그리고 50 퍼센트의 인구 조사국의 직원들로 구성되어 있음.
* 테스트 데이터 세트에는 동일한 분할 후에 다른 사람의 손으로 쓴 숫자가 포함되어 있음.
* 데이터 세트 파일을 다운로드하여 NumPy 배열로 전처리하는 대신, 우리는 MNIST 데이터 세트를 더 편리하게 로드할 수 있는 skickit-learn의 새로운 fetch_openml 함수를 사용함.

```python
>>> from sklearn.datasets import fetch_openml
>>> X, y = fetch_openml('mnist_784', version=1,
... return_X_y=True)
>>> X = X.values
>>> y = y.astype(int).values
```

* MNIST의 이미지가 어떻게 보이는지에 대한 아이디어를 얻기 위해, Matplotlib의 imshow 함수를 통해 플롯할 수 있는 원래 28x28 이미지로 784 픽셀 벡터를 재구성한 후 숫자 0-9의 예를 시각화하면 아래 그림과 같음.

```python
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(nrows=2, ncols=5,
... sharex=True, sharey=True)
>>> ax = ax.flatten()
>>> for i in range(10):
... img = X[y == i][0].reshape(28, 28)
... ax[i].imshow(img, cmap='Greys')
>>> ax[0].set_xticks([])
>>> ax[0].set_yticks([])
>>> plt.tight_layout()
>>> plt.show()
```

3.1
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.3. A plot showing one randomly chosen handwritten digit from each class
</figcaption>

* 게다가, 같은 숫자의 여러 예시를 그려서 각각의 글씨가 실제로 얼마나 다른지는 다음과 같음.

```python
>>> fig, ax = plt.subplots(nrows=5,
... ncols=5,
... sharex=True,
... sharey=True)
>>> ax = ax.flatten()
>>> for i in range(25):
... img = X[y == 7][i].reshape(28, 28)
... ax[i].imshow(img, cmap='Greys')
>>> ax[0].set_xticks([])
>>> ax[0].set_yticks([])
>>> plt.tight_layout()
>>> plt.show()
```

4.1
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.4. Different variants of the handwritten digit 7
</figcaption>


## 4. Coding the neural network training loop

* 이번 섹션에서는 모델을 훈련하는 내용을 포함함. 
* 먼저 데이터 로드를 위한 몇 가지 기능을 정의하고 이 기능을 여러 루프의 데이터 세트에 걸쳐 반복되는 훈련 루프에 내장할 것임. 

* 우리가 정의할 첫 번째 기능은 미니 배치 생성기로, 데이터 세트를 가져와서 확률적 경사 하강 훈련을 위해 원하는 크기의 미니 배치로 나누며 코드는 다음과 같음.

```python
>>> import numpy as np
>>> num_epochs = 50
>>> minibatch_size = 100
>>> def minibatch_generator(X, y, minibatch_size):
... indices = np.arange(X.shape[0])
... np.random.shuffle(indices)
... for start_idx in range(0, indices.shape[0] - minibatch_size
... + 1, minibatch_size):
... batch_idx = indices[start_idx:start_idx + minibatch_size]
... yield X[batch_idx], y[batch_idx]
```

* 다음으로, 우리는 훈련 과정을 모니터링하고 모델을 평가하는 데 사용할 수 있는 손실 함수와 성능 메트릭을 정의, MSE 손실 및 정확도 기능은 다음과 같이 구현

```python
>>> def mse_loss(targets, probas, num_labels=10):
... onehot_targets = int_to_onehot(
... targets, num_labels=num_labels
... )
... return np.mean((onehot_targets - probas)**2)
>>> def accuracy(targets, predicted_labels):
... return np.mean(predicted_labels == targets)
```

* 이전 기능을 테스트하고 이전 섹션에서 인스턴스화한 모델의 초기 검증 세트 MSE 및 정확성을 계산함.

```python
>>> _, probas = model.forward(X_valid)
>>> mse = mse_loss(y_valid, probas)
>>> print(f'Initial validation MSE: {mse:.1f}')
Initial validation MSE: 0.3
>>> predicted_labels = np.argmax(probas, axis=1)
>>> acc = accuracy(y_valid, predicted_labels)
>>> print(f'Initial validation accuracy: {acc*100:.1f}%')
Initial validation accuracy: 9.4%
```

## 5. Evaluating the neural network performance

* 역전파, NN의 훈련 절차 논의 이전에 훈련한 모델의 성능에 대해 살펴봄.
* train()에서 Matplotlib를 사용하여 결과를 시각화할 수 있도록 각 epoch의 훈련 손실 및 검증 정확도를 수집.
* 먼저 MSE 훈련 손실에 대해 살펴봄. 

```python
>>> plt.plot(range(len(epoch_loss)), epoch_loss)
>>> plt.ylabel('Mean squared error')
>>> plt.xlabel('Epoch')
>>> plt.show()
```

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.5. A plot of the MSE by the number of training epochs
</figcaption>

* 은 처음 10 에폭 동안 상당히 감소했고 마지막 10 에폭 동안 서서히 수렴되는 것처럼 보임.
* 그러나 에폭 40과 에폭 50 사이의 작은 기울기는 추가 에폭에 대한 훈련으로 손실이 더 줄어들 것임을 나타냄

* 다음으로 교육 및 검증 정확도에 대해 살펴봄.

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.6. Classification accuracy by the number of training epochs
</figcaption>

* 위 그림은 훈련을 할 수록 훈련과 검증 정확도 사이의 차이가 커진다는 것을 보여줌. 
* 대략 25 에폭에서 훈련과 검증의 정확도 값이 거의 같으며, 그 후 네트워크는 훈련 데이터에 과적합하기 시작함. 
* 마지막으로 시험데이터베이스에 대한 예측정확도를 계산하여 모델의 일반화 성능을 평가해봄.

```python
>>> test_mse, test_acc = compute_mse_and_acc(model, X_test, y_test)
>>> print(f'Test accuracy: {test_acc*100:.2f}%')
Test accuracy: 94.51%
```

* 테스트 정확도가 마지막 하위 섹션에서 교육 중에 보고한 마지막 에폭(94.74%)에 해당하는 검증 세트 정확도에 매우 가깝다는 것을 알 수 있음.
* 또한, 각각의 훈련 정확도는 95.59%로 최소로 더 높을 뿐이며, 이는 모델이 훈련 데이터에 약간만 적합하다는 것을 재확인함.

## 6.Training neural networks via backpropagation

* 이 섹션에서는 NN에서 가중치를 매우 효율적으로 학습할 수 있는 방법을 이해하기 위해 역전파의 수학을 살펴봄.
* 출력 레이어의 활성화를 얻기 위해 순방향 전파를 적용해야 하며, 이를 아래와 같이 공식화함.
* $Z^{(h)} = X^{(h)T}+b^{(h)}$ (net input of the hidden layer; 숨겨진 계층의 순 입력)
* $A^{(h)} =  \sigma Z^{(h)}$ (activation of the hidden layer; 은닉층의 활성화)
* $Z^{(out)} = A^{(h)}W^{(out)T}+b^{(out)}$ (net input of the output layer; 출력 계층의 순 입력)
* $A^{(out)} =  \sigma Z^{(out)}$ (activation of the output layer; 출력 계층 활성화)
* 간단히 말해서, 우리는 2개의 입력 기능, 3개의 숨겨진 노드, 2개의 출력 노드가 있는 네트워크의 경우 아래 그림의 화살표와 같이 네트워크의 연결을 통해 입력 기능을 전달하기만 하면됨.

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.7. Forward-propagating the input features of an NN
</figcaption>

* 역 전파에서, 우리는 오류를 오른쪽에서 왼쪽으로 전파하며 이것을 모델 가중치(및 바이어스 단위)에 대한 손실의 기울기를 계산하기 위해 전진 패스의 계산에 체인 규칙을 적용하는 것으로 생각할 수 있음.
* 역 전파하는 계산 경로는 아래의 굵은 화살표를 통해 강조 표시됨.

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.8. Backpropagating the error of an NN
</figcaption>


# Chapter12. Parallelizing Neural Network Training with PyTorch
* PyTorch는 현재 사용 가능한 가장 인기 있는 딥 러닝 라이브러리 중 하나임.
* 파이토치를 통해 이전의 NumPy 구현보다 훨씬 더 효율적으로 신경망(NN)을 구현할 수 있음. 
* 이 챕터에서는 PyTorch를 사용하여 교육 성과에 어떤 이점을 제공하는지 살펴봄. 

## 1. PyTorch and training performance

* PyTorch는 기계 학습 작업의 속도를 크게 높일 수 있음. 

#### What is PyTorch?

* PyTorch는 딥러닝 및 머신러닝 알고리즘을 구현하고 실행하기 위한 확장 가능한 멀티 플랫폼 프로그래밍 인터페이스를 의미함.
* 파이토치는 주로 페이스북 AI 리서치(FAIR) 연구소의 연구원들과 엔지니어들에 의해 개발되었음.

## 1. First steps with PyTorch

#### Installing PyTorch
#### Creating tensors in PyTorch

# Chapter13. Going Deeper – The Mechanics of PyTorch

* 이 장에서는 NN을 구현하기 위해 PyTorch의 API의 다양한 측면을 사용할 것임.
* 특히 표준 아키텍처의 구현을 매우 편리하게 하기 위해 여러 계층의 추상화를 제공하는 torch.nn 모듈을 다시 사용함.
* torch.nn 모듈을 사용하여 모델을 구축하는 다양한 방법을 설명하기 위해, 고전적인 배타적(XOR) 문제를 고려함. 
* 첫째, 순차 클래스를 사용하여 다층 퍼셉트론을 구축
* 그런 다음, 하위 분류 nn과 같은 다른 방법을 고려(사용자 지정 계층을 정의하는 모듈)
* 마지막으로, 원시 입력에서 예측에 이르는 기계 학습 단계를 다루는 두 가지 실제 프로젝트를 수행


#### Understanding computation graphs

* PyTorch는 핵심에 계산 그래프를 구축하는 데 의존하며, 이 계산 그래프를 사용하여 입력에서 출력까지 텐서 간의 관계를 도출함.
* 0(scalar) 텐서 a,b,c를 가지고 있으며 z=2*(a-b)+c를 평가한다고 가정할 때, 이 평가는 아래 그림과 같이 계산 그래프로 나타낼 수 있음. 

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.5. How a computation graph works
</figcaption>

* 위 그림의 계산 그래프는 단순한 노드 네트워크임. 각 노드는 입력 텐서 또는 텐서에 함수를 적용하고 출력으로 0개 이상의 텐서를 반환하는 연산임. 
* PyTorch는 이 계산 그래프를 만들고 그에 따라 기울기를 계산하는 데 사용함.

#### Creating a graph in PyTorch

* 이전 그림과 같이 z=2*(a-b)+c를 평가하기 위해 pytorch에서 그래프를 만드는 방법을 설명하는 간단한 예제를 살펴봄. 변수 a,b,c는 스칼라이며 이것을 단순히 a,b,c 입력 인수로 하는 정규 파이썬 함수를 정의함.  

```python
>>> import torch
>>> def compute_z(a, b, c):
... r1 = torch.sub(a, b)
... r2 = torch.mul(r1, 2)
... z = torch.add(r2, c)
... return z
```

* 이제 계산을 수행하기 위해 텐서 객체를 함수 인수로 사용하여 이 함수를 간단히 호출함.
* 덧셈, 서브(또는 빼기), 멀티(또는 곱하기)와 같은 PyTorch 함수는 또한 PyTorch 텐서 객체의 형태로 더 높은 순위의 입력을 제공할 수 있다는 점에 유의
* 다음 코드 예제에서는 스칼라 입력(순위 0)과 순위 1 및 순위 2 입력을 목록으로 제공

```python
>>> print('Scalar Inputs:', compute_z(torch.tensor(1),
... torch.tensor(2), torch.tensor(3)))
Scalar Inputs: tensor(1)
>>> print('Rank 1 Inputs:', compute_z(torch.tensor([1]),
... torch.tensor([2]), torch.tensor([3])))
Rank 1 Inputs: tensor([1])
>>> print('Rank 2 Inputs:', compute_z(torch.tensor([[1]]),
... torch.tensor([[2]]), torch.tensor([[3]])))
Rank 2 Inputs: tensor([[1]])
```












