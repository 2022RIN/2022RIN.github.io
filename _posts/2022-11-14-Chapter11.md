---
layout: single
title:  "Chapter11/12/13"
use_math: true
---
* **Chapter11. Implementing a Multilayer Artificial Neural Network from Scratch**
* **Chapter12. Parallelizing Neural Network Training with PyTorch**
* **Chapter13. Going Deeper – The Mechanics of PyTorch**


# Chapter11. Implementing a Multilayer Artificial Neural Network from Scratch

* 딥 러닝은 여러 계층을 가진 인공 신경망(NN)을 효율적으로 훈련하는 것과 관련된 기계 학습의 하위 분야임.

## 1. Introducing the multilayer neural network architecture

* 이 섹션에서는 여러 단일 뉴런을 다층 feedforward NN에 연결하는 방법에 대해 배움, 완전히 연결된 네트워크의 특별한 유형은 MLP라 부름.


<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.1. A two-layer MLP
</figcaption>

* 위 그림의 MLP는 데이터 입력 옆 숨겨진 레이어와 출력 레이어가 하나씩 존재함. 
* 숨김 계층의 장치는 입력 기능에 완전히 연결되고 출력 계층은 숨김 계층에 완전히 연결됨.
* 이러한 네트워크에 둘 이상의 숨겨진 레이어가 있는 경우, 우리는 그것을 심층 NN이라고 부름.

## 2. Activating a neural network via forward propagation

* 이 섹션에서는 MLP 모델의 출력을 계산하기 위한 순방향 전파 과정에 대해 설명
* MLP 학습 절차를 세 가지 간단한 단계로 요약하면 다음과 같음. 
* 1) 입력 계층에서 시작하여 네트워크를 통해 훈련 데이터의 패턴을 전달하여 출력을 생성
* 2) 네트워크의 출력에 기초하여, 우리는 나중에 설명할 손실 함수를 사용하여 최소화하고자 하는 손실을 계산
* 3) 손실을 역전파하고, 네트워크의 각 가중치 및 바이어스 단위에 대한 도함수를 찾고, 모델을 업데이트

* 마지막으로, 우리는 이 세 가지 단계를 여러 시대에 걸쳐 반복하고 MLP의 가중치 및 바이어스 매개 변수를 학습한 후, 전방 전파를 사용하여 네트워크 출력을 계산하고 임계값 함수를 적용하여 이전 섹션에서 설명한 원-핫 표현에서 예측된 클래스 레이블을 얻음.

* 은닉층의 각 단위는 입력층의 모든 단위와 연결되어 있기 때문에, 먼저 다음과 같이 은닉층의 활성화 단위를 계산함.
$$z_{1}^{(h)}=x_{1}^{(in)}w_{1,1}^{(h)}+x_{2}^{(in)}w_{1,2}^{(h)}+...+x_{m}^{(in)}w_{1,m}^{(h)}$$
$$a_{1}^{(h)}=\sigma(z_{1}^{(h)})$$

* 여기서, $z_{1}^{(h)}$는 순 입력을 의미하고 $\sigma$는 활성화 함수를 의미함. 
* 이미지 분류와 같은 복잡한 문제를 해결하기 위해 MLP 모델에서 비선형 활성화 기능을 필요로함. 
* 예를 들어 Scikit-Learn을 사용한 기계 학습 분류기 둘러보기 3장의 로지스틱 회귀에 대한 섹션에서 기억하는 Sigmoid(로지스틱) 활성화 기능이 필요

$$\sigma(z) =  \frac{1}{1+e^{-z}} $$


<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.2. The sigmoid activation function
</figcaption>

* S자형 함수는 위 그림과 같이 0에서 1까지의 범위의 로지스틱 분포에 순 입력 z를 매핑하는 S자형 곡선으로, z = 0에서 y축을 절단함.
* MLP는 feedforward artificial NN(피드포워드 인공NN)의 전형적인 예임. 
* 피드포워드라는 용어는 각 계층이 루프 없이 다음 계층에 대한 입력 역할을 한다는 사실을 의미
* 다층 퍼셉트론이라는 용어는 이 네트워크 아키텍처에서 인공 뉴런은 퍼셉트론이 아닌 일반적으로 시그모이드 단위이기 때문에 약간 혼란스럽게 들릴 수 있음.
* MLP의 뉴런을 0과 1 사이의 연속적인 범위에서 값을 반환하는 로지스틱 회귀 단위로 생각할 수 있음.

## 3. Coding the neural network training loop

* 이번 섹션에서는 모델을 훈련하는 내용을 포함함. 
* 먼저 데이터 로드를 위한 몇 가지 기능을 정의하고 이 기능을 여러 루프의 데이터 세트에 걸쳐 반복되는 훈련 루프에 내장할 것임. 

* 우리가 정의할 첫 번째 기능은 미니 배치 생성기로, 데이터 세트를 가져와서 확률적 경사 하강 훈련을 위해 원하는 크기의 미니 배치로 나누며 코드는 다음과 같음.

```python
>>> import numpy as np
>>> num_epochs = 50
>>> minibatch_size = 100
>>> def minibatch_generator(X, y, minibatch_size):
... indices = np.arange(X.shape[0])
... np.random.shuffle(indices)
... for start_idx in range(0, indices.shape[0] - minibatch_size
... + 1, minibatch_size):
... batch_idx = indices[start_idx:start_idx + minibatch_size]
... yield X[batch_idx], y[batch_idx]
```

* 다음으로, 우리는 훈련 과정을 모니터링하고 모델을 평가하는 데 사용할 수 있는 손실 함수와 성능 메트릭을 정의, MSE 손실 및 정확도 기능은 다음과 같이 구현

```python
>>> def mse_loss(targets, probas, num_labels=10):
... onehot_targets = int_to_onehot(
... targets, num_labels=num_labels
... )
... return np.mean((onehot_targets - probas)**2)
>>> def accuracy(targets, predicted_labels):
... return np.mean(predicted_labels == targets)
```

* 이전 기능을 테스트하고 이전 섹션에서 인스턴스화한 모델의 초기 검증 세트 MSE 및 정확성을 계산함.

```python
>>> _, probas = model.forward(X_valid)
>>> mse = mse_loss(y_valid, probas)
>>> print(f'Initial validation MSE: {mse:.1f}')
Initial validation MSE: 0.3
>>> predicted_labels = np.argmax(probas, axis=1)
>>> acc = accuracy(y_valid, predicted_labels)
>>> print(f'Initial validation accuracy: {acc*100:.1f}%')
Initial validation accuracy: 9.4%
```

## 4. Evaluating the neural network performance

* 역전파, NN의 훈련 절차 논의 이전에 훈련한 모델의 성능에 대해 살펴봄.
* train()에서 Matplotlib를 사용하여 결과를 시각화할 수 있도록 각 epoch의 훈련 손실 및 검증 정확도를 수집.
* 먼저 MSE 훈련 손실에 대해 살펴봄. 

```python
>>> plt.plot(range(len(epoch_loss)), epoch_loss)
>>> plt.ylabel('Mean squared error')
>>> plt.xlabel('Epoch')
>>> plt.show()
```

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.3. A plot of the MSE by the number of training epochs
</figcaption>

* 은 처음 10 에폭 동안 상당히 감소했고 마지막 10 에폭 동안 서서히 수렴되는 것처럼 보임.
* 그러나 에폭 40과 에폭 50 사이의 작은 기울기는 추가 에폭에 대한 훈련으로 손실이 더 줄어들 것임을 나타냄

* 다음으로 교육 및 검증 정확도에 대해 살펴봄.

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.4. Classification accuracy by the number of training epochs
</figcaption>

* 위 그림은 훈련을 할 수록 훈련과 검증 정확도 사이의 차이가 커진다는 것을 보여줌. 
* 대략 25 에폭에서 훈련과 검증의 정확도 값이 거의 같으며, 그 후 네트워크는 훈련 데이터에 과적합하기 시작함. 

# Chapter12. Parallelizing Neural Network Training with PyTorch
* PyTorch는 현재 사용 가능한 가장 인기 있는 딥 러닝 라이브러리 중 하나임.
* 파이토치를 통해 이전의 NumPy 구현보다 훨씬 더 효율적으로 신경망(NN)을 구현할 수 있음. 
* 이 챕터에서는 PyTorch를 사용하여 교육 성과에 어떤 이점을 제공하는지 살펴봄. 


## 1. PyTorch and training performance

* PyTorch는 기계 학습 작업의 속도를 크게 높일 수 있음. 

#### What is PyTorch?

* PyTorch는 딥러닝 및 머신러닝 알고리즘을 구현하고 실행하기 위한 확장 가능한 멀티 플랫폼 프로그래밍 인터페이스를 의미함.
* 파이토치는 주로 페이스북 AI 리서치(FAIR) 연구소의 연구원들과 엔지니어들에 의해 개발되었음.


## 1. First steps with PyTorch

#### Installing PyTorch


# Chapter13. Going Deeper – The Mechanics of PyTorch






