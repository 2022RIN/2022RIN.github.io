---
layout: single
title:  "Chapter6. Learning Best Practices for Model Evaluation and Hyperparameter Tuning"
use_math: true
---
## 1. Streamlining workflows with pipelines 

* 이 섹션에서는 sickit-learn의 Pipeline 클래스에 대해 배움. 

#### * The main steps in principal component analysis

* 위스콘신 유방암 데이터 세트는 569개의 악성 및 양성 종양 샘플이 포함되어있음. 두 열은 샘플의 진단(M = 악성, B = 양성)의 고유한 ID 번호를 저장하며, 3~32 열에는 세포 핵의 이미지에서 계산된 30개의 실제 값 특징이 포함됨.

1. 먼저 UCI 웹 사이트에서 판다를 사용하여 데이터 세트를 읽어옴.

```python
>> import pandas as pd
>>> df = pd.read_csv('https://archive.ics.uci.edu/ml/'
... 'machine-learning-databases'
... '/breast-cancer-wisconsin/wdbc.data',
... header=None)
```

2. 다음으로 30개의 특성을 NumPy 배열 X에 할당하고, LabelEncoder 객체를 사용하여 클래스 레이블을 원래 문자열 표현('M' 및 'B')에서 정수로 변환함.

```python
>>> from sklearn.preprocessing import LabelEncoder
>>> X = df.loc[:, 2:].values
>>> y = df.loc[:, 1].values
>>> le = LabelEncoder()
>>> y = le.fit_transform(y)
>>> le.classes_
array(['B', 'M'], dtype=object)
```

3. 배열에서 클래스 레이블(진단)을 인코딩한 후 y, 악성 종양은 클래스 1로, 양성 종양은 클래스 0으로 각각 표시되며, 두 개의 더미 클래스 레이블에 장착된 LabelEncoder의 변환 방법을 호출하여 이 매핑을 다시 확인할 수 있음.

```python
>> le.transform(['M', 'B'])
array([1, 0])
```

4. 다음 섹션에서 첫 번째 모델 파이프라인을 구축하기 전에 데이터 세트를 별도의 교육 데이터 세트(80%)와 별도의 테스트 데이터 세트(20%)로 나눔.

```python
>>> from sklearn.model_selection import train_test_split
>>> X_train, X_test, y_train, y_test = \
... train_test_split(X, y,
... test_size=0.20,
... stratify=y,
... random_state=1)
```

#### * Combining transformers and estimators in a pipeline
* 이전 챕터들에서 많은 학습 알고리즘에게서 최적의 성능을 끌어내기 위해 입력 특성은 동일한 규모를 가져야함.이에따라 위스콘신 유방암 데이터 세트도 특성 표준화가 필요함. 
* 5장에서 소개한 PCA를 통해 30차원의 데이터를 2차원의 부분공간으로 압축한다고 가정함. 

* 훈련 및 테스트 데이터 세트에 대한 학습 및 데이터 변환 단계를 별도로 거치지 않고 StandardScaler, PCA 및 LogisticRegression 개체를 Pipeline에 연결할 수 있음. 

```python
>>> from sklearn.preprocessing import StandardScaler
>>> from sklearn.decomposition import PCA
>>> from sklearn.linear_model import LogisticRegression
>>> from sklearn.pipeline import make_pipeline
>>> pipe_lr = make_pipeline(StandardScaler(),
... PCA(n_components=2),
... LogisticRegression())
>>> pipe_lr.fit(X_train, y_train)
>>> y_pred = pipe_lr.predict(X_test)
>>> test_acc = pipe_lr.score(X_test, y_test)
>>> print(f'Test accuracy: {test_acc:.3f}')
Test accuracy: 0.956
```
* **위 코드 분석**
* **make_pipeline**: 임의의 수의 사이킷런 변환기(fit 및 transform메서드 입력으로 지원하는 객체)를 취한 다음, 사이킷런 추정기(fit 및 predict메서드을 입력으로 구현하는 객체)를 연결시켜줌. 
* 위 코드에서는 두 개의 변환기 **StandardScaler** 및 **PCA**와 **LogisticRegression** 추정기를 **make_pipeline**의 입력으로 제공하였으며, 이 함수는 사이킷런의 **Pipeline** 클래스 객체를 생성하여 반환해줌. 
* 사이킷런의 **Pipeline** 클래스를 변압기와 추정기 주변의 메타 추정기(meta-estimator) 또는 랩로 생각할 수 있음.
* **Pipeline** 의 fit 메서드를 호출하면 **StandardScaler** 의 fit 및 transform메서드를 호출함. 이는 다음 **PCA**에 전달되며 **PCA** 의 fit 및 transform메서드를 호출하며 이는 추정기에 데이터가 전달됨.
* 
* 위 코드에서 pipe_lr 파이프라인에 fit 메서드를 실행했을 때, StandardScaler는 훈련 데이터에 대해 fit 및 transform 호출을 처음 수행하였음. 이렇게 변환된 훈련 데이터는 다음 개체인 PCA로 전달되었음. 
* 이제 **LogisticRegression** 은 이를 토대로 학습을 진행함. **여기서, 파이프라인 중간 단계 개수는 제한이 없지만 마지막 파이프라인 요소는 추정기가 되어야함.**
* 파이프라인의 마지막 단계가 추정기인 경우, 파이프라인도 **predict** 메서드를 구현함. **predict** 메서드 호출에 데이터 세트를 제공하는 경우 데이터는 transform메서드 호출을 통해 중간단계를 통과하고, 마지막 단계에서 추정기 개체가 변환된 데이터에 대한 **predict** 을 반환함. 

fig1

## 2. Using k-fold cross-validation to assess model performance

#### * The holdout method

* 기계 학습 모델의 일반화 성능을 추정하기 위한 고전적이고 대중적인 접근 방식은 홀드아웃 방법이 있음. 
* 홀드아웃 방법을 사용하여 초기 데이터 세트를 별도의 훈련 및 테스트 데이터 세트로 분할함. 전자는 모델 훈련에 사용되며 후자는 일반화 성능을 추정하는데 사용됨. 
* 일반적인 기계 학습 응용 프로그램에서 우리는 처음보는 데이터에 대한 예측 성능을 더욱 향상시키기 위해 하이퍼파라미터(hyperparameters)를 튜닝함. 이 프로세스를 **모델 선택(model selection)** 이라함. 
* 그러나 모델 선택 중에 동일한 테스트 데이터 세트를 반복적으로 재사용하면 훈련 데이터의 일부가 되어 모델이 overfit될 가능성이 높아짐.
* 모델 선택에 홀드아웃 방법을 사용하는 더 나은 방법은 데이터를 훈련 데이터 세트, 검증 데이터(validation dataset) 세트 및 테스트 데이터 세트의 세 부분으로 분리하는 방법임.
* 훈련 데이터 세트는 다양한 모델에 적합하도록 사용되며, 검증 데이터 세트의 성능은 모델 선택에 사용됨.
* 훈련 및 모델 선택 단계에서 모델이 이전에 보지 못한 테스트 데이터 세트를 사용하는 것은 새로운 데이터로 일반화하는 능력에 대한 편견이 덜한 추정치를 얻을 수 있다는 이점이 있음.

fig2

* 위 그림은 서로 다른 초 매개 변수 값을 사용하여 훈련 후 모델의 성능을 반복적으로 평가하기 위해 검증 데이터 세트를 사용하는 홀드아웃 교차 검증(holdout cross-validation)의 개념을 보여줌. 하이퍼파라미터 값의 조정에 만족하면 테스트 데이터 세트에서 모델의 일반화 성능을 추정함.
* 홀드아웃 방법의 단점: 성능 추정치가 훈련 데이터 세트를 훈련 및 검증 하위 집합으로 분할하는 방법에 민감하다는 점임.
* 추정치는 데이터의 예에 따라 달라짐. 다음 하위 섹션에서 k개의 훈련 데이터 하위 집합에 대해 k번의 홀드아웃 방법을 반복하는 k번의 교차 검증, 성능 추정을 위한 보다 강력한 기술을 소개함.

#### * K-fold cross-validation

* K-폴드 교차 검증에서, 대체 없이 무작위로 훈련 데이터 세트를 k폴드로 분할함.
* 여기서 모델 훈련에는 training folds라고 불리는 k-1개의 폴드가 사용되고, 성능 평가에는 test fold라고 불리는 1개의 폴드가 사용됨. 이 절차는 k번 반복되기 때문에, k개의 모델과 성능 추정치를 얻음.
* 그런 다음 서로 다른 독립적 test fold를 기반으로 모델의 평균 성능을 계산하기 때문에 홀드아웃 방법에 비해 교육 데이터의 하위 분할에 덜 민감한 성능 추정치를 얻음.
* 일반적으로 모델 튜닝을 위해 k-폴드 교차 검증을 사용함. 즉, test fold에서 모델 성능을 평가하여 추정한 만족스러운 일반화 성능을 산출하는 최적의 하이퍼파라미터 값을 찾음.
* 하이퍼파라미터 값을 찾으면 전체 훈련 데이터 세트에서 모델을 재교육하고 독립 테스트 데이터 세트를 사용하여 최종 성능 추정차를 얻음.
* k-폴드 교차 검증 후 전체 훈련 데이터 세트에 모델을 맞추는 이유:
    * 첫째, 일반적으로 단일 최종 모델(개별 모델 대비)에 관심이 있음.
    * 둘째, 학습 알고리듬에 더 많은 교육 예를 제공함으로써 일반적으로 더 정확하고 강력한 모델을 얻을 수 있음.

* k-폴드 교차 검증은 대체되지 않은 리샘플링 기법이기 때문에, 각 반복에서 각 예제가 정확히 한 번 사용되고 training folds, test fold는 분리된다는 장점이 있음. (즉, fold 사이 겹침이 없음.)


fig3

* 위 그림은 k=10을 사용한 k-폴드 교차 검증의 개념을 요약함. 훈련 데이터 세트는 10개의 fold로 나뉘며, 10번의 반복 동안 9개의 fold는 훈련에, 1개는 평가를 위한 테스트 세트로 사용됨.
* 또한 각 접힘에 대한 추정 성능 $E_{i}$(예: 분류 정확도 또는 오류)를 사용하여 모형의 추정 평균 성능 $E$를 계산함.
* k-폴드 교차 검증에서는 모든 데이터 포인트가 평가에 사용되기 때문에 k-폴드 교차 검증은 검증 세트가 있는 홀드아웃 방법보다 데이터 세트를 더 잘 사용함.
* Ron Kohavi의 실험과 경험적 증거를 통해, k-fold 교차 검증의 좋은 표준 값은 10임.
* 그러나 비교적 작은 훈련 세트를 사용하는 경우 fold 수를 늘리는 것이 유용할 수 있음. k의 값을 높이면 각 반복에 더 많은 훈련 데이터가 사용되므로 개별 모델 추정치를 평균하여 일반화 성능을 추정하는 비관적 편향이 낮아짐. 그러나 k 값이 크면 교차 검증 알고리듬의 런타임도 증가하고 훈련 fold 값이 서로 더 비슷하기 때문에 분산이 더 높은 추정치를 산출할 수 있으며, 대규모 데이터 세트를 사용하는 경우, k = 5와 같은 더 작은 값 포크를 선택할 수 있으며, 다른 fold에서 모델을 재장착하고 평가하는 계산 비용을 줄이면서 모델의 평균 성능에 대한 정확한 추정치를 얻을 수 있음.


* 표준 k-폴드 교차 검증(standard k-fold cross-validation) 접근법에 비해 약간 개선된 것은 계층화된 k-폴드 교차 검증(stratified k-fold cross-val-idation)으로, 특히 클래스 비율이 같지 않은 경우 더 나은 편향 및 분산 추정치를 산출할 수 있어 더 유용함.
* 계층화 교차 검증에서 클래스 레이블 비율은 각 폴드에 보존되어 각 폴드가 훈련 데이터 세트의 클래스 비율을 대표하도록 보장함.

```python
>>> import numpy as np
>>> from sklearn.model_selection import StratifiedKFold
>>> kfold = StratifiedKFold(n_splits=10).split(X_train, y_train)
>>> scores = []
>>> for k, (train, test) in enumerate(kfold):
... pipe_lr.fit(X_train[train], y_train[train])
... score = pipe_lr.score(X_train[test], y_train[test])
... scores.append(score)
... print(f'Fold: {k+1:02d}, '
... f'Class distr.: {np.bincount(y_train[train])}, '
... f'Acc.: {score:.3f}')
Fold: 01, Class distr.: [256 153], Acc.: 0.935
Fold: 02, Class distr.: [256 153], Acc.: 0.935
Fold: 03, Class distr.: [256 153], Acc.: 0.957
Fold: 04, Class distr.: [256 153], Acc.: 0.957
Fold: 05, Class distr.: [256 153], Acc.: 0.935
Fold: 06, Class distr.: [257 153], Acc.: 0.956
Fold: 07, Class distr.: [257 153], Acc.: 0.978
Fold: 08, Class distr.: [257 153], Acc.: 0.933
Fold: 09, Class distr.: [257 153], Acc.: 0.956
Fold: 10, Class distr.: [257 153], Acc.: 0.956
>>> mean_acc = np.mean(scores)
>>> std_acc = np.std(scores)
>>> print(f'\nCV accuracy: {mean_acc:.3f} +/- {std_acc:.3f}')
CV accuracy: 0.950 +/- 0.014
```

* 위 코드에서, 계층화된 것을 초기화하고, 훈련 데이터 세트에서 y_train 클래스 레이블이 있는 sklearn.model_selection 모듈의 KFold 반복기이며, n_splits 매개 변수를 통해 fold 수를 지정하였음.
* kfold를 반복할 때, 훈련에서 반환된 인덱스를 사용하여 이 장의 시작 부분에서 설정한 로지스틱 회귀 파이프라인에 맞춤.
* pipe_lr 파이프라인을 사용하여 각 반복에서 예제가 적절하게 조정(예: 표준화)되었는지 확인하였음.
* 그런 다음 테스트 지수를 사용하여 모델의 정확도 점수를 계산했고, 이를 점수 목록에 수집하여 평균 정확도와 추정치의 표준 편차를 계산함.

```python
>>> from sklearn.model_selection import cross_val_score
>>> scores = cross_val_score(estimator=pipe_lr,
... X=X_train,
... y=y_train,
... cv=10,
... n_jobs=1)
>>> print(f'CV accuracy scores: {scores}')
CV accuracy scores: [ 0.93478261 0.93478261 0.95652174
 0.95652174 0.93478261 0.95555556
 0.97777778 0.93333333 0.95555556
 0.95555556]
>>> print(f'CV accuracy: {np.mean(scores):.3f} '
... f'+/- {np.std(scores):.3f}')
CV accuracy: 0.950 +/- 0.014
```
* 이전 코드 예제는 k-폴드 교차 검증이 작동하는 방식을 설명하는 데 유용했지만, 위 코드와 같이 skickit-learn은 k-폴드 교차 검증 점수를 구현하여 계층화된 k-폴드 교차 검증을 사용하여 모델을 쉽게 평가할 수 있음.


## 3. Debugging algorithms with learning and validation curves

* 이 섹션에서는 학습 알고리즘의 성능을 향상시키는 데 도움이 될 수 있는 간단하고 강력한 두 가지 진단 도구인 학습 곡선과 검증 곡선(learning curves and validation curves)에 대해 살펴봄.
* 다음 하위 섹션에서는 학습 곡선을 사용하여 학습 알고리즘에 과적합(고분산), overfitting (high variance) 또는 과소적합(고편향), underfitting (high bias) 문제가 있는지 진단하는 방법에 대해 논의함. 또한, 학습 알고리듬의 일반적인 문제를 해결하는 데 도움이 될 수 있는 검증 곡선을 살펴봄.

#### * Diagnosing bias and variance problems with learning curves


주어진 교육 데이터 세트에 대해 모델이 너무 복잡하면(예: 매우 심층적인 의사 결정 트리를 생각해 보십시오), 모델은 교육 데이터를 과도하게 적합시키는 경향이 있고 보이지 않는 데이터에 잘 일반화되지 않습니다. 종종, 그것은 과적합 정도를 줄이기 위해 더 많은 훈련 예제를 수집하는 데 도움이 될 수 있습니다.

그러나 실제로는 비용이 많이 들거나 더 많은 데이터를 수집하는 것이 불가능할 수 있습니다. 모델 교육 및 검증 정확도를 교육 데이터 세트 크기의 함수로 표시함으로써 모델이 높은 분산 또는 높은 편향으로 고통받고 있는지, 더 많은 데이터를 수집하면 이 문제를 해결하는 데 도움이 될 수 있는지 여부를 쉽게 감지할 수 있습니다.

그러나 skickit-learn에서 학습 곡선을 그리는 방법에 대해 논의하기 전에 다음 그림을 통해 이러한 두 가지 일반적인 모델 문제에 대해 논의해 보겠습니다.


fig4

