---
layout: single
title:  "Chapter6. Learning Best Practices for Model Evaluation and Hyperparameter Tuning"
use_math: true
---
## 1. Streamlining workflows with pipelines 

* 이 섹션에서는 sickit-learn의 Pipeline 클래스에 대해 배움. 

#### * The main steps in principal component analysis

* 위스콘신 유방암 데이터 세트는 569개의 악성 및 양성 종양 샘플이 포함되어있음. 두 열은 샘플의 진단(M = 악성, B = 양성)의 고유한 ID 번호를 저장하며, 3~32 열에는 세포 핵의 이미지에서 계산된 30개의 실제 값 특징이 포함됨.

1. 먼저 UCI 웹 사이트에서 판다를 사용하여 데이터 세트를 읽어옴.

```python
>> import pandas as pd
>>> df = pd.read_csv('https://archive.ics.uci.edu/ml/'
... 'machine-learning-databases'
... '/breast-cancer-wisconsin/wdbc.data',
... header=None)
```

2. 다음으로 30개의 특성을 NumPy 배열 X에 할당하고, LabelEncoder 객체를 사용하여 클래스 레이블을 원래 문자열 표현('M' 및 'B')에서 정수로 변환함.

```python
>>> from sklearn.preprocessing import LabelEncoder
>>> X = df.loc[:, 2:].values
>>> y = df.loc[:, 1].values
>>> le = LabelEncoder()
>>> y = le.fit_transform(y)
>>> le.classes_
array(['B', 'M'], dtype=object)
```

3. 배열에서 클래스 레이블(진단)을 인코딩한 후 y, 악성 종양은 클래스 1로, 양성 종양은 클래스 0으로 각각 표시되며, 두 개의 더미 클래스 레이블에 장착된 LabelEncoder의 변환 방법을 호출하여 이 매핑을 다시 확인할 수 있음.

```python
>> le.transform(['M', 'B'])
array([1, 0])
```

4. 다음 섹션에서 첫 번째 모델 파이프라인을 구축하기 전에 데이터 세트를 별도의 교육 데이터 세트(80%)와 별도의 테스트 데이터 세트(20%)로 나눔.

```python
>>> from sklearn.model_selection import train_test_split
>>> X_train, X_test, y_train, y_test = \
... train_test_split(X, y,
... test_size=0.20,
... stratify=y,
... random_state=1)
```

#### * Combining transformers and estimators in a pipeline
* 이전 챕터들에서 많은 학습 알고리즘에게서 최적의 성능을 끌어내기 위해 입력 특성은 동일한 규모를 가져야함.이에따라 위스콘신 유방암 데이터 세트도 특성 표준화가 필요함. 
* 5장에서 소개한 PCA를 통해 30차원의 데이터를 2차원의 부분공간으로 압축한다고 가정함. 

* 훈련 및 테스트 데이터 세트에 대한 학습 및 데이터 변환 단계를 별도로 거치지 않고 StandardScaler, PCA 및 LogisticRegression 개체를 Pipeline에 연결할 수 있음. 

```python
>>> from sklearn.preprocessing import StandardScaler
>>> from sklearn.decomposition import PCA
>>> from sklearn.linear_model import LogisticRegression
>>> from sklearn.pipeline import make_pipeline
>>> pipe_lr = make_pipeline(StandardScaler(),
... PCA(n_components=2),
... LogisticRegression())
>>> pipe_lr.fit(X_train, y_train)
>>> y_pred = pipe_lr.predict(X_test)
>>> test_acc = pipe_lr.score(X_test, y_test)
>>> print(f'Test accuracy: {test_acc:.3f}')
Test accuracy: 0.956
```
* **위 코드 분석**
* **make_pipeline**: 임의의 수의 사이킷런 변환기(fit 및 transform메서드 입력으로 지원하는 객체)를 취한 다음, 사이킷런 추정기(fit 및 predict메서드을 입력으로 구현하는 객체)를 연결시켜줌. 
* 위 코드에서는 두 개의 변환기 **StandardScaler** 및 **PCA**와 **LogisticRegression** 추정기를 **make_pipeline**의 입력으로 제공하였으며, 이 함수는 사이킷런의 **Pipeline** 클래스 객체를 생성하여 반환해줌. 
* 사이킷런의 **Pipeline** 클래스를 변압기와 추정기 주변의 메타 추정기(meta-estimator) 또는 랩로 생각할 수 있음.
* **Pipeline** 의 fit 메서드를 호출하면 **StandardScaler** 의 fit 및 transform메서드를 호출함. 이는 다음 **PCA**에 전달되며 **PCA** 의 fit 및 transform메서드를 호출하며 이는 추정기에 데이터가 전달됨.
* 
* 위 코드에서 pipe_lr 파이프라인에 fit 메서드를 실행했을 때, StandardScaler는 훈련 데이터에 대해 fit 및 transform 호출을 처음 수행하였음. 이렇게 변환된 훈련 데이터는 다음 개체인 PCA로 전달되었음. 
* 이제 **LogisticRegression** 은 이를 토대로 학습을 진행함. **여기서, 파이프라인 중간 단계 개수는 제한이 없지만 마지막 파이프라인 요소는 추정기가 되어야함.**
* 파이프라인의 마지막 단계가 추정기인 경우, 파이프라인도 **predict** 메서드를 구현함. **predict** 메서드 호출에 데이터 세트를 제공하는 경우 데이터는 transform메서드 호출을 통해 중간단계를 통과하고, 마지막 단계에서 추정기 개체가 변환된 데이터에 대한 **predict** 을 반환함. 

fig1

## 2. Using k-fold cross-validation to assess model performance

#### * The holdout method

* 기계 학습 모델의 일반화 성능을 추정하기 위한 고전적이고 대중적인 접근 방식은 홀드아웃 방법이 있음. 
* 홀드아웃 방법을 사용하여 초기 데이터 세트를 별도의 훈련 및 테스트 데이터 세트로 분할함. 전자는 모델 훈련에 사용되며 후자는 일반화 성능을 추정하는데 사용됨. 
* 일반적인 기계 학습 응용 프로그램에서 우리는 처음보는 데이터에 대한 예측 성능을 더욱 향상시키기 위해 하이퍼파라미터(hyperparameters)를 튜닝함. 이 프로세스를 **모델 선택(model selection)** 이라함. 
* 그러나 모델 선택 중에 동일한 테스트 데이터 세트를 반복적으로 재사용하면 훈련 데이터의 일부가 되어 모델이 overfit될 가능성이 높아짐.
* 모델 선택에 홀드아웃 방법을 사용하는 더 나은 방법은 데이터를 훈련 데이터 세트, 검증 데이터(validation dataset) 세트 및 테스트 데이터 세트의 세 부분으로 분리하는 방법임.
* 훈련 데이터 세트는 다양한 모델에 적합하도록 사용되며, 검증 데이터 세트의 성능은 모델 선택에 사용됨.
* 훈련 및 모델 선택 단계에서 모델이 이전에 보지 못한 테스트 데이터 세트를 사용하는 것은 새로운 데이터로 일반화하는 능력에 대한 편견이 덜한 추정치를 얻을 수 있다는 이점이 있음.

fig2

* 위 그림은 서로 다른 초 매개 변수 값을 사용하여 훈련 후 모델의 성능을 반복적으로 평가하기 위해 검증 데이터 세트를 사용하는 홀드아웃 교차 검증(holdout cross-validation)의 개념을 보여줌. 하이퍼파라미터 값의 조정에 만족하면 테스트 데이터 세트에서 모델의 일반화 성능을 추정함.
* 홀드아웃 방법의 단점: 성능 추정치가 훈련 데이터 세트를 훈련 및 검증 하위 집합으로 분할하는 방법에 민감하다는 점임.
* 추정치는 데이터의 예에 따라 달라짐. 다음 하위 섹션에서 k개의 훈련 데이터 하위 집합에 대해 k번의 홀드아웃 방법을 반복하는 k번의 교차 검증, 성능 추정을 위한 보다 강력한 기술을 소개함.

#### * K-fold cross-validation

* K-폴드 교차 검증에서, 대체 없이 무작위로 훈련 데이터 세트를 k폴드로 분할함.
* 여기서 모델 훈련에는 training folds라고 불리는 k-1개의 폴드가 사용되고, 성능 평가에는 test fold라고 불리는 1개의 폴드가 사용됨. 이 절차는 k번 반복되기 때문에, k개의 모델과 성능 추정치를 얻음.
* 그런 다음 서로 다른 독립적 test fold를 기반으로 모델의 평균 성능을 계산하기 때문에 홀드아웃 방법에 비해 교육 데이터의 하위 분할에 덜 민감한 성능 추정치를 얻음.
* 일반적으로 모델 튜닝을 위해 k-폴드 교차 검증을 사용함. 즉, test fold에서 모델 성능을 평가하여 추정한 만족스러운 일반화 성능을 산출하는 최적의 하이퍼파라미터 값을 찾음.
* 하이퍼파라미터 값을 찾으면 전체 훈련 데이터 세트에서 모델을 재교육하고 독립 테스트 데이터 세트를 사용하여 최종 성능 추정차를 얻음.
* k-폴드 교차 검증 후 전체 훈련 데이터 세트에 모델을 맞추는 이유:
    * 첫째, 일반적으로 단일 최종 모델(개별 모델 대비)에 관심이 있음.
    * 둘째, 학습 알고리듬에 더 많은 교육 예를 제공함으로써 일반적으로 더 정확하고 강력한 모델을 얻을 수 있음.

* k-폴드 교차 검증은 대체되지 않은 리샘플링 기법이기 때문에, 각 반복에서 각 예제가 정확히 한 번 사용되고 training folds, test fold는 분리된다는 장점이 있음. (즉, fold 사이 겹침이 없음.)


fig3

* 위 그림은 k=10을 사용한 k-폴드 교차 검증의 개념을 요약함. 훈련 데이터 세트는 10개의 fold로 나뉘며, 10번의 반복 동안 9개의 fold는 훈련에, 1개는 평가를 위한 테스트 세트로 사용됨.
* 또한 각 접힘에 대한 추정 성능 $E_{i}$(예: 분류 정확도 또는 오류)를 사용하여 모형의 추정 평균 성능 $E$를 계산함.
* k-폴드 교차 검증에서는 모든 데이터 포인트가 평가에 사용되기 때문에 k-폴드 교차 검증은 검증 세트가 있는 홀드아웃 방법보다 데이터 세트를 더 잘 사용함.
* Ron Kohavi의 실험과 경험적 증거를 통해, k-fold 교차 검증의 좋은 표준 값은 10임.
* 그러나 비교적 작은 훈련 세트를 사용하는 경우 fold 수를 늘리는 것이 유용할 수 있음. k의 값을 높이면 각 반복에 더 많은 훈련 데이터가 사용되므로 개별 모델 추정치를 평균하여 일반화 성능을 추정하는 비관적 편향이 낮아짐. 그러나 k 값이 크면 교차 검증 알고리듬의 런타임도 증가하고 훈련 fold 값이 서로 더 비슷하기 때문에 분산이 더 높은 추정치를 산출할 수 있으며, 대규모 데이터 세트를 사용하는 경우, k = 5와 같은 더 작은 값 포크를 선택할 수 있으며, 다른 fold에서 모델을 재장착하고 평가하는 계산 비용을 줄이면서 모델의 평균 성능에 대한 정확한 추정치를 얻을 수 있음.


* 표준 k-폴드 교차 검증(standard k-fold cross-validation) 접근법에 비해 약간 개선된 것은 계층화된 k-폴드 교차 검증(stratified k-fold cross-val-idation)으로, 특히 클래스 비율이 같지 않은 경우 더 나은 편향 및 분산 추정치를 산출할 수 있어 더 유용함.
* 계층화 교차 검증에서 클래스 레이블 비율은 각 폴드에 보존되어 각 폴드가 훈련 데이터 세트의 클래스 비율을 대표하도록 보장함.

```python
>>> import numpy as np
>>> from sklearn.model_selection import StratifiedKFold
>>> kfold = StratifiedKFold(n_splits=10).split(X_train, y_train)
>>> scores = []
>>> for k, (train, test) in enumerate(kfold):
... pipe_lr.fit(X_train[train], y_train[train])
... score = pipe_lr.score(X_train[test], y_train[test])
... scores.append(score)
... print(f'Fold: {k+1:02d}, '
... f'Class distr.: {np.bincount(y_train[train])}, '
... f'Acc.: {score:.3f}')
Fold: 01, Class distr.: [256 153], Acc.: 0.935
Fold: 02, Class distr.: [256 153], Acc.: 0.935
Fold: 03, Class distr.: [256 153], Acc.: 0.957
Fold: 04, Class distr.: [256 153], Acc.: 0.957
Fold: 05, Class distr.: [256 153], Acc.: 0.935
Fold: 06, Class distr.: [257 153], Acc.: 0.956
Fold: 07, Class distr.: [257 153], Acc.: 0.978
Fold: 08, Class distr.: [257 153], Acc.: 0.933
Fold: 09, Class distr.: [257 153], Acc.: 0.956
Fold: 10, Class distr.: [257 153], Acc.: 0.956
>>> mean_acc = np.mean(scores)
>>> std_acc = np.std(scores)
>>> print(f'\nCV accuracy: {mean_acc:.3f} +/- {std_acc:.3f}')
CV accuracy: 0.950 +/- 0.014
```

* 위 코드에서, 계층화된 것을 초기화하고, 훈련 데이터 세트에서 y_train 클래스 레이블이 있는 sklearn.model_selection 모듈의 KFold 반복기이며, n_splits 매개 변수를 통해 fold 수를 지정하였음.
* kfold를 반복할 때, 훈련에서 반환된 인덱스를 사용하여 이 장의 시작 부분에서 설정한 로지스틱 회귀 파이프라인에 맞춤.
* pipe_lr 파이프라인을 사용하여 각 반복에서 예제가 적절하게 조정(예: 표준화)되었는지 확인하였음.
* 그런 다음 테스트 지수를 사용하여 모델의 정확도 점수를 계산했고, 이를 점수 목록에 수집하여 평균 정확도와 추정치의 표준 편차를 계산함.

```python
>>> from sklearn.model_selection import cross_val_score
>>> scores = cross_val_score(estimator=pipe_lr,
... X=X_train,
... y=y_train,
... cv=10,
... n_jobs=1)
>>> print(f'CV accuracy scores: {scores}')
CV accuracy scores: [ 0.93478261 0.93478261 0.95652174
 0.95652174 0.93478261 0.95555556
 0.97777778 0.93333333 0.95555556
 0.95555556]
>>> print(f'CV accuracy: {np.mean(scores):.3f} '
... f'+/- {np.std(scores):.3f}')
CV accuracy: 0.950 +/- 0.014
```
* 이전 코드 예제는 k-폴드 교차 검증이 작동하는 방식을 설명하는 데 유용했지만, 위 코드와 같이 skickit-learn은 k-폴드 교차 검증 점수를 구현하여 계층화된 k-폴드 교차 검증을 사용하여 모델을 쉽게 평가할 수 있음.


## 3. Debugging algorithms with learning and validation curves

* 이 섹션에서는 학습 알고리즘의 성능을 향상시키는 데 도움이 될 수 있는 간단하고 강력한 두 가지 진단 도구인 학습 곡선과 검증 곡선(learning curves and validation curves)에 대해 살펴봄.
* 다음 하위 섹션에서는 학습 곡선을 사용하여 학습 알고리즘에 과적합(고분산), overfitting (high variance) 또는 과소적합(고편향), underfitting (high bias) 문제가 있는지 진단하는 방법에 대해 논의함. 또한, 학습 알고리듬의 일반적인 문제를 해결하는 데 도움이 될 수 있는 검증 곡선을 살펴봄.

#### * Diagnosing bias and variance problems with learning curves

* 주어진 훈련 데이터 세트에서 모델이 너무 복잡한경우 모델은 훈련 데이터를 overfitting하는 경향이 있고 보이지 않는 데이터에 일반화가 잘 되지 않는 경우가 있음. overfitting을 줄이기 위해 더 많은 훈련 예제를 수집하는 것은 해결방안 중 하나임.
*  그러나 데이터 샘플은 수집하는 것은 비용이 들거나 해결하기 어려운 문제임. 모델 훈련 및 검증 정확도를 훈련 데이터 세트 크기의 함수로 표시함으로써 모델이 높은 분산 또는 높은 편향으로 고통받고 있는지, 더 많은 데이터를 수집하면 이 문제를 해결하는 데 도움이 될 수 있는지 여부를 알 수 있음.

fig4

* 왼쪽 위의 그래프는 치우침이 높은 모형을 보여 줍니다. 이 모델은 훈련 및 교차 검증 정확도가 모두 낮으므로 훈련 데이터에 적합하지 않음을 나타냄. 이 문제를 해결하는 일반적인 방법은 추가 피쳐를 수집하거나 구성하거나 support vector machine(SVM) 또는 로지스틱 회귀 분류기에서 정규화 정도를 줄임으로써 모델 매개 변수의 수를 늘리는 것임.
* 오른쪽 위 그래프는 높은 분산으로 어려움을 겪는 모형을 보여주는데, 이는 훈련과 교차 검증 정확도 사이의 큰 차이가 있음을 나타냄. 이 문제(overfitting)를 해결하기 위해, 더 많은 훈련 데이터를 수집하고, 모델의 복잡성을 줄이거나, 정규화 매개 변수를 증가시킬 수 있음.
* 비정규 모델의 경우 특성 선택(4장) 또는 특성 추출(5장)을 통해 형상 수를 줄여 overfitting 정도를 줄이는 데 도움이 될 수 있음. 일반적으로 더 많은 훈련 데이터를 수집하면 과적합 확률이 감소하는 경향이 있지만, 훈련 데이터에 노이즈가 많거나 이미 최적에 매우 근접한 경우에는 도움이 되지 않을 수 있음.


* 먼저 sickit-learn의 학습 곡선 함수를 사용하여 모델을 평가하는 방법에 대해 알아봄. 

```python
>>> import matplotlib.pyplot as plt
>>> from sklearn.model_selection import learning_curve
>>> pipe_lr = make_pipeline(StandardScaler(),
... LogisticRegression(penalty='l2',
... max_iter=10000))
>>> train_sizes, train_scores, test_scores =\
... learning_curve(estimator=pipe_lr,
... X=X_train,
... y=y_train,
... train_sizes=np.linspace(
... 0.1, 1.0, 10),
... cv=10,
... n_jobs=1)
>>> train_mean = np.mean(train_scores, axis=1)
>>> train_std = np.std(train_scores, axis=1)
>>> test_mean = np.mean(test_scores, axis=1)
>>> test_std = np.std(test_scores, axis=1)
>>> plt.plot(train_sizes, train_mean,
... color='blue', marker='o',
... markersize=5, label='Training accuracy')
>>> plt.fill_between(train_sizes,
... train_mean + train_std,
... train_mean - train_std,
... alpha=0.15, color='blue')
>>> plt.plot(train_sizes, test_mean,
... color='green', linestyle='--',
... marker='s', markersize=5,
... label='Validation accuracy')
>>> plt.fill_between(train_sizes,
... test_mean + test_std,
... test_mean - test_std,
... alpha=0.15, color='green')
>>> plt.grid()
>>> plt.xlabel('Number of training examples')
>>> plt.ylabel('Accuracy')
>>> plt.legend(loc='lower right')
>>> plt.ylim([0.8, 1.03])
>>> plt.show()
```

* 위 코드에서, 로지스틱 회귀 개체를 인스턴스화할 때 max_iter=message를 추가 인수로 전달하여 더 작은 데이터 세트 크기나 극단적인 정규화 매개 변수 값(다음 섹션에서 설명)에 대한 수렴 문제를 피함. 이전 코드를 실행하여 다음 학습 곡선 그림을 얻을 수 있음.

fig5

* learning_curve 함수의 train_sizes 매개 변수를 통해 학습 곡선을 생성하는 데 사용되는 훈련 예제의 절대 또는 상대 수를 제어할 수 있음. 훈련 데이터 세트 크기에 대해 10개의 균일한 간격의 상대 간격을 사용하도록 train_deling=np.linspace(0.1, 1.0, 10)를 설정함. 기본적으로, learning_message 함수는 계층화된 k-폴드 교차 검증을 사용하여 분류기의 교차 검증 정확도를 계산하고, 10배 계층화된 교차 검증을 위해 cv 매개 변수를 통해 k = 10을 설정함.
* Matplotlib의 플롯 함수를 사용하여 플롯한 여러 크기의 훈련 데이터 세트에 대해 반환된 교차 검증된 훈련 및 테스트 점수에서 평균 정확도를 간단히 계산하였음. 또한 추정치의 분산을 나타내기 위해 fill_between 함수를 사용하여 평균 정확도의 표준 편차를 그림에 추가함.
* 그림에서 볼 수 있듯이, 우리 모델은 훈련 중에 250개 이상의 예를 본 경우 훈련 및 검증 데이터 세트 모두에서 상당히 잘 수행됨. 또한 250개 미만의 예제를 가진 훈련 데이터 세트에 대해 교육 정확도가 증가하고, 검증과 훈련 정확도 사이의 격차가 확대되어 과적합 정도가 증가한다는 것을 확인할 수 있음.

#### * Addressing over- and underfitting with validation curves

* 검증 곡선은 과적합 또는 과소적합과 같은 문제를 해결하여 모형의 성능을 향상시키는 데 유용한 도구임. 검증 곡선은 학습 곡선과 관련이 있지만, 훈련과 테스트 정확도를 샘플 크기의 함수로 표시하는 대신 모델 매개 변수의 값을 변경함.

```python
>>> from sklearn.model_selection import validation_curve
>>> param_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]
>>> train_scores, test_scores = validation_curve(
... estimator=pipe_lr,
... X=X_train,
... y=y_train,
... param_name='logisticregression__C',
... param_range=param_range,
... cv=10)
>>> train_mean = np.mean(train_scores, axis=1)
>>> train_std = np.std(train_scores, axis=1)
>>> test_mean = np.mean(test_scores, axis=1)
>>> test_std = np.std(test_scores, axis=1)
>>> plt.plot(param_range, train_mean,
... color='blue', marker='o',
... markersize=5, label='Training accuracy')
>>> plt.fill_between(param_range, train_mean + train_std,
... train_mean - train_std, alpha=0.15,
... color='blue')
>>> plt.plot(param_range, test_mean,
... color='green', linestyle='--',
... marker='s', markersize=5,
... label='Validation accuracy')
>>> plt.fill_between(param_range,
... test_mean + test_std,
... test_mean - test_std,
... alpha=0.15, color='green')
>>> plt.grid()
>>> plt.xscale('log')
>>> plt.legend(loc='lower right')
>>> plt.xlabel('Parameter C')
>>> plt.ylabel('Accuracy')
>>> plt.ylim([0.8, 1.0])
>>> plt.show()
```

* 이전 코드를 사용하여 파라미터 C에 대한 검증 곡선도를 구하면 다음과 같음.

fig6

* learning_curve 함수와 마찬가지로 validation_curve 함수는 기본적으로 계층화된 k-fold 교차 검증을 사용하여 분류기의 성능을 추정함. 
* validation_curve 함수 내에서 평가하고자 하는 파라미터를 지정함. 이 경우, Param_range 매개 변수를 통해 설정한 지정된 값 범위에 대해 skickit-learn 파이프라인 내의 LogisticRegression 개체에 액세스하기 위해 'logisticRegression__C'(LogisticRegression 분류기의 역 정규화 매개 변수인 C)로 작성하여 사용함.
* 그림을 통해, C 값에 대한 정확도 차이는 미묘하지만 정규화 강도(C의 작은 값)를 높이면 모델이 데이터에 약간 적합하지 않다는 것을 알 수 있음. 그러나 C 값이 클 경우 정규화의 강도를 낮추는 것을 의미하므로 모형이 데이터를 약간 과대 적합하는 경향이 있음. 이 경우 단점은 C 값의 0.01과 0.1 사이에 있는 것으로 나타남.

#### * Fine-tuning machine learning models via grid search


## 5. Tuning hyperparameters via grid search
* 그리드 검색 방식은 매우 간단함. 서로 다른 하이퍼 파라미터에 대한 값 목록을 지정하고 컴퓨터는 각 조합에 대한 모델 성능을 평가하여 이 집합에서 최적의 값 조합을 얻음.

```python
>>> from sklearn.model_selection import GridSearchCV
>>> from sklearn.svm import SVC
>>> pipe_svc = make_pipeline(StandardScaler(),
... SVC(random_state=1))
>>> param_range = [0.0001, 0.001, 0.01, 0.1,
... 1.0, 10.0, 100.0, 1000.0]
>>> param_grid = [{'svc__C': param_range,
... 'svc__kernel': ['linear']},
... {'svc__C': param_range,
... 'svc__gamma': param_range,
... 'svc__kernel': ['rbf']}]
>>> gs = GridSearchCV(estimator=pipe_svc,
... param_grid=param_grid,
... scoring='accuracy',
... cv=10,
... refit=True,
... n_jobs=-1)
>>> gs = gs.fit(X_train, y_train)
>>> print(gs.best_score_)
0.9846153846153847
>>> print(gs.best_params_)
{'svc__C': 100.0, 'svc__gamma': 0.001, 'svc__kernel': 'rbf'}
```

* 위 코드에서 sklearn.model_selection 모듈에서 GridSearchCV 개체를 초기화하여 SVM 파이프라인을 교육하고 튜닝하였음. GridSearchCV의 param_grid 매개 변수를 사전 목록으로 설정하여 튜닝할 매개 변수를 지정함. 
* 선형 SVM의 경우 역 정규화 매개 변수 C만 평가했고, 방사형 기저 함수(RBF) 커널 SVM의 경우 svc__C 및 svc_gamma 매개 변수를 모두 조정하였음. svc__gamma 매개 변수는 커널 SVM과 관련이 있음.
* GridSearchCV는 k-폴드 교차 검증을 사용하여 서로 다른 하이퍼파라미터 설정으로 훈련된 모델을 비교함.
* cv=10 설정을 통해 10배 교차 검증을 수행하고 이 10배 사이의 평균 정확도(채점='black'을 통해)를 계산하여 모델 성능을 평가함.
* N_job=-1은 GridSearchCV가 모든 처리 코어를 사용하여 모델을 서로 다른 접힘에 병렬로 적합시켜 그리드 검색 속도를 높일 수 있도록 설정 -> n_jobs=Domply로 변경할 수 있음.
* 훈련 데이터를 사용하여 그리드 검색을 수행한 후 best_score_ 속성을 통해 가장 성능이 우수한 모델의 점수를 얻고 best_params_ 속성을 통해 액세스할 수 있는 매개 변수를 확인함.
* 마지막 단계로, 독립 테스트 데이터 세트를 사용하여 GridSearchCV 개체의 best_estimator_ 속성을 사용하여 가장 잘 선택된 모델의 성능을 추정함.

```python
>>> clf = gs.best_estimator_
>>> clf.fit(X_train, y_train)
>>> print(f'Test accuracy: {clf.score(X_test, y_test):.3f}')
Test accuracy: 0.974
```

#### * Exploring hyperparameter configurations more widely with randomized search

* 그리드 검색(grid search)은 exhaustive search이므로 사용자 지정 파라미터 그리드에 포함된 경우 최적의 하이퍼 파라미터 구성을 찾을 수 있음. 그러나 대규모 하이퍼파라미터 그리드를 지정하면 실제로 그리드 검색이 어려워짐.
* 다른 매개 변수 조합을 샘플링하는 다른 접근 방식은 randomized search이 있음.
* randomized search에서 분포(또는 이산 세트)에서 무작위로 하이퍼 파라미터 구성을 그림. 그리드 검색과 달리, 무작위 검색은 하이퍼 파라미터 공간에 대한 철저한 검색을 수행하지 않음. 그럼에도 불구하고 이를 통해 보다 비용 효율적인 방식으로 광범위한 하이퍼 파라미터 값 설정을 탐색할 수 있음.

fig7

* 위 그림은 앞서 설명한 개념을 나타내며, 그리드 검색 및 랜덤 검색을 통해 검색되는 9개의 하이퍼 파라미터 설정의 고정 그리드를 보여줌.

* 이제 랜덤 검색을 사용하여 SVM을 조정하는 방법을 살펴봄. Scikit-learn은 이전 하위 섹션에서 사용한 GridSearchCV와 유사한 RandomizedSearchCV 클래스를 구현함. 주요 차이점은 매개 변수 그리드의 일부로 분포를 지정하고 평가할 총 하이퍼 매개 변수 구성 수를 지정이 가능함. 예를 들어, 이전 섹션의 그리드 검색 예에서 SVM을 조정할 때 몇 가지 하이퍼파라미터에 사용한 하이퍼파라미터 범위를 고려함.

```python
example in the previous section:
>>> param_range = [0.0001, 0.001, 0.01, 0.1,
... 1.0, 10.0, 100.0, 1000.0]
```

* RandomizedSearchCV는 범주형 하이퍼 파라미터를 고려할 때 유용한 매개 변수 그리드에 대한 입력으로 유사한 이산 값 목록을 허용할 수 있지만, 주된 힘은 이러한 목록을 샘플링할 분포로 대체할 수 있다는 사실에 있음. 예를 들어, 이전 목록을 SciPy의 다음 분포로 대체할 수 있음.

```python
>>> param_range = scipy.stats.loguniform(0.0001, 1000.0)
```

* 예를 들어, 정규 균일 분포 대신 로그 균일 분포를 사용하면 충분히 많은 시행 횟수에서 [10.0, 100.0] 범위와 동일한 수의 표본을 추출할 수 있음. 동작을 확인하기 위해 다음과 같이 rvs(10) 방법을 통해 이 분포에서 10개의 랜덤 표본을 추출할 수 있음.
* 이전 섹션의 GridSearchCV와 같이 RandomizedSearchCV가 작동하고 SVM을 튜닝하는 방법을 살펴봄.

```python
>>> np.random.seed(1)
>>> param_range.rvs(10)
array([8.30145146e-02, 1.10222804e+01, 1.00184520e-04, 1.30715777e-02,
 1.06485687e-03, 4.42965766e-04, 2.01289666e-03, 2.62376594e-02,
 5.98924832e-02, 5.91176467e-01])
```

* 위 코드 예제를 기반으로, n_iter=20을 설정하여 파라미터 범위를 지정하고 반복 횟수(20회 반복)를 지정할 수 있다는 점을 제외하고, 사용이 GridSearchCV와 매우 유사하다는 것을 알 수 있음.

* 이제 이전 섹션의 GridSearchCV와 같이 RandomizedSearchCV가 작동하고 SVM을 튜닝하는 방법을 살펴봄.

```python
>>> from sklearn.model_selection import RandomizedSearchCV
>>> pipe_svc = make_pipeline(StandardScaler(),
... SVC(random_state=1))
>>> param_grid = [{'svc__C': param_range,
... 'svc__kernel': ['linear']},
... {'svc__C': param_range,
... 'svc__gamma': param_range,
... 'svc__kernel': ['rbf']}]
>>> rs = RandomizedSearchCV(estimator=pipe_svc,
... param_distributions=param_grid,
... scoring='accuracy',
... refit=True,
... n_iter=20,
... cv=10,
... random_state=1,
... n_jobs=-1)
>>> rs = rs.fit(X_train, y_train)
>>> print(rs.best_score_)
0.9670531400966184
>>> print(rs.best_params_)
{'svc__C': 0.05971247755848464, 'svc__kernel': 'linear'}
```

* 위 코드를 기반으로, n_iter=20을 설정하여 파라미터 범위를 지정하고 반복 횟수(20회 반복)를 지정할 수 있다는 점을 제외하고, 사용이 GridSearchCV와 매우 유사하다는 것을 알 수 있음.


#### * More resource-efficient hyperparameter search with successive halving

* skickit-learn은 랜덤 검색의 아이디어를 한 단계 더 나아가 적절한 하이퍼 파라미터 구성을 보다 효율적으로 찾을 수 있는 연속적인 반감 변형 HalvingRandomSearchCV를 구현함.
* 후보 구성 집합이 많이 주어졌을 때 연속 반감하면 단 하나의 구성만 남을 때까지 유망한 하이퍼 파라미터 구성이 연속적으로 폐기됨. 이는 다음 단계로 간단히 요약됨.
    1. 랜덤 샘플링을 통해 후보 구성을 크게 그림
    2. 제한된 리소스로 모델을 훈련함.(예: 전체 교육 세트를 사용하는 대신) 교육 데이터의 작은 부분 집합
    3. 예측 성과에 따라 하위 50%를 폐기
    4. 사용 가능한 리소스의 양을 늘려서 2단계로 돌아감.
* 이는 나의 하이퍼 파라미터 구성만 남을 때까지 반복됨. 또한 HalvingGridSearchCV라고 하는 그리드 검색 변종에 대한 연속적인 절반의 구현이 존재하며 여기서 모든 지정된 하이퍼 파라미터 구성이 랜덤 샘플 대신 1단계에서 사용됨.

* 실험 지원을 활성화한 후 다음과 같이 연속 반감된 랜덤 검색을 사용할 수 있음.

```python
>>> from sklearn.model_selection import HalvingRandomSearchCV
>>> hs = HalvingRandomSearchCV(pipe_svc,
... param_distributions=param_grid,
... n_candidates='exhaust',
... resource='n_samples',
... factor=1.5,
... random_state=1,
... n_jobs=-1)
```

* 위 코드에서 resource='n_filename'(기본값) 설정은 라운드마다 다른 리소스로 교육 세트 크기를 고려하도록 지정
* factor parameter를 통해 각 라운드에서 탈락되는 후보 수를 확인이 가능함.
* 예를 들어, 요인=2를 설정하면 후보자의 절반이 제거되고, 요인=1.5는 100%/1.5÷66%만 다음 라운드에 진출한다는 것을 의미함. 
* RandomizedSearchCV에서처럼 고정된 반복 횟수를 선택하는 대신, n_propertates='property'(기본값)를 설정함.
* 이 경우 마지막 라운드에서 최대 리소스 수(여기: 훈련 예제)가 사용되도록 하이퍼 파라미터 구성의 수를 샘플링함. 
* 그런 다음 RandomizedSearchCV와 유사한 검색을 수행

```python
>>> hs = hs.fit(X_train, y_train)
>>> print(hs.best_score_)
0.9617647058823529
>>> print(hs.best_params_)
{'svc__C': 4.934834261073341, 'svc__kernel': 'linear'}

>>> clf = hs.best_estimator_
>>> print(f'Test accuracy: {hs.score(X_test, y_test):.3f}')
Test accuracy: 0.982
```

* GridSearchCV 및 RandomizedSearchCV 결과를 HalvingRandomSearchCV의 모델과 비교하면 후자가 테스트 세트에서 약간 더 나은 성능을 발휘하는 모델(97.4 퍼센트의 정확도 대비 98.2 퍼센트의 정확도)을 산출함.

#### * Algorithm selection with nested cross-validation

* k-폴드 교차 검증을 그리드 검색 또는 랜덤 검색과 함께 사용하는 것은 이전 하위 섹션에서 보았던 것처럼 하이퍼 매개 변수 값을 변경하여 머신 러닝 모델의 성능을 미세 조정하는 데 유용한 접근법임. 그러나 다른 권장 접근 방식은 중첩 교차 검증(nested cross-validation)임.
* 중첩 교차 검증에서는 데이터를 훈련 및 테스트 접기로 분할하기 위한 외부 k폴드 교차 검증 루프가 있으며, 내부 루프는 훈련 접기에서 k폴드 교차 검증을 사용하여 모델을 선택하는 데 사용됨.
* 모형을 선택한 후 테스트 폴드를 사용하여 모형 성능을 평가함. 

fig8

* 위 그림은 5개의 외부 및 2개의 내부 접힘만 있는 중첩 교차 검증의 개념을 설명함. 이는 계산 성능이 중요한 대규모 데이터 세트에 유용하며, 이러한 유형의 중첩 교차 검증은 5×2 교차 검증이라함. 


* sickit-learn에서는 다음과 같이 그리드 검색을 통해 중첩 교차 검증을 수행할 수 있음.

```python
>>> param_range = [0.0001, 0.001, 0.01, 0.1,
... 1.0, 10.0, 100.0, 1000.0]
>>> param_grid = [{'svc__C': param_range,
... 'svc__kernel': ['linear']},
... {'svc__C': param_range,
... 'svc__gamma': param_range,
... 'svc__kernel': ['rbf']}]
>>> gs = GridSearchCV(estimator=pipe_svc,
... param_grid=param_grid,
... scoring='accuracy',
... cv=2)
>>> scores = cross_val_score(gs, X_train, y_train,
... scoring='accuracy', cv=5)
>>> print(f'CV accuracy: {np.mean(scores):.3f} '
... f'+/- {np.std(scores):.3f}')
CV accuracy: 0.974 +/- 0.015
```

* 반환된 평균 교차 검증 정확도는 모델의 초 매개 변수를 조정하고 보이지 않는 데이터에 사용할 경우 예상되는 것에 대한 좋은 추정치를 제공
* 예를 들어, 중첩 교차 검증 방식을 사용하여 SVM 모델을 단순한 의사 결정 트리 분류기와 비교할 수 있음. 

```python
>>> from sklearn.tree import DecisionTreeClassifier
>>> gs = GridSearchCV(
... estimator=DecisionTreeClassifier(random_state=0),
... param_grid=[{'max_depth': [1, 2, 3, 4, 5, 6, 7, None]}],
... scoring='accuracy',
... cv=2
... )
>>> scores = cross_val_score(gs, X_train, y_train,
... scoring='accuracy', cv=5)
>>> print(f'CV accuracy: {np.mean(scores):.3f} '
... f'+/- {np.std(scores):.3f}')
CV accuracy: 0.934 +/- 0.016
```

* 위코드를통해 SVM 모델의 중첩 교차 검증 성능(97.4%)이 Decision Tree의 성능(93.4%)보다 훨씬 우수하므로 이 특정 데이터 세트와 동일한 모집단에서 가져온 새 데이터를 분류하는 것이 더 좋을 수 있음.


#### * Looking at different performance evaluation metrics

* 이전 섹션과 장에서 예측 정확도를 사용하여 다양한 기계 학습 모델을 평가했는데, 이는 모델의 성능을 일반적으로 정량화하는 데 유용한 메트릭임.
*  그러나 정밀도, 리콜,  F1 score 및 Matthews correlation coefficient (MCC)와 같이 모델의 관련성을 측정하는 데 사용할 수 있는 몇 가지 다른 성능 메트릭이 있음.

#### * Reading a confusion matrix

* 그림과 같이 혼란 행렬(confusion matrix)은 분류기의 true positive (TP), true negative (TN), false positive (FP), and false negative (FN) 예측의 카운트를 보고하는 정사각형 행렬임.

* 이러한 메트릭은 실제 클래스 레이블과 예측 클래스 레이블을 비교하여 수동으로 쉽게 계산할 수 있지만 skickit-learn은 다음과 같이 사용할 수 있는 편리한 unusion_matrix 함수를 제공함.

```python
>>> from sklearn.metrics import confusion_matrix
>>> pipe_svc.fit(X_train, y_train)
>>> y_pred = pipe_svc.predict(X_test)
>>> confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)
>>> print(confmat)
[[71 1]
[ 2 40]]
```

* 코드를 실행한 후 반환된 배열은 분류기가 테스트 데이터 세트에서 수행한 다양한 유형의 오류에 대한 정보를 제공함. Matplotlib의 Matshow 기능을 사용하여 이 정보를 그림의 혼란 매트릭스 그림에 매핑할 수 있음.

```python
>>> fig, ax = plt.subplots(figsize=(2.5, 2.5))
>>> ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)
>>> for i in range(confmat.shape[0]):
... for j in range(confmat.shape[1]):
... ax.text(x=j, y=i, s=confmat[i, j],
... va='center', ha='center')
>>> ax.xaxis.set_ticks_position('bottom')
>>> plt.xlabel('Predicted label')
>>> plt.ylabel('True label')
>>> plt.show()
```

* 이제 레이블을 추가한 다음 혼란 행렬도를 사용하면 쉬운 결과 해석이 가능함. 

fig9

* 위 예시에서 클래스 1(malignant)이 양의 클래스라 가정하면, 우리 모델은 클래스 0(TN)에 속하는 예시 중 71개와 클래스 1(TP)에 속하는 예시 40개를 각각 정확하게 분류하였음. 그러나 모델은 클래스 1의 두가지 예를 클래스 0(FN)으로 잘못 분류하였으며, benign tumor (FP)이지만 malignant이라고 예측하였음. 다음 절에서는 해당 정보를 사용하여 다양한 오류 매트릭을 계산하는 방안에 대해 논의함. 


#### * Optimizing the precision and recall of a classification model

* 예측 오차(ERR)와 정확도(ACC)는 모두 잘못 분류된 예수의 수에 대한 일반적인 정보를 제공
* 오차는 모든 잘못된 예측의 합을 총 예측 수로 나눈 값으로 이해할 수 있으며, 정확도는 정확한 예측의 합을 총 예측 수로 나눈 값으로 각각 계산됨.

$$ ERR = \frac{FP+FN}{FP+FN+TP+TN} $$

* 예측 정확도는 다음 오류에서 직접 계산됨.

$$ ACC = \frac{TP+TN}{FP+FN+TP+TN} = 1-ERR $$

* true positive rate(TPR)과 false positive rate(FPR)은 불균형 클래스 문제에 특히 유용한 성능 지표임.

$$ FRR = \frac{FP}{N} = \frac{FP}{FP+TN} $$

$$ TPR = \frac{TP}{P} = \frac{TP}{FN+TP} $$

* 예를 들어, 종양 진단에서 환자를 적절한 치료로 돕기 위해 악성 종양의 발견에 더 관심을 가짐. 그러나 환자에게 불필요한 걱정을 끼치지 않도록 malignant(FP)으로 잘못 분류된 양성 종양의 수를 줄이는 것도 중요함. FPR과 달리, TPR은 총 positives pool(P)에서 정확하게 식별된 양성(또는 관련) 예제의 비율에 대한 유용한 정보를 제공함.

* 성능 메트릭 precision(PRE) 및 recall(REC)은 TP 및 TN 속도와 관련이 있으며, 실제로 REC는 TPR과 동의어

$$ FRR = TRP = \frac{TP}{P} = \frac{TP}{FN+TP} $$

* 다시 말해, 리콜은 얼마나 많은 관련 레코드(양성)가 이와 같이 캡처되는지(참 긍정)를 수량화합니다. Precision은 관련된 것으로 예측된 레코드 수(참 및 거짓 긍정의 합계)가 실제로 관련이 있는지(참 긍정)를 정량화함. 

$$ PRE = \frac{TP}{TP+FP} $$





