---
layout: single
title:  "Chapter11/12/13"
use_math: true
---
* **Chapter11. Implementing a Multilayer Artificial Neural Network from Scratch**
* **Chapter12. Parallelizing Neural Network Training with PyTorch**
* **Chapter13. Going Deeper – The Mechanics of PyTorch**


# Chapter11. Implementing a Multilayer Artificial Neural Network from Scratch

* 딥 러닝은 여러 계층을 가진 인공 신경망(NN)을 효율적으로 훈련하는 것과 관련된 기계 학습의 하위 분야임.
* 인공 NN의 기본 개념을 학습하며, 이미지 및 텍스트 분석에 특히 적합한 고급 파이썬 기반 딥 러닝 라이브러리와 심층 신경망(DNN) 아키텍처를 소개


## 1. Introducing the multilayer neural network architecture

* 이 섹션에서는 여러 단일 뉴런을 다층 feedforward NN에 연결하는 방법에 대해 배움, 완전히 연결된 네트워크의 특별한 유형은 MLP라 부름.


![1](https://user-images.githubusercontent.com/113302607/201665662-94db86f9-4e6a-403a-8923-678a5d4c2904.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.1. A two-layer MLP
</figcaption>

* 위 그림의 MLP는 데이터 입력 옆 숨겨진 레이어와 출력 레이어가 하나씩 존재함. 
* 숨김 계층의 장치는 입력 기능에 완전히 연결되고 출력 계층은 숨김 계층에 완전히 연결됨.
* 이러한 네트워크에 둘 이상의 숨겨진 레이어가 있는 경우, 우리는 그것을 심층 NN이라고 부름.

## 2. Activating a neural network via forward propagation

* 이 섹션에서는 MLP 모델의 출력을 계산하기 위한 순방향 전파 과정에 대해 설명
* MLP 학습 절차를 세 가지 간단한 단계로 요약하면 다음과 같음. 


* 1) 입력 계층에서 시작하여 네트워크를 통해 훈련 데이터의 패턴을 전달하여 출력을 생성
* 2) 네트워크의 출력에 기초하여, 우리는 나중에 설명할 손실 함수를 사용하여 최소화하고자 하는 손실을 계산
* 3) 손실을 역전파하고, 네트워크의 각 가중치 및 바이어스 단위에 대한 도함수를 찾고, 모델을 업데이트

* 마지막으로, 우리는 이 세 가지 단계를 여러 시대에 걸쳐 반복하고 MLP의 가중치 및 바이어스 매개 변수를 학습한 후, 전방 전파를 사용하여 네트워크 출력을 계산하고 임계값 함수를 적용하여 이전 섹션에서 설명한 원-핫 표현에서 예측된 클래스 레이블을 얻음.

* 은닉층의 각 단위는 입력층의 모든 단위와 연결되어 있기 때문에, 먼저 다음과 같이 은닉층의 활성화 단위를 계산함.
* $z_{1}^{(h)}=x_{1}^{(in)}w_{1,1}^{(h)}+x_{2}^{(in)}w_{1,2}^{(h)}+...+x_{m}^{(in)}w_{1,m}^{(h)}$
* $a_{1}^{(h)}=\sigma(z_{1}^{(h)})$

* 여기서, $z_{1}^{(h)}$는 순 입력을 의미하고 $\sigma$는 활성화 함수를 의미함. 
* 이미지 분류와 같은 복잡한 문제를 해결하기 위해 MLP 모델에서 비선형 활성화 기능을 필요로함. 
* 예를 들어 Scikit-Learn을 사용한 기계 학습 분류기 둘러보기 3장의 로지스틱 회귀에 대한 섹션에서 기억하는 Sigmoid(로지스틱) 활성화 기능이 필요

* $\sigma(z) =  \frac{1}{1+e^{-z}} $

![2](https://user-images.githubusercontent.com/113302607/201665742-fd9e3406-560a-4f99-8acc-523f8773e272.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.2. The sigmoid activation function
</figcaption>

* S자형 함수는 위 그림과 같이 0에서 1까지의 범위의 로지스틱 분포에 순 입력 z를 매핑하는 S자형 곡선으로, z = 0에서 y축을 절단함.
* MLP는 feedforward artificial NN(피드포워드 인공NN)의 전형적인 예임. 
* 피드포워드라는 용어는 각 계층이 루프 없이 다음 계층에 대한 입력 역할을 한다는 사실을 의미
* 다층 퍼셉트론이라는 용어는 이 네트워크 아키텍처에서 인공 뉴런은 퍼셉트론이 아닌 일반적으로 시그모이드 단위이기 때문에 약간 혼란스럽게 들릴 수 있음.
* MLP의 뉴런을 0과 1 사이의 연속적인 범위에서 값을 반환하는 로지스틱 회귀 단위로 생각할 수 있음.

## 3. Classifying handwritten digits

#### Obtaining and preparing the MNIST dataset

* MNIST 데이터 세트는 미국 국립표준기술원(NIST)의 두 데이터 세트에서 구성,교육 데이터 세트는 250명의 다른 사람들의 손으로 쓴 숫자, 50 퍼센트의 고등학생, 그리고 50 퍼센트의 인구 조사국의 직원들로 구성되어 있음.
* 테스트 데이터 세트에는 동일한 분할 후에 다른 사람의 손으로 쓴 숫자가 포함되어 있음.
* 데이터 세트 파일을 다운로드하여 NumPy 배열로 전처리하는 대신, 우리는 MNIST 데이터 세트를 더 편리하게 로드할 수 있는 skickit-learn의 새로운 fetch_openml 함수를 사용함.

```python
>>> from sklearn.datasets import fetch_openml
>>> X, y = fetch_openml('mnist_784', version=1,
... return_X_y=True)
>>> X = X.values
>>> y = y.astype(int).values
```

* MNIST의 이미지가 어떻게 보이는지에 대한 아이디어를 얻기 위해, Matplotlib의 imshow 함수를 통해 플롯할 수 있는 원래 28x28 이미지로 784 픽셀 벡터를 재구성한 후 숫자 0-9의 예를 시각화하면 아래 그림과 같음.

```python
>>> import matplotlib.pyplot as plt
>>> fig, ax = plt.subplots(nrows=2, ncols=5,
... sharex=True, sharey=True)
>>> ax = ax.flatten()
>>> for i in range(10):
... img = X[y == i][0].reshape(28, 28)
... ax[i].imshow(img, cmap='Greys')
>>> ax[0].set_xticks([])
>>> ax[0].set_yticks([])
>>> plt.tight_layout()
>>> plt.show()
```

![3 1](https://user-images.githubusercontent.com/113302607/201666557-3269a013-6fbb-45d2-9e1b-4d1bde679561.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.3. A plot showing one randomly chosen handwritten digit from each class
</figcaption>

* 게다가, 같은 숫자의 여러 예시를 그려서 각각의 글씨가 실제로 얼마나 다른지는 다음과 같음.

```python
>>> fig, ax = plt.subplots(nrows=5,
... ncols=5,
... sharex=True,
... sharey=True)
>>> ax = ax.flatten()
>>> for i in range(25):
... img = X[y == 7][i].reshape(28, 28)
... ax[i].imshow(img, cmap='Greys')
>>> ax[0].set_xticks([])
>>> ax[0].set_yticks([])
>>> plt.tight_layout()
>>> plt.show()
```

![4 1](https://user-images.githubusercontent.com/113302607/201666591-59a15204-ceb3-4440-9b6c-872954d4c9e2.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.4. Different variants of the handwritten digit 7
</figcaption>


## 4. Coding the neural network training loop

* 이번 섹션에서는 모델을 훈련하는 내용을 포함함. 
* 먼저 데이터 로드를 위한 몇 가지 기능을 정의하고 이 기능을 여러 루프의 데이터 세트에 걸쳐 반복되는 훈련 루프에 내장할 것임. 

* 우리가 정의할 첫 번째 기능은 미니 배치 생성기로, 데이터 세트를 가져와서 확률적 경사 하강 훈련을 위해 원하는 크기의 미니 배치로 나누며 코드는 다음과 같음.

```python
>>> import numpy as np
>>> num_epochs = 50
>>> minibatch_size = 100
>>> def minibatch_generator(X, y, minibatch_size):
... indices = np.arange(X.shape[0])
... np.random.shuffle(indices)
... for start_idx in range(0, indices.shape[0] - minibatch_size
... + 1, minibatch_size):
... batch_idx = indices[start_idx:start_idx + minibatch_size]
... yield X[batch_idx], y[batch_idx]
```

* 다음으로, 우리는 훈련 과정을 모니터링하고 모델을 평가하는 데 사용할 수 있는 손실 함수와 성능 메트릭을 정의, MSE 손실 및 정확도 기능은 다음과 같이 구현

```python
>>> def mse_loss(targets, probas, num_labels=10):
... onehot_targets = int_to_onehot(
... targets, num_labels=num_labels
... )
... return np.mean((onehot_targets - probas)**2)
>>> def accuracy(targets, predicted_labels):
... return np.mean(predicted_labels == targets)
```

* 이전 기능을 테스트하고 이전 섹션에서 인스턴스화한 모델의 초기 검증 세트 MSE 및 정확성을 계산함.

```python
>>> _, probas = model.forward(X_valid)
>>> mse = mse_loss(y_valid, probas)
>>> print(f'Initial validation MSE: {mse:.1f}')
Initial validation MSE: 0.3
>>> predicted_labels = np.argmax(probas, axis=1)
>>> acc = accuracy(y_valid, predicted_labels)
>>> print(f'Initial validation accuracy: {acc*100:.1f}%')
Initial validation accuracy: 9.4%
```

## 5. Evaluating the neural network performance

* 역전파, NN의 훈련 절차 논의 이전에 훈련한 모델의 성능에 대해 살펴봄.
* train()에서 Matplotlib를 사용하여 결과를 시각화할 수 있도록 각 epoch의 훈련 손실 및 검증 정확도를 수집.
* 먼저 MSE 훈련 손실에 대해 살펴봄. 

```python
>>> plt.plot(range(len(epoch_loss)), epoch_loss)
>>> plt.ylabel('Mean squared error')
>>> plt.xlabel('Epoch')
>>> plt.show()
```

![3](https://user-images.githubusercontent.com/113302607/201666049-d610745c-f247-4f65-913a-3ccae4698e7d.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.5. A plot of the MSE by the number of training epochs
</figcaption>

* 은 처음 10 에폭 동안 상당히 감소했고 마지막 10 에폭 동안 서서히 수렴되는 것처럼 보임.
* 그러나 에폭 40과 에폭 50 사이의 작은 기울기는 추가 에폭에 대한 훈련으로 손실이 더 줄어들 것임을 나타냄

* 다음으로 교육 및 검증 정확도에 대해 살펴봄.


![4](https://user-images.githubusercontent.com/113302607/201666069-65af6833-9ede-4c18-858c-15ead708adda.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.6. Classification accuracy by the number of training epochs
</figcaption>

* 위 그림은 훈련을 할 수록 훈련과 검증 정확도 사이의 차이가 커진다는 것을 보여줌. 
* 대략 25 에폭에서 훈련과 검증의 정확도 값이 거의 같으며, 그 후 네트워크는 훈련 데이터에 과적합하기 시작함. 
* 마지막으로 시험데이터베이스에 대한 예측정확도를 계산하여 모델의 일반화 성능을 평가해봄.

```python
>>> test_mse, test_acc = compute_mse_and_acc(model, X_test, y_test)
>>> print(f'Test accuracy: {test_acc*100:.2f}%')
Test accuracy: 94.51%
```

* 테스트 정확도가 마지막 하위 섹션에서 교육 중에 보고한 마지막 에폭(94.74%)에 해당하는 검증 세트 정확도에 매우 가깝다는 것을 알 수 있음.
* 또한, 각각의 훈련 정확도는 95.59%로 최소로 더 높을 뿐이며, 이는 모델이 훈련 데이터에 약간만 적합하다는 것을 재확인함.

## 6.Training neural networks via backpropagation

* 이 섹션에서는 NN에서 가중치를 매우 효율적으로 학습할 수 있는 방법을 이해하기 위해 역전파의 수학을 살펴봄.
* 출력 레이어의 활성화를 얻기 위해 순방향 전파를 적용해야 하며, 이를 아래와 같이 공식화함.
* $Z^{(h)} = X^{(h)T}+b^{(h)}$ (net input of the hidden layer; 숨겨진 계층의 순 입력)
* $A^{(h)} =  \sigma Z^{(h)}$ (activation of the hidden layer; 은닉층의 활성화)
* $Z^{(out)} = A^{(h)}W^{(out)T}+b^{(out)}$ (net input of the output layer; 출력 계층의 순 입력)
* $A^{(out)} =  \sigma Z^{(out)}$ (activation of the output layer; 출력 계층 활성화)
* 간단히 말해서, 우리는 2개의 입력 기능, 3개의 숨겨진 노드, 2개의 출력 노드가 있는 네트워크의 경우 아래 그림의 화살표와 같이 네트워크의 연결을 통해 입력 기능을 전달하기만 하면됨.

![6](https://user-images.githubusercontent.com/113302607/201666811-1e84377f-3f81-496d-b344-f1582d2f917a.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.7. Forward-propagating the input features of an NN
</figcaption>

* 역 전파에서, 우리는 오류를 오른쪽에서 왼쪽으로 전파하며 이것을 모델 가중치(및 바이어스 단위)에 대한 손실의 기울기를 계산하기 위해 전진 패스의 계산에 체인 규칙을 적용하는 것으로 생각할 수 있음.
* 역 전파하는 계산 경로는 아래의 굵은 화살표를 통해 강조 표시됨.

![8](https://user-images.githubusercontent.com/113302607/201666821-4f9f1329-e255-40b5-964b-162364c4c3ac.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.8. Backpropagating the error of an NN
</figcaption>


# Chapter12. Parallelizing Neural Network Training with PyTorch
* PyTorch는 현재 사용 가능한 가장 인기 있는 딥 러닝 라이브러리 중 하나임.
* 파이토치를 통해 이전의 NumPy 구현보다 훨씬 더 효율적으로 신경망(NN)을 구현할 수 있음. 
* 이 챕터에서는 PyTorch를 사용하여 교육 성과에 어떤 이점을 제공하는지 살펴봄. 

## 1. PyTorch and training performance

* PyTorch는 기계 학습 작업의 속도를 크게 높일 수 있음. 

#### What is PyTorch?

* PyTorch는 딥러닝 및 머신러닝 알고리즘을 구현하고 실행하기 위한 확장 가능한 멀티 플랫폼 프로그래밍 인터페이스를 의미함.
* 파이토치는 주로 페이스북 AI 리서치(FAIR) 연구소의 연구원들과 엔지니어들에 의해 개발되었음.
* PyTorch는 노드 집합으로 구성된 계산 그래프를 기반으로 구축됨.
* 각 노드는 0개 이상의 입력 또는 출력을 가질 수 있는 연산을 나타내며 PyTorch는 연산을 평가하고 계산을 실행하고 구체적인 값을 즉시 반환하는 명령형 프로그래밍 환경을 제공함. 
* 따라서 PyTorch의 계산 그래프는 사전에 구성되고 이후에 실행되는 것이 아니라 암묵적으로 정의됨.
* 수학적으로 텐서는 스칼라, 벡터, 행렬 등의 일반화로 이해할 수 있음. 구체적으로 스칼라는 rank-0 텐서로 정의될 수 있고, 벡터는 rank-1 텐서로 정의될 수 있으며 세 번째 차원에 쌓인 행렬은 rank-3 텐서로 정의 될 수 있음. 
* 파이토치의 텐서는 자동 분화에 최적화되어 있으며 GPU에서 실행할 수 있다는 점을 제외하면, NumPy 배열과 유사함. 


## 1. First steps with PyTorch
#### Installing PyTorch

```python
pip install torch torchvision
```

#### Creating tensors in PyTorch

* torch.tensor 또는 torch.from_numpy 함수를 사용하여 목록이나 NumPy 배열에서 텐서를 간단하게 만들 수 있음.

```python
>>> import torch
>>> import numpy as np
>>> np.set_printoptions(precision=3)
>>> a = [1, 2, 3]
>>> b = np.array([4, 5, 6], dtype=np.int32)
>>> t_a = torch.tensor(a)
>>> t_b = torch.from_numpy(b)
>>> print(t_a)
>>> print(t_b)
tensor([1, 2, 3])
tensor([4, 5, 6], dtype=torch.int32)
```

* 위 코드 실행을 통해, 텐서 t_a와 t_b 생성, 그 속성인 shape = (3,)과 dtype=int32가 소스로 부터 채택되었다. Numpy 배열과 마찬가지로 다음 속성 확인도 가능함. 

```python
>>> t_ones = torch.ones(2, 3)
>>> t_ones.shape
torch.Size([2, 3])
>>> print(t_ones)
tensor([[1., 1., 1.],
 [1., 1., 1.]])
```

* 마지막으로, 랜덤 값의 텐서를 만드는 것은 다음과 같이 수행됨.

```python
>>> rand_tensor = torch.rand(2,3)
>>> print(rand_tensor)
tensor([[0.1409, 0.2848, 0.8914],
 [0.9223, 0.2924, 0.7889]])
```

#### Building a linear regression model

* 이 하위 섹션에서는 선형 회귀 문제를 해결하기 위한 간단한 모델을 구축함. 먼저 NumPy에서 장난감 데이터 세트를 만들고 시각화함.

```python
>>> X_train = np.arange(10, dtype='float32').reshape((10, 1))
>>> y_train = np.array([1.0, 1.3, 3.1, 2.0, 5.0, 
... 6.3, 6.6,7.4, 8.0,
... 9.0], dtype='float32')
>>> plt.plot(X_train, y_train, 'o', markersize=10)
>>> plt.xlabel('x')
>>> plt.ylabel('y')
>>> plt.show()
```

![121](https://user-images.githubusercontent.com/113302607/201677077-a4a65d81-c307-4d72-9907-332bc23f1260.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.1. A scatterplot of the training examples
</figcaption>

* 다음으로, 우리는 기능(중심화 및 표준 편차로 나누기 평균)을 표준화하고 교육 세트 및 해당 DataLoader를 위한 PyTorch Dataset을 만듦.


```python
>>> from torch.utils.data import TensorDataset
>>> X_train_norm = (X_train - np.mean(X_train)) / np.std(X_train)
>>> X_train_norm = torch.from_numpy(X_train_norm)
>>> y_train = torch.from_numpy(y_train)
>>> train_ds = TensorDataset(X_train_norm, y_train)
>>> batch_size = 1
>>> train_dl = DataLoader(train_ds, batch_size, shuffle=True)
```

* 이제 선형 회귀 분석 모델을 z = wx + b로 정의할 수 있음. 여기서는 torch.nn 모듈을 사용
*  모델() 함수를 정의하여 이 모델이 입력 데이터를 사용하여 출력을 생성하는 방법을 결정

```python
>>> torch.manual_seed(1)
>>> weight = torch.randn(1)
>>> weight.requires_grad_()
>>> bias = torch.zeros(1, requires_grad=True)
>>> def model(xb):
... return xb @ weight + bias 
```

* 모델을 정의한 후, 우리는 최적의 모델 가중치를 찾기 위해 최소화하고자 하는 손실 함수를 정의할 수 있음.
* 여기서 손실함수로 평균 제곱 오차(MES)를 선택함.

```python
>>> def loss_fn(input, target):
... return (input-target).pow(2).mean()
```

# Chapter13. Going Deeper – The Mechanics of PyTorch

* 이 장에서는 NN을 구현하기 위해 PyTorch의 API의 다양한 측면을 사용할 것임.
* 특히 표준 아키텍처의 구현을 매우 편리하게 하기 위해 여러 계층의 추상화를 제공하는 torch.nn 모듈을 다시 사용함.
* torch.nn 모듈을 사용하여 모델을 구축하는 다양한 방법을 설명하기 위해, 고전적인 배타적(XOR) 문제를 고려함. 
* 첫째, 순차 클래스를 사용하여 다층 퍼셉트론을 구축
* 그런 다음, 하위 분류 nn과 같은 다른 방법을 고려(사용자 지정 계층을 정의하는 모듈)
* 마지막으로, 원시 입력에서 예측에 이르는 기계 학습 단계를 다루는 두 가지 실제 프로젝트를 수행


#### Understanding computation graphs

* PyTorch는 핵심에 계산 그래프를 구축하는 데 의존하며, 이 계산 그래프를 사용하여 입력에서 출력까지 텐서 간의 관계를 도출함.
* 0(scalar) 텐서 a,b,c를 가지고 있으며 z=2*(a-b)+c를 평가한다고 가정할 때, 이 평가는 아래 그림과 같이 계산 그래프로 나타낼 수 있음. 

![5](https://user-images.githubusercontent.com/113302607/201666859-815b604f-9808-4b0f-ba40-63d94994bfe2.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.1. How a computation graph works
</figcaption>

* 위 그림의 계산 그래프는 단순한 노드 네트워크임. 각 노드는 입력 텐서 또는 텐서에 함수를 적용하고 출력으로 0개 이상의 텐서를 반환하는 연산임. 
* PyTorch는 이 계산 그래프를 만들고 그에 따라 기울기를 계산하는 데 사용함.

#### Creating a graph in PyTorch

* 이전 그림과 같이 z=2*(a-b)+c를 평가하기 위해 pytorch에서 그래프를 만드는 방법을 설명하는 간단한 예제를 살펴봄. 변수 a,b,c는 스칼라이며 이것을 단순히 a,b,c 입력 인수로 하는 정규 파이썬 함수를 정의함.  

```python
>>> import torch
>>> def compute_z(a, b, c):
... r1 = torch.sub(a, b)
... r2 = torch.mul(r1, 2)
... z = torch.add(r2, c)
... return z
```

* 이제 계산을 수행하기 위해 텐서 객체를 함수 인수로 사용하여 이 함수를 간단히 호출함.
* 덧셈, 서브(또는 빼기), 멀티(또는 곱하기)와 같은 PyTorch 함수는 또한 PyTorch 텐서 객체의 형태로 더 높은 순위의 입력을 제공할 수 있다는 점에 유의
* 다음 코드 예제에서는 스칼라 입력(순위 0)과 순위 1 및 순위 2 입력을 목록으로 제공

```python
>>> print('Scalar Inputs:', compute_z(torch.tensor(1),
... torch.tensor(2), torch.tensor(3)))
Scalar Inputs: tensor(1)
>>> print('Rank 1 Inputs:', compute_z(torch.tensor([1]),
... torch.tensor([2]), torch.tensor([3])))
Rank 1 Inputs: tensor([1])
>>> print('Rank 2 Inputs:', compute_z(torch.tensor([[1]]),
... torch.tensor([[2]]), torch.tensor([[3]])))
Rank 2 Inputs: tensor([[1]])
```


## 1.Simplifying implementations of common architectures via the torch.nn module
#### Implementing models based on nn.Sequential

```python
>>> model = nn.Sequential(
... nn.Linear(4, 16),
... nn.ReLU(),
... nn.Linear(16, 32),
... nn.ReLU()
... )
>>> model
Sequential(
 (0): Linear(in_features=4, out_features=16, bias=True)
 (1): ReLU()
 (2): Linear(in_features=16, out_features=32, bias=True)
 (3): ReLU()
)
```

* 제1 완전 연결 층의 출력은 제1 ReLU 층에 대한 입력으로 사용
* 제1 ReLU 층의 출력은 제2 완전 연결 층의 입력이 됨.
* 마지막으로, 제2 완전 연결 레이어의 출력은 제2 ReLU 레이어에 대한 입력으로 사용
* 예를 들어, 매개 변수에 다른 활성화 기능, 이니셜라이저 또는 정규화 방법을 적용하여 이러한 계층을 추가로 구성할 수 있음.
* 이러한 범주의 대부분에 대해 사용 가능한 옵션의 포괄적이고 완전한 목록은 공식 문서에서 찾을 수 있음.
* 다음 코드 예제에서는 가중치에 대한 초기 값 분포를 지정하여 첫 번째 완전 연결 계층을 구성
* 그런 다음 가중치 행렬에 대한 L1 패널티 기간을 계산하여 두 번째 완전 연결 레이어를 구성

```python
>>> nn.init.xavier_uniform_(model[0].weight)
>>> l1_weight = 0.01
>>> l1_penalty = l1_weight * model[2].weight.abs().sum()
```

* 여기서, 우리는 Xavier 초기화로 첫 번째 선형 층의 가중치를 초기화 했음. 그리고 우리는 두 번째 선형층의 가중치에 대한 L1 규범을 계산했음.

#### Choosing a loss function

* 최적화 알고리듬의 선택과 관련하여, SGD와 Adam이 가장 널리 사용되는 방법임.
* 손실 함수의 선택은 작업(task)에 따라 달라짐.
* 이 예에서는 SGD 옵티마이저와 교차 엔트로피 손실을 이진 분류에 사용할 것임.

```python
>>> loss_fn = nn.BCELoss()
>>> optimizer = torch.optim.SGD(model.parameters(), lr=0.001)
```

* 다음으로, 우리는 좀 더 실용적인 예: 고전적인 XOR 분류 문제를 해결

#### Solving an XOR classification problem

* XOR 분류 문제는 두 클래스 사이의 비선형 결정 경계 캡처와 관련하여 모델의 용량을 분석하기 위한 고전적인 문제임.
* [–1, 1] 사이의 균일한 분포에서 도출된 두 가지 특징(x0, x1)을 가진 200개의 훈련 예제의 장난감 데이터 세트를 생성
* 그런 다음, 우리는 다음 규칙에 따라 훈련 예 i에 대한 지상 진실 라벨을 지정

![122](https://user-images.githubusercontent.com/113302607/201677129-a8b00f3d-2f2c-4d91-919b-ef2ec92563ba.png)

* 우리는 데이터의 절반(100개의 훈련 사례)을 훈련에 사용하고 나머지 절반은 검증을 위해 사용함.
* 데이터를 생성하고 이를 훈련 및 검증 데이터 세트로 분할하는 코드는 다음과 같음.

```python
>>> import matplotlib.pyplot as plt
>>> import numpy as np
>>> torch.manual_seed(1)
>>> np.random.seed(1)
>>> x = np.random.uniform(low=-1, high=1, size=(200, 2))
>>> y = np.ones(len(x))
>>> y[x[:, 0] * x[:, 1]<0] = 0
>>> n_train = 100
>>> x_train = torch.tensor(x[:n_train, :], dtype=torch.float32)
>>> y_train = torch.tensor(y[:n_train], dtype=torch.float32)
>>> x_valid = torch.tensor(x[n_train:, :], dtype=torch.float32)
>>> y_valid = torch.tensor(y[n_train:], dtype=torch.float32)
>>> fig = plt.figure(figsize=(6, 6))
>>> plt.plot(x[y==0, 0], x[y==0, 1], 'o', alpha=0.75, markersize=10)
>>> plt.plot(x[y==1, 0], x[y==1, 1], '<', alpha=0.75, markersize=10)
>>> plt.xlabel(r'$x_1$', size=15)
>>> plt.ylabel(r'$x_2$', size=15)
>>> plt.show()
```

* 이 코드는 클래스 레이블을 기반으로 다른 마커로 표시된 교육 및 유효성 검사 예제의 다음과 같은 산점도를 생성


![123](https://user-images.githubusercontent.com/113302607/201677158-2cf67d75-9790-444b-bfaa-f39bb9a91437.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.2. Scatterplot of training and validation examples
</figcaption>


* 일반적으로 우리가 더 많은 층을 가질수록, 그리고 우리가 각 층에 더 많은 뉴런을 가질수록, 모델의 용량은 더 커질 것임.
* 모델 용량은 모델이 얼마나 쉽게 복잡한 함수에 근접할 수 있는지를 측정하는 척도로 생각할 수 있음.
* 매개 변수가 더 많다는 것은 네트워크가 더 복잡한 기능을 맞출 수 있다는 것을 의미하지만, 큰 모델은 일반적으로 훈련하기가 더 어려움.(그리고 과대적합되기 쉬움.)

```python
>>> model = nn.Sequential(
... nn.Linear(2, 1),
... nn.Sigmoid()
... )
>>> model
Sequential(
 (0): Linear(in_features=2, out_features=1, bias=True)
 (1): Sigmoid()
)
```
* 매개 변수가 더 많다는 것은 네트워크가 더 복잡한 기능을 맞출 수 있다는 것을 의미하지만, 큰 모델은 일반적으로 훈련하기가 더 어려움.(그리고 과대적합되기 쉬움.)
* 모델을 정의한 후, 우리는 이진 분류와 SGD 최적화 도구를 위한 교차 엔트로피 손실 함수를 초기화함.

```python
and the SGD optimizer:
>>> loss_fn = nn.BCELoss()
>>> optimizer = torch.optim.SGD(model.parameters(), lr=0.001)
```

* 다음으로, 우리는 열차 데이터에 2의 배치 크기를 사용하는 데이터 로더를 만듦.

```python
>>> from torch.utils.data import DataLoader, TensorDataset
>>> train_ds = TensorDataset(x_train, y_train)
>>> batch_size = 2
>>> torch.manual_seed(1)
>>> train_dl = DataLoader(train_ds, batch_size, shuffle=True)
```

```python
>>> torch.manual_seed(1)
>>> num_epochs = 200
>>> def train(model, num_epochs, train_dl, x_valid, y_valid):
... loss_hist_train = [0] * num_epochs
... accuracy_hist_train = [0] * num_epochs
... loss_hist_valid = [0] * num_epochs
... accuracy_hist_valid = [0] * num_epochs
... for epoch in range(num_epochs):
... for x_batch, y_batch in train_dl:
... pred = model(x_batch)[:, 0]
... loss = loss_fn(pred, y_batch)
... loss.backward()
... optimizer.step()
... optimizer.zero_grad()
... loss_hist_train[epoch] += loss.item()
... is_correct = ((pred>=0.5).float() == y_batch).float()
... accuracy_hist_train[epoch] += is_correct.mean()
... loss_hist_train[epoch] /= n_train
... accuracy_hist_train[epoch] /= n_train/batch_size
... pred = model(x_valid)[:, 0]
... loss = loss_fn(pred, y_valid)
... loss_hist_valid[epoch] = loss.item()
... is_correct = ((pred>=0.5).float() == y_valid).float()
... accuracy_hist_valid[epoch] += is_correct.mean()
... return loss_hist_train, loss_hist_valid, \
... accuracy_hist_train, accuracy_hist_valid
>>> history = train(model, num_epochs, train_dl, x_valid, y_valid)
```

* 다음 코드에서 우리는 훈련과 검증 손실과 정확성을 포함한 학습 곡선을 그림.
* 다음 코드는 교육 성과를 표시

```python
>>> fig = plt.figure(figsize=(16, 4))
>>> ax = fig.add_subplot(1, 2, 1)
>>> plt.plot(history[0], lw=4)
>>> plt.plot(history[1], lw=4)
>>> plt.legend(['Train loss', 'Validation loss'], fontsize=15)
>>> ax.set_xlabel('Epochs', size=15)
>>> ax = fig.add_subplot(1, 2, 2)
>>> plt.plot(history[2], lw=4)
>>> plt.plot(history[3], lw=4)
>>> plt.legend(['Train acc.', 'Validation acc.'], fontsize=15)
>>> ax.set_xlabel('Epochs', size=15)
```

![124](https://user-images.githubusercontent.com/113302607/201677201-1bf40478-ee41-4e6c-ad0f-839f1e9bf705.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.3. Loss and accuracy results
</figcaption>
