---
layout: single
title:  "Chapter17. Generative Adversarial Networks for Synthesizing New Data: 새로운 데이터를 합성하기 위한 생성적 적대 네트워크"
use_math: true
---

#### Topic
* 이 장에서는 생성적 적대 네트워크(GAN)를 탐구하고 새로운 데이터 샘플을 합성하는 데 적용하는 방법을 알아봄. 
* GAN은 딥 러닝에서 가장 중요한 돌파구 중 하나로 간주되어 컴퓨터가 새로운 데이터(예: 새로운 이미지)를 생성할 수 있게함. 

## 1. Introducing generative adversarial networks: 생성적 적대 네트워크

* **GAN의 전반적인 목표는 훈련 데이터 세트와 동일한 분포를 가진 새로운 데이터를 합성하는 것임.** 
* 따라서, 원래 형태의 GAN은 레이블이 지정된 데이터가 필요하지 않기 때문에 기계 학습 작업의 **비지도 학습 범주**에 속하는 것으로 간주됨. 

#### Starting with autoencoders: 오토인코더로 시작 

* GAN의 작동 방식에 대해 논의하기 전에 먼저 훈련 데이터를 압축 및 압축 해제할 수 있는 자동 인코더로 시작함.
* 오토인코더는 인코더(encoder) 네트워크와 디코더(decoder) 네트워크라는 두 개의 네트워크로 구성
* 인코더 네트워크는 예시 x(즉, $x \in R^d$)와 연관된 d차원 입력 특징 벡터를 수신하여 p차원 벡터, z(즉, $x \in R^p$)로 인코딩함. 즉, 인코더의 역할은 $z = f(x)$를 모델링하는 방법을 배우는 것임. 인코딩된 벡터 z는 잠재 벡터 또는 잠재 특징 표현이라고도함.
* 일반적으로, 잠재 벡터의 차원성은 입력 예제의 차원보다 작음. 즉, p < d. 따라서, 우리는 인코더가 데이터 압축 기능으로 작용한다고 말할 수 있음.
* 그런 다음 디코더는 디코더를 함수로 생각할 수 있는 저차원 잠재 벡터 z에서 𝒙̂를 압축 해제함.
* 간단한 자동 인코더 아키텍처는 그림 1에 나타나 있는데, 여기서 인코더와 디코더 부분은 각각 완전히 연결된 하나의 레이어로만 구성됨. 

![1](https://user-images.githubusercontent.com/113302607/204251372-b3be04ac-8d5d-47b8-bb14-a50dad76d8df.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.1. The architecture of an autoencoder
</figcaption>

* 그림 1은 인코더와 디코더 내에 숨겨진 레이어가 없는 자동 인코더를 묘사하지만, 우리는 물론 비선형성을 가진 여러 숨겨진 레이어를 추가하여 보다 효과적인 데이터 압축 및 재구성 기능을 학습할 수 있는 심층 자동 인코더를 구성할 수 있음. 
* 또한 이 절에서 언급한 자동 인코더는 완전히 연결된 계층을 사용함. 그러나 우리가 이미지로 작업할 때 14장 "심층 컨볼루션 신경망으로 이미지 분류"에서 배웠듯 완전히 연결된 레이어를 컨볼루션 레이어로 대체할 수 있음.

#### Generative models for synthesizing new data: 새로운 데이터를 합성하기 위한 생성 모델

* 자동 인코더는 결정론적 모델이며, 이는 입력 x가 주어지면 자동 인코더가 압축된 버전의 입력을 저차원 공간에서 재구성할 수 있음을 의미
*  따라서 압축 표현의 변환을 통해 입력을 재구성하는 것 외에 새로운 데이터를 생성할 수 없음.
*  반면에 생성 모델은 (잠재 표현에 해당하는) 임의의 벡터 z로부터 새로운 예인 $\tilde{x}$를 생성할 수 있음.
*  생성 모델의 개략적인 표현은 다음 그림과 같음. 랜덤 벡터 z는 완전히 알려진 특성을 가진 분포에서 나오기 때문에 이러한 분포에서 쉽게 표본을 추출할 수 있음. 예를 들어 z의 각 원소는 [-1,1] 범위의 균일한 분포에서 나올 수 있음. 

![2](https://user-images.githubusercontent.com/113302607/204251402-1fa5e4e4-776d-4ec7-a14b-875365022fc1.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.2. TA generative model
</figcaption>

* 자동 인코더의 디코더 구성 요소가 생성 모델과 유사하다는 것을 알 수 있음. 특히 둘 다 잠재 벡터 z를 입력으로 받고 x와 같은 공간에서 출력을 반환(오토 인코더의 경우 $\hat{x}$는 입력 x의 재구성이고, 생성 모델의 경우 $\tilde{x}$는 합성 샘플임.)
* 이 둘의 주요 차이점은 자동 인코더에서 z의 분포를 모른다는 것이며, 생성 모델에서는 z의 분포를 완전히 특성화할 수 있다는 것임.
* 그러나 자동 인코더를 생성 모델로 일반화하는 것은 가능하다. 한 가지 접근법은 가변 자동 인코더(variational autoencoder; VAE)임. 입력 예 x를 수신하는 VAE에서 인코더 네트워크는 잠재 벡터의 분포의 두 모멘트인 평균, $\mu $, 분산 $ \sigma ^{2}$를 계산하는 방식으로 수정됨.
* VAE를 훈련하는 동안 네트워크는 이러한 모멘트를 표준 정규 분포(즉, 평균 및 단위 분산 0)의 모멘트와 일치시켜야함.
* 그런 다음 VAE 모델이 훈련된 후 인코더가 폐기되고, 우리는 디코더 네트워크를 사용하여 "학습된" 가우스 분포에서 무작위 z 벡터를 제공하여 새로운 예제인 𝒙̃를 생성할 수 있음

## 2. Generating new samples with GANs: GAN을 사용하여 새 샘플 생성

* GAN이 간단히 말해 무엇을 하는지 이해하기 위해 먼저 알려진 분포에서 샘플링된 랜덤 벡터 z를 수신하고 출력 이미지 x를 생성하는 네트워크가 있다고 가정
* 이 네트워크 생성기를 (generator; G)라고 부르고 생성된 출력을 참조하기 위해 표기법을 사용할 것임.
* 목표는 얼굴 이미지, 건물 이미지, 동물 이미지 또는 MNIST와 같은 손으로 쓴 숫자와 같은 일부 이미지를 생성하는 것이라고 가정
* 랜덤 가중치를 사용하여 이 네트워크를 초기화함. 따라서 이러한 가중치를 조정하기 전에 첫 번째 출력 영상은 흰색 노이즈처럼보임.
* 이미지의 품질을 평가할 수 있는 기능이 있다고 가정(평가자 기능(assessor function)이라고 부름).
* 그러한 기능이 존재하는 경우, 우리는 그 기능의 피드백을 사용하여 생성된 이미지의 품질을 향상시키기 위해 가중치를 조정하는 방법을 생성기 네트워크에 알려줄 수 있음.
* 이러한 방식으로, 우리는 해당 평가자 기능의 피드백을 기반으로 생성기를 훈련시켜, 생성기가 현실적으로 보이는 이미지를 생성하기 위해 출력을 개선하는 방법을 배울 수 있음.
* 앞 단락에서 설명한 것처럼 평가자 기능은 이미지 생성 작업을 매우 쉽게 만들 수 있지만, 문제는 이미지 품질을 평가하기 위한 그러한 보편적인 기능이 존재하는지 여부와 만약 그렇다면 어떻게 정의되는지임.


![3](https://user-images.githubusercontent.com/113302607/204251428-a84f179f-9eaf-4251-81dd-4097b5f3fa09.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.3. The discriminator distinguishes between the real image and the one created by the 
generator
</figcaption>

* 그림 3과 같이 GAN 모델은 판별기(discriminator; D)라고 불리는 추가적인 NN으로 구성되어 있는데, 이는 실제 이미지 x에서 합성된 이미지인 $\tilde{x}$를 감지하는 방법을 배우는 분류기임.
* GAN 모델에서 생성기와 판별기라는 두 개의 네트워크는 함께 훈련됨. 
* 먼저 모델 가중치를 초기화한 후 생성기는 현실적으로 보이지 않는 이미지를 생성함. 마찬가지로, 판별기는 생성기에 의해 합성된 실제 이미지와 이미지를 제대로 구별하지 못함. 
* 그러나 훈련을 통해 두 네트워크는 서로 상호작용하면서 더 좋아지게됨. 실제로 두 네트워크는 적대적 게임을 하며, 여기서 생성기는 판별기 속도를 속일 수 있도록 출력을 개선하는 방법을 배움. 동시에 판별기는 합성된 이미지를 더 잘 감지하게됨. 

## 3. Implementing the generator and the discriminator networks: 제너레이터 및 판별기 네트워크 구현

* 아래 그림에 나온 것처럼 하나 이상의 숨겨진 계층이 있는 두 개의 완전 연결 네트워크로서 발전기와 판별기를 가진 첫 번째 GAN 모델의 구현을 시작할 것임.

![4](https://user-images.githubusercontent.com/113302607/204251449-eb2d85cc-87e5-4bb5-9bb5-731a1c513005.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.4. depicts the original GAN based on fully connected layers, which we will refer to as a vanilla GAN.
</figcaption>

* 이 모델에서는 숨겨진 각 레이어에 대해 누출이 있는 ReLU 활성화 기능을 적용할 것임. ReLU를 사용하면 희소 그레이디언트가 발생하며, 이는 입력 값의 전체 범위에 대한 그레이디언트가 필요할 때 적합하지 않을 수있음.
* 판별기 네트워크에서, 각각의 숨겨진 계층은 또한 드롭아웃 계층에 따름. 또한 생성기의 출력 레이어는 쌍곡선 탄젠트(tanh) 활성화 함수를 사용(tanh 활성화는 학습에 도움이 되므로 발전기 네트워크에 사용하는 것이 좋음.)
* 판별기의 출력 레이어에는 로짓을 얻기 위한 활성화 기능(즉, 선형 활성화)이 없음. 또는, 우리는 S자형 활성화 함수를 사용하여 확률을 출력으로 얻을 수 있음.
* 두 네트워크 각각에 대해 두 개의 도우미 기능을 정의할 것이며, PyTorchn의 모델을 인스턴스화할 것임. 순차 클래스이며 설명에 따라 도면층을 추가함. 코드는 다음과 같음. 

```python
>>> import torch.nn as nn
>>> import numpy as np
>>> import matplotlib.pyplot as plt
>>> ## define a function for the generator:
>>> def make_generator_network(
... input_size=20,
... num_hidden_layers=1,
... num_hidden_units=100,
... num_output_units=784):
... model = nn.Sequential()
... for i in range(num_hidden_layers):
... model.add_module(f'fc_g{i}',
... nn.Linear(input_size, num_hidden_units))
... model.add_module(f'relu_g{i}', nn.LeakyReLU())
... input_size = num_hidden_units
... model.add_module(f'fc_g{num_hidden_layers}',
... nn.Linear(input_size, num_output_units))
... model.add_module('tanh_g', nn.Tanh())
... return model
>>>
>>> ## define a function for the discriminator:
>>> def make_discriminator_network(
... input_size,
... num_hidden_layers=1,
... num_hidden_units=100,
... num_output_units=1):
... model = nn.Sequential()
... for i in range(num_hidden_layers):
... model.add_module(
... f'fc_d{i}',
... nn.Linear(input_size, num_hidden_units, bias=False)
... )
... model.add_module(f'relu_d{i}', nn.LeakyReLU())
... model.add_module('dropout', nn.Dropout(p=0.5))
... input_size = num_hidden_units
... model.add_module(f'fc_d{num_hidden_layers}',
... nn.Linear(input_size, num_output_units))
... model.add_module('sigmoid', nn.Sigmoid())
... return model
```

* 다음으로 모델에 대한 교육 설정을 지정함. 삽화 목적으로만 매우 간단한 GAN 모델을 구현하고 완전히 연결된 계층을 사용하기 때문에, 우리는 각 네트워크에 100개의 단위가 있는 단일 숨겨진 계층만 사용할 것임.
* 다음 코드를 통해 두 네트워크를 지정하고 초기화하고 요약 정보를 인쇄함.

```python
>>> image_size = (28, 28)
>>> z_size = 20
>>> gen_hidden_layers = 1
>>> gen_hidden_size = 100
>>> disc_hidden_layers = 1
>>> disc_hidden_size = 100
>>> torch.manual_seed(1)
>>> gen_model = make_generator_network(
... input_size=z_size,
... num_hidden_layers=gen_hidden_layers,
... num_hidden_units=gen_hidden_size,
... num_output_units=np.prod(image_size)
... )
>>> print(gen_model)
Sequential(
 (fc_g0): Linear(in_features=20, out_features=100, bias=False)
 (relu_g0): LeakyReLU(negative_slope=0.01)
 (fc_g1): Linear(in_features=100, out_features=784, bias=True)
 (tanh_g): Tanh()
)
>>> disc_model = make_discriminator_network(
... input_size=np.prod(image_size),
... num_hidden_layers=disc_hidden_layers,
... num_hidden_units=disc_hidden_size
... )
>>> print(disc_model)
Sequential(
 (fc_d0): Linear(in_features=784, out_features=100, bias=False)
 (relu_d0): LeakyReLU(negative_slope=0.01)
 (dropout): Dropout(p=0.5, inplace=False)
 (fc_d1): Linear(in_features=100, out_features=1, bias=True)
 (sigmoid): Sigmoid()
)
```

#### Training the GAN model

* nn의 인스턴스를 만들 것임. BCELoss는 우리의 손실 함수로서 그리고 그것을 우리가 방금 처리한 배치와 관련된 발전기와 판별기에 대한 이진 교차 엔트로피 손실을 계산하는 데 사용함.
* 이를 위해 각 출력에 대한 실측 자료 레이블도 필요함. 생성기의 경우 생성된 이미지에 대한 예측 확률 d_proba_fake를 포함하는 벡터와 동일한 모양의 1s 벡터를 생성할 것임.
* 판별기 손실의 경우, d_proba_fake와 관련된 가짜 예제를 탐지하기 위한 손실과 d_proba_real에 기반한 실제 예제를 탐지하기 위한 손실의 두 가지 용어가 있음.
* 가짜 용어에 대한 기본 진실 레이블은 우리가 torch. zeros()(또는 torch.zeros_like()) 함수를 통해 생성할 수 있는 0s의 벡터가 될 것임. 마찬가지로, 우리는 1s의 벡터를 생성하는 torch.ones(또는 torch.one_like()) 함수를 통해 실제 이미지에 대한 실측값을 생성할 수 있음.

```python
>>> loss_fn = nn.BCELoss()
>>> ## Loss for the Generator
>>> g_labels_real = torch.ones_like(d_proba_fake)
>>> g_loss = loss_fn(d_proba_fake, g_labels_real)
>>> print(f'Generator Loss: {g_loss:.4f}')
Generator Loss: 0.6863
>>> ## Loss for the Discriminator
>>> d_labels_real = torch.ones_like(d_proba_real)
>>> d_labels_fake = torch.zeros_like(d_proba_fake)
>>> d_loss_real = loss_fn(d_proba_real, d_labels_real)
>>> d_loss_fake = loss_fn(d_proba_fake, d_labels_fake)
>>> print(f'Discriminator Losses: Real {d_loss_real:.4f} Fake {d_loss_
fake:.4f}')
Discriminator Losses: Real 0.6226 Fake 0.7007
```

* 이전 코드 예제는 GAN 모델 훈련 뒤의 전반적인 개념을 이해하기 위한 다양한 손실 항의 단계별 계산을 보여줌.
* 다음 코드는 GAN 모델을 설정하고 훈련 루프를 구현하며, 여기서 우리는 이러한 계산을 for 루프에 포함시킬 것임.
* 먼저 실제 데이터 세트, 제너레이터 및 판별기 모델에 대한 데이터 로더 설정과 두 모델 각각에 대한 별도의 Adam 옵티마이저 설정으로 시작함.

```python
>>> batch_size = 64
>>> torch.manual_seed(1)
>>> np.random.seed(1)
>>> mnist_dl = DataLoader(mnist_dataset, batch_size=batch_size,
... shuffle=True, drop_last=True)
>>> gen_model = make_generator_network(
... input_size=z_size,
... num_hidden_layers=gen_hidden_layers,
... num_hidden_units=gen_hidden_size,
... num_output_units=np.prod(image_size)
... ).to(device)
>>> disc_model = make_discriminator_network(
... input_size=np.prod(image_size),
... num_hidden_layers=disc_hidden_layers,
... num_hidden_units=disc_hidden_size
... ).to(device)
>>> loss_fn = nn.BCELoss()
>>> g_optimizer = torch.optim.Adam(gen_model.parameters())
>>> d_optimizer = torch.optim.Adam(disc_model.parameters())
```

![5](https://user-images.githubusercontent.com/113302607/204251495-3bd0c019-eb5b-46c6-a75f-3136a921a559.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.5. The discriminator performance
</figcaption>

* 앞 그림의 판별기 출력에서 알 수 있듯이, 훈련 초기 단계에서 판별기는 실제 사례와 가짜 사례를 꽤 정확하게 구별하는 것을 빠르게 배울 수 있었음. 즉, 가짜 사례는 확률이 0에 가까웠고, 실제 사례는 확률이 1에 가까웠음.
* 그 이유는 가짜 사례들이 실제 사례들과 전혀 다르기 때문에, 진짜와 가짜를 구별하는 것이 오히려 쉬웠기 때문임.
* 훈련이 더 진행될수록 생성기는 현실적인 이미지를 합성하는 데 더 능숙해질 것이며, 이는 0.5에 가까운 실제 사례와 가짜 사례 모두의 확률을 초래할 것임.
* 게다가, 우리는 또한 훈련 중에 생성기의 출력, 즉 합성된 이미지가 어떻게 변하는지 볼 수 있음. 다음 코드에서 에포크 선택을 위해 생성기에 의해 생성된 이미지 중 일부를 시각화할 것임.

```python
>>> selected_epochs = [1, 2, 4, 10, 50, 100]
>>> fig = plt.figure(figsize=(10, 14))
>>> for i,e in enumerate(selected_epochs):
... for j in range(5):
... ax = fig.add_subplot(6, 5, i*5+j+1)
... ax.set_xticks([])
... ax.set_yticks([])
... if j == 0:
... ax.text(
... -0.06, 0.5, f'Epoch {e}',
... rotation=90, size=18, color='red',
... horizontalalignment='right',
... verticalalignment='center',
... transform=ax.transAxes
... )
... 
... image = epoch_samples[e-1][j]
... ax.imshow(image, cmap='gray_r')
... 
>>> plt.show()
```

![6](https://user-images.githubusercontent.com/113302607/204251518-ad2c29dc-a21f-4ad8-8521-5a6761298023.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.6. Images produced by the generator
</figcaption>

* 위 그림에서 볼 수 있듯, 제너레이터 네트워크는 교육이 진행될수록 점점 더 사실적인 이미지를 생성했음.그러나 100년이 지난 후에도 생성된 이미지는 MNIST 데이터 세트에 포함된 손으로 쓴 숫자와 여전히 매우 다르게 보임.
* 이 섹션에서는 제너레이터와 판별기 모두에 대해 단일 완전히 연결된 숨겨진 레이어만 있는 매우 간단한 GAN 모델을 설계했음.
* 다음 섹션에서는 생성기와 판별기 네트워크 모두에 컨볼루션 레이어를 사용하는 심층 컨볼루션 GAN(DCGAN)을 구현

## 4. Improving the quality of synthesized images using a convolutional and Wasserstein GAN: 컨볼루션 및 Wasserstein GAN을 사용하여 합성된 이미지의 품질 향상

* 이 섹션에서는 DCGAN을 구현하여 이전 GAN 예제에서 본 성능을 향상시킬 수 있음. 또한, 추가 핵심 기술인 와서스테인 **WGAN(Wasserstein GAN)**에 대해 간략하게 이야기할 것임.


#### Transposed convolution

* 전치된 컨볼루션 연산을 이해 예시:
* 크기가 nxn인 입력 특성 맵이 있다고 가정, 그런 다음 특정 패딩 및 스트라이드 매개 변수가 있는 2D 컨볼루션 연산을 이 nxn 입력에 적용하여 크기가 mxm인 출력 특성 맵을 생성
* 이제 문제는 입력과 출력 사이의 연결 패턴을 유지하면서 이 mxm 출력 특성 맵에서 초기 치수 nxn의 피처 맵을 얻기 위해 다른 컨볼루션 연산을 어떻게 적용할 수 있는가 하는 것임.
* nxn 입력 행렬의 모양만 복구되고 실제 행렬 값은 복구되지 않음.

![7](https://user-images.githubusercontent.com/113302607/204251546-0500b328-973e-4457-9ddf-2ae11587ff01.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.7. Transposed convolution
</figcaption>

* 전치된 컨볼루션을 사용한 형상 맵 업샘플링은 입력 형상 맵의 요소 사이에 0을 삽입하여 작동함. 아래 그림은 2×2의 스트라이드와 2×2의 커널 크기를 가진 4×4 크기의 입력에 전치 컨볼루션을 적용하는 예를 보여줌. 
* 중앙의 9x9 크기 매트릭스는 입력 형상 맵에 이러한 0을 삽입한 후의 결과를 보여줌.
* 그런 다음 스트라이드가 1인 2×2 커널을 사용하여 정상적인 컨볼루션을 수행하면 크기가 8×8이 됨.
* 2의 스트라이드로 출력에 대해 정규 컨볼루션을 수행하여 원래 입력 크기와 동일한 크기인 4x4의 출력 피쳐 맵을 생성함으로써 역방향을 확인할 수 있음.

![8](https://user-images.githubusercontent.com/113302607/204251564-3bfda5a3-9613-4768-a1bb-93584e24cc05.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.8. Applying transposed convolution to a 4×4 input
</figcaption>


#### Implementing the generator and discriminator

* 모델이 학습하는 방식과 학습 과정에서 합성된 예제의 품질이 어떻게 변화하는지 보기 위해 일부 시대에 저장된 예제를 시각화함. 


![9](https://user-images.githubusercontent.com/113302607/204251583-3a25938a-6805-449c-9eed-d507518ecad4.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.9. Generated images from the DCGAN
</figcaption>

* vanilla GAN에 대한 섹션에서와 동일한 코드를 사용하여 결과를 시각화함. 새로운 예를 비교하면 DCGAN이 훨씬 높은 품질의 이미지를 생성할 수 있음을 알 수 있음. GAN 생성기의 결과를 어떻게 평가할 수 있는지 궁금할 수 있습니다. 가장 간단한 접근법은 시각적 평가로, 대상 도메인과 프로젝트 목표의 맥락에서 합성된 이미지의 품질을 평가하는 것을 포함.



