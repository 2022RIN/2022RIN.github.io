---
layout: single
title: "Chapter 18. 그래프 구조화 데이터에서 종속성을 포착하기 위한 그래프 신경망"
use_math: true
---

## Topic 

* 이 챕터에서 그래프 데이터, 즉 **그래프 신경망(graph neural network, GNNs)** 에서 작동하는 딥러닝 모델의 클래스를 소개함. 

## 1. 그래프 데이터 소개 

* 넓은 범위로, 그래프는 우리가 데이터에서 관계를 설명하고 포착하는 특정한 방식을 나타냄.
* 그래프는 **비선형적이고 추상적인 특정 종류의 데이터 구조이며 추상적인 개체** 이기 때문에 그래프를 조작할 수 있도록 구체적인 표현을 정의할 필요가 있음. 
* 또한, 그래프는 다른 표현을 필요로 할 수 있는 특정 속성을 가지도록 정의될 수 있음. 그림 1은 일반적인 유형의 그래프가 요약되어있으며 하위 섹션에서 더 자세히 설명함.

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.1. 일반적인 유형의 그래프 
</figcaption>

###  Undirected graphs: 무방향 그래프 

* **무방향 그래프(undirected graph):** 노드의 순서와 연결이 중요하지 않은 에지를 통해 연결된 노드(그래프 이론에서는 정점(vertices)이라고도 함)로 구성됨. 

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.2. 무방향 그래프의 두 가지 유형 
</figcaption>

* 무방향 그래프로 표현될 수 있는 데이터의 다른 일반적인 예로는 이미지, protein-protein 상호작용 네트워크, 포인트 클라우드가 있음.
* 수학적으로 무방향 그래프 $G$는 한 쌍 $(V, E)$이며, 여기서 $V$는 그래프의 노드 집합이고, $E$는 한 쌍의 노드를 구성하는 에지 집합임.
* 행렬 A의 각 요소 $x_{ij}$는 1 또는 0이며, 1은 노드 $i$와 $j$ 사이의 에지를 나타냄. 그래프가 방향성이 없기 때문에 A의 추가 속성은 $x_{ij} = x_{ji}$임. 

###  Directed graphs: 방향 그래프 

* **방향 그래프(Directed graph):** 무방향 그래프와 달리 방향 그래프는 방향 에지를 통해 노드를 연결함. 

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.3. 방향 그래프의 예시 
</figcaption>

* 수학적으로 이들은 변의 집합인 $E$가 순서쌍의 집합이라는 점을 제외하고는 무방향 그래프와 같은 방식으로 정의됨.
* 따라서 $x_{ij} = x_{ji}$일 필요가 없음. 
* 방향 그래프의 예로는 인용 네트워크가 있는데, 여기서 노드는 출판물이고 노드의 에지는 주어진 논문이 인용한 논문의 노드로 향한다.

###  Labeled graphs: 레이블이 있는 그래프 

* 우리가 작업하는 데 관심이 있는 많은 그래프에는 각 노드와 에지와 관련된 추가 정보가 있음.
* 예를 들어, 카페인 분자를 고려하면, 분자는 각 노드가 화학 원소(예: O, C, N 또는 H 원자)이고 각 모서리가 두 노드 사이의 결합 유형(예: 단일 또는 이중 결합)인 그래프로 나타낼 수 있음. 
* 이러한 노드 및 에지 기능은 일부 용량으로 인코딩되어야함. 
* 노드 세트와 에지 세트 튜플(V, E)로 정의된 그래프 G가 주어지면, 우리는 $|V|×f_V$ 노드 피처 매트릭스 $X$를 정의함. 여기서 $f_V$는 각 노드의 레이블 벡터의 길이를 의미함. 
* 에지 레이블의 경우 $|E|×f_E$ 에지 피처 매트릭스 $X_E$를 정의하며, 여기서 $f_E$는 각 에지의 레이블 벡터의 길이임. 
* 분자는 레이블링된 그래프로 표현될 수 있는 데이터의 훌륭한 예이며, 우리는 이 장 전체에 걸쳐 분자 데이터를 연구할 것임. 따라서 이번 기회에 다음 섹션에서 그들의 대표성을 자세히 다룰 것임.

###  Representing molecules as graphs: 분자를 그래프로 나타냄

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.4. 카페인 분자의 그래프 표현 
</figcaption>

* 화학적 개요로서, 분자는 화학 결합에 의해 결합된 원자들의 그룹으로 생각될 수 있음. 서로 다른 화학 원소들에 대응하는 다른 원자들이 있는데, 예를 들어, 공통 원소로는 탄소(C), 산소(O), 질소(N), 수소(H)가 있음.
* 또한, 원자들 사이의 연결을 형성하는 다른 종류의 결합들이 있는데, 예를 들어, 단일 또는 이중 결합들임.
* 분자를 노드 레이블 매트릭스로 표현할 수 있으며, 여기서 각 행은 관련 노드의 원자 유형에 대한 단일 핫 인코딩이며 또한, 각 행이 연관된 에지의 결합 유형의 단일 핫 인코딩인 에지 레이블 매트릭스가 있음.
* 이 표현을 단순화하기 위해 수소 원자의 위치를 기본적인 화학 규칙으로 추론할 수 있기 때문에 때때로 암시적으로 만들어짐.
* 앞서 본 카페인 분자를 고려하여 암시적 수소 원자를 사용한 그래프 표현의 예는 그림 4에 나와 있음.

## 2. 그래프 컨벌루션의 이해 

* 이전 섹션에서는 그래프 데이터를 표현하는 방법을 보여주었음. 다음은 이러한 표현을 효과적으로 활용할 수 있는 도구에 대해 논의함.
* 다음 하위 섹션에서는 GNN을 구축하기 위한 핵심 구성 요소인 그래프 컨볼루션을 소개함.

###  그래프 컨볼루션 사용 동기 

* 이미지의 맥락에서, 컨볼루션 필터를 이미지 위로 미끄러뜨리는 과정으로 생각할 수 있으며, 각 단계에서 필터와 수용 필드(현재 이미지의 상단 부분) 사이에서 가중 합계가 계산됨.
* CNN 장에서 논의한 바와 같이 필터는 특정 기능의 검출기로 볼 수 있음. 특성 감지에 대한 이 접근 방식은 예를 들어 이미지 데이터에 다음과 같은 몇 가지 이유로 이미지에 적합: 
    * **1) 시프트 불변성:** 이미지 특성은 위치에 관계없이(예: 변환 후) 인식할 수 있음. 고양이는 왼쪽 위, 오른쪽 아래, 이미지의 다른 부분에 있든 고양이로 인식될 수 있음.
    * **2) 지역성:** 주변 픽셀들은 밀접하게 연관되어 있음.
    * **3) 계층:** 이미지의 더 큰 부분은 종종 관련된 더 작은 부분의 조합으로 나눌 수 있음. 고양이는 머리와 다리를 가지고 있다; 머리는 눈과 코를 가지고 있다; 눈은 동공과 홍채를 가지고 있다. 와 같이 나눌 수 있음. 
    
    
## 3. 그래프 컨벌루션 구현 

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.5. 그래프의 표현 
</figcaption>

* 그림 5는 $nxn$ 인접 행렬 A와 $nxf_{in}$ 노드 특징 행렬 X에 의해 지정된 노드 레이블이 있는 무방향 그래프를 보여줌.
* 여기서 유일한 특성은 각 노드의 색상(녹색(G), 파란색(B) 또는 주황색(O)의 원핫 표현임.
* 그래프 조작 및 시각화를 위한 가장 다목적인 라이브러리 중 하나는 NetworkX이며, 레이블 행렬 X와 노드 행렬 A에서 그래프를 구성하는 방법을 설명하는 데 사용할 것임.

```python
>>> import numpy as np
>>> import networkx as nx
>>> G = nx.Graph()
... # Hex codes for colors if we draw graph
>>> blue, orange, green = "#1f77b4", "#ff7f0e", "#2ca02c"
>>> G.add_nodes_from([
... (1, {"color": blue}),
... (2, {"color": orange}),
... (3, {"color": blue}),
... (4, {"color": green})
... ])
>>> G.add_edges_from([(1,2), (2,3), (1,3), (3,4)])
>>> A = np.asarray(nx.adjacency_matrix(G).todense())
>>> print(A)
[[0 1 1 0]
[1 0 1 0]
[1 1 0 1]
[0 0 1 0]]
>>> def build_graph_color_label_representation(G, mapping_dict):
... one_hot_idxs = np.array([mapping_dict[v] for v in
... nx.get_node_attributes(G, 'color').values()])
>>> one_hot_encoding = np.zeros(
... (one_hot_idxs.size, len(mapping_dict)))
>>> one_hot_encoding[
... np.arange(one_hot_idxs.size), one_hot_idxs] = 1
>>> return one_hot_encoding
>>> X = build_graph_color_label_representation(
... G, {green: 0, blue: 1, orange: 2})
>>> print(X)
[[0., 1., 0.],
[0., 0., 1.],
[0., 1., 0.],
[1., 0., 0.]]
```

* 이전 코드에서 구성된 그래프를 그리기 위해 다음 코드를 사용

```python
>>> color_map = nx.get_node_attributes(G, 'color').values()
>>> nx.draw(G,with_labels=True, node_color=color_map)
```

* 그래프 컨볼루션을 사용하여 X의 각 행을 해당 행에 해당하는 노드에 저장된 정보의 임베딩으로 해석할 수 있음. 
* 그래프 컨볼루션은 이웃과 자신의 임베딩을 기반으로 각 노드의 임베딩을 업데이트함. 예제 구현의 경우 그래프 컨볼루션은 다음과 같은 형태를 취함.

(1)


<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.6. 그래프와 메시지 양식에 구현된 컨볼루션(메시지 전달 아이디어를 시각화하고 구현한 컨볼루션 요약)
</figcaption>


## 4. PyTorch에서 처음부터 GNN 구현

* 그래프 신경망의 기본 구현에 대해 설명함. 

###  노드네트워크 모델 정의

* 우리는 GNN의 PyTorch를 처음부터 끝까지 구현하는 것으로 이 섹션을 시작함. NodeNetwork라고 부르는 주요 신경망 모델을 시작으로 하향식 접근을 할 것임. 그리고 나서 개별 세부 사항을 채울 것임.

```python
import networkx as nx
import torch
from torch.nn.parameter import Parameter
import numpy as np
import math
import torch.nn.functional as F
class NodeNetwork(torch.nn.Module):
 def __init__(self, input_features):
 super().__init__()
 self.conv_1 = BasicGraphConvolutionLayer (
 input_features, 32)
 self.conv_2 = BasicGraphConvolutionLayer(32, 32)
 self.fc_1 = torch.nn.Linear(32, 16)
 self.out_layer = torch.nn.Linear(16, 2)
 def forward(self, X, A, batch_mat):
 x = F.relu(self.conv_1(X, A))
 x = F.relu(self.conv_2(x, A))
 output = global_sum_pool(x, batch_mat)
 output = self.fc_1(output)
 output = self.out_layer(output)
 return F.softmax(output, dim=1)
```

* 위 코드에서 정의한 NodeNetwork 모델은 다음과 같이 요약할 수 있음. 
    * 1) 두 개의 그래프 컨볼루션 **(self.conv_1 및 self.conv_2)** 을 수행
    * 2) 나중에 정의할 **global_sum_pool**을 통해 모든 노드 임베딩을 풀링
    * 3) 풀링된 임베딩을 두 개의 완전히 연결된 레이어**(self.fc_1 및 self.out_layer)** 를 통해 실행
    * 4) 소프트맥스를 통해 클래스 멤버쉽 확률 출력

* 각 계층이 무엇을 하고 있는지에 대한 시각화와 함께 네트워크의 구조는 다음과 같이 요약됨.

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.7. 각 신경망 계층의 시각화 
</figcaption>


###  노드네트워크 그래프 컨볼루션 레이어 코딩 

* 이전 NodeNetwork 클래스 내에서 사용된 그래프 컨볼루션 작업(BasicGraphConvolutionLayer)을 정의함.

```python
class BasicGraphConvolutionLayer(torch.nn.Module):
 def __init__(self, in_channels, out_channels):
 super().__init__()
 self.in_channels = in_channels
  self.out_channels = out_channels
 self.W2 = Parameter(torch.rand(
 (in_channels, out_channels), dtype=torch.float32))
 self.W1 = Parameter(torch.rand(
 (in_channels, out_channels), dtype=torch.float32))
 
 self.bias = Parameter(torch.zeros(
 out_channels, dtype=torch.float32))
 def forward(self, X, A):
 potential_msgs = torch.mm(X, self.W2)
 propagated_msgs = torch.mm(A, potential_msgs)
 root_update = torch.mm(X, self.W1)
 output = propagated_msgs + root_update + self.bias
 return output
```

* 완전히 연결된 레이어 및 이미지 컨볼루션 레이어와 마찬가지로 (ReLU와 같은 비선형성을 적용하기 전에) 레이어 출력의 선형 조합의 인터셉트가 달라질 수 있도록 편향 용어를 추가함. 
* forward() 방법은 이전 하위 섹션에서 논의한 포워드 패스의 행렬 형태를 바이어스 항을 추가하여 구현함. 

* 기본 그래프 컨볼루션 레이어를 사용하려면 이전에 기본 그래프 컨볼루션 구현: 섹션에서 정의한 그래프 및 인접 행렬에 적용함.

```python
>>> print('X.shape:', X.shape)
X.shape: (4, 3)
>>> print('A.shape:', A.shape)
A.shape: (4, 4)
>>> basiclayer = BasicGraphConvolutionLayer(3, 8)
>>> out = basiclayer(
... X=torch.tensor(X, dtype=torch.float32),
... A=torch.tensor(A, dtype=torch.float32)
... )
>>> print('Output shape:', out.shape)
Output shape: torch.Size([4, 8])
```

* 위의 코드 예제를 바탕으로, 우리는 우리의 기본 그래프 컨볼루션 레이어가 세 가지 특징으로 구성된 네 개의 노드 그래프를 여덟 가지 특징을 가진 표현으로 변환했음을 알 수 있음.


###  다양한 그래프 크기를 처리하기 위해 글로벌 풀링 계층 추가 

* 다음으로 NodeNetwork 클래스에서 사용된 global_sum_pool() 함수를 정의함.
* 여기서 global_sum_pool()은 글로벌 풀링 계층을 구현함. 
* 글로벌 풀링 계층은 그래프의 모든 노드 임베딩을 고정 크기 출력으로 집계함.

* 모든 노드 임베딩을 합산하면 정보 손실이 발생하므로 데이터를 재구성하는 것이 바람직하지만 그래프의 크기가 다를 수 있으므로 실현 가능하지 않음. 
* 전역 풀링은 합계, 최대값 및 평균과 같은 순열 불변 함수를 사용하여 수행할 수 있음. 다음은 global_sum_pool()의 구현임.

```python
def global_sum_pool(X, batch_mat):
 if batch_mat is None or batch_mat.dim() == 1:
 return torch.sum(X, dim=0).unsqueeze(0)
 else:
 return torch.mm(batch_mat, X)
```

* 데이터가 일괄 처리되지 않거나 배치 크기가 1인 경우 이 함수는 현재 노드 임베딩을 합산함.
* 그렇지 않으면 임베딩은 그래프 데이터가 배치되는 방식에 기초한 구조를 가진 batch_mat와 곱해짐. 
* 데이터 집합의 모든 데이터가 동일한 차원을 가질 때, 데이터를 배치하는 것은 데이터를 쌓음으로써 차원을 추가하는 것처럼 간단함.
* 그래프 크기가 다양하기 때문에 패딩을 사용하지 않는 한 이 접근법은 그래프 데이터에서 실현 가능하지 않음. 그러나 그래프 크기가 크게 다를 수 있는 경우 패딩은 비효율적일 수 있음.
* 일반적으로 다양한 그래프 크기를 처리하는 더 좋은 방법은 각 배치를 단일 그래프로 처리하는 것임. 여기서 배치의 각 그래프는 나머지 그래프와 연결이 끊어진 하위 그래프임. 이는 아래 그림에 설명되어있음. 

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.8. 다양한 그래프 크기를 처리하는 방법 
</figcaption>


###  데이터 로더 준비 

* 이 섹션에서는 이전 하위 섹션의 코드가 모두 어떻게 결합되는지 알아봄. 
* 먼저 몇 가지 그래프를 생성하여 PyTorch 데이터 세트에 넣은다음 GNN에 대해 데이터 로더의 콜릿 함수(collate function)를 사용할 것임. 그러나 그래프를 정의하기 전에 나중에 사용할 사전 표현을 구축하는 함수를 만듦.

```python
def get_graph_dict(G, mapping_dict):
 # Function builds dictionary representation of graph G
 A = torch.from_numpy(
 np.asarray(nx.adjacency_matrix(G).todense())).float()
 # build_graph_color_label_representation()
 # was introduced with the first example graph
 X = torch.from_numpy(
 build_graph_color_label_representation(
 G, mapping_dict)).float()
 # kludge since there is not specific task for this example
 y = torch.tensor([[1,0]]).float()
 return {'A': A, 'X': X, 'y': y, 'batch': None}
```

* 이 함수는 NetworkX 그래프를 사용하여 인접 행렬 A, 노드 특성 행렬 X 및 이진 레이블이 포함된 사전을 반환함. 
* 실제 작업에서 이 모델을 훈련하지 않기 때문에 레이블을 임의로 설정함.
* 그런 다음 nx.adjacency_matrix()는 NetworkX 그래프를 가져와서 todense()를 사용하여 조밀한 np.array 형태로 변환하는 희소 표현을 반환
* 이제 그래프를 구성하고 get_graph_dict 함수를 사용하여 NetworkX 그래프를 네트워크에서 처리할 수 있는 형식으로 변환

```python
>>> # building 4 graphs to treat as a dataset
>>> blue, orange, green = "#1f77b4", "#ff7f0e","#2ca02c"
>>> mapping_dict= {green:0, blue:1, orange:2}
>>> G1 = nx.Graph()
>>> G1.add_nodes_from([
... (1,{"color": blue}),
... (2,{"color": orange}),
... (3,{"color": blue}),
... (4,{"color": green})
... ])
>>> G1.add_edges_from([(1, 2), (2, 3), (1, 3), (3, 4)])
>>> G2 = nx.Graph()
>>> G2.add_nodes_from([
... (1,{"color": green}),
... (2,{"color": green}),
... (3,{"color": orange}),
... (4,{"color": orange}),
... (5,{"color": blue})
... ])
>>> G2.add_edges_from([(2, 3),(3, 4),(3, 1),(5, 1)])
>>> G3 = nx.Graph()
>>> G3.add_nodes_from([
... (1,{"color": orange}),
... (2,{"color": orange}),
... (3,{"color": green}),
... (4,{"color": green}),
... (5,{"color": blue}),
... (6,{"color":orange})
... ])
>>> G3.add_edges_from([(2,3), (3,4), (3,1), (5,1), (2,5), (6,1)])
>>> G4 = nx.Graph()
>>> G4.add_nodes_from([
... (1,{"color": blue}),
... (2,{"color": blue}),
... (3,{"color": green})
... ])
>>> G4.add_edges_from([(1, 2), (2, 3)])
>>> graph_list = [get_graph_dict(graph, mapping_dict) for graph in
... [G1, G2, G3, G4]]
```

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.9. 생성된 4개의 그래프
</figcaption>

* 이 코드 블록은 4개의 NetworkX 그래프를 구성하여 목록에 저장함.
* 그래프의 add_edges_from() 메서드는 각 튜플이 요소(노드) 사이의 에지를 정의하는 튜플 목록을 가져옴. 이제 이 그래프에 대한 PyTorch 데이터 세트를 구성할 수 있음.
* 사용자 지정 데이터 세트를 사용하는 것이 불필요한 노력으로 보일 수 있지만, 데이터 로더에서 collate_graphs()를 사용하는 방법을 보여줄 수 있음. 

```python
from torch.utils.data import Dataset
class ExampleDataset(Dataset):
 # Simple PyTorch dataset that will use our list of graphs
 def __init__(self, graph_list):
 self.graphs = graph_list
 def __len__(self):
 return len(self.graphs)
 def __getitem__(self,idx):
 mol_rep = self.graphs[idx]
 return mol_rep
```

## 5. 파이썬 기하학 라이브러리를 이용한 GNN 구현 

* 이 섹션에서는 GNN을 훈련하는 과정을 단순화하는 PyTorch 기하학 라이브러리를 사용하여 GNN을 구현할 것임. 우리는 작은 분자로 구성된 데이터 세트인 QM9에 GNN을 적용하여 전기장에 의해 전하가 왜곡되는 분자의 경향을 나타내는 등방성 분극성을 예측함.

* 먼저 작은 분자의 데이터 세트를 로드하고 PyTorch Geometric이 데이터를 저장하는 방법을 살펴봄.


```python
>>> # For all examples in this section we use the following imports.
>>> # Note that we are using torch_geometric's DataLoader.
>>> import torch
>>> from torch_geometric.datasets import QM9
>>> from torch_geometric.loader import DataLoader
>>> from torch_geometric.nn import NNConv, global_add_pool
>>> import torch.nn.functional as F
>>> import torch.nn as nn
>>> import numpy as np
>>> # let's load the QM9 small molecule dataset
>>> dset = QM9('.')
>>> len(dset)
130831
>>> # Here's how torch geometric wraps data
>>> data = dset[0]
>>> data
Data(edge_attr=[8, 4], edge_index=[2, 8], idx=[1], name="gdb_1", pos=[5, 3], 
x=[5, 11], y=[1, 19], z=[5])
>>> # can access attributes directly
>>> data.z
tensor([6, 1, 1, 1, 1])
>>> # the atomic number of each atom can add attributes
>>> data.new_attribute = torch.tensor([1, 2, 3])
>>> data
Data(edge_attr=[8, 4], edge_index=[2, 8], idx=[1], name="gdb_1", new_
attribute=[3], pos=[5, 3], x=[5, 11], y=[1, 19], z=[5])
>>> # can move all attributes between devices
>>> device = torch.device(
... "cuda:0" if torch.cuda.is_available() else "cpu"
... )
>>> data.to(device)
>>> data.new_attribute.is_cuda
True
```

* Data 객체는 그래프 데이터를 위한 편리하고 유연한 래퍼임.
* 많은 파이토치 기하학적 객체가 데이터 객체를 올바르게 처리하려면 데이터 객체에 특정 키워드가 필요함.
* 특히, x는 노드 특성을 포함해야 하고, edge_attr은 에지 특성을 포함해야 하며, edge_index는 에지 리스트를 포함해야 하며, y는 레이블을 포함해야함.
* QM9 데이터에는 주목할 만한 몇 가지 추가 속성이 포함되어 있음. pos, 3D 그리드에서 각 분자의 원자 위치, 그리고 z, 분자 내 각 원자의 원자 번호임.
* QM9의 라벨은 쌍극자 모멘트, 자유 에너지, 엔탈피 또는 등방성 분극과 같은 분자의 물리적 특성임.
* 이제 검정 데이터의 부분 집합을 사용하여 산점도를 만들 수 있음.
* 테스트 데이터 세트가 상대적으로 크기 때문에(10,000개의 분자) 결과가 다소 혼란스러울 수 있으며, 단순성을 위해 처음 500개의 예측과 대상만 플롯함. 


<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.10 실제 등방성 분극성에 대한 예측 등방성 분극성
</figcaption>

* 그래프를 기반으로, 점이 상대적으로 대각선 근처에 있다는 점을 고려할 때, 우리의 단순한 GNN은 초 매개 변수 조정 없이도 등방성 편광 값을 예측하는 데 적절한 일을 한 것으로 보임.










