---
layout: single
title:  "Chapter7. Combining Different Models for Ensemble Learning"
use_math: true
---

## 1. Learning with ensembles

* <mark style='background-color: #fff5b1'>앙상블 방법(ensemble methods)</mark> 의 목표는 **서로 다른 분류기를 각각의 개별로서의 단독 분류기보다 성능이 우수한 하나의 메타 분류기로 결합하는 것임.**
* 예를 들어, 10명의 전문가로부터 예측을 수집했을 때, 앙상블 방법을 사용하면 10명의 전문가의 예측을 결합하여 각 개별 전문가의 예측보다 더 정확하고 강력한 예측을 도출할 수 있음.
* 앙상블에는 몇 가지 다른 접근법이 존재하며, 이 절에서는 앙상블의 작동원리와 앙상블이 좋은 성능을 가지는 이유에 대해 알아봄.

![1](https://user-images.githubusercontent.com/113302607/196036804-692b08c3-42ca-4355-ae6b-753ef2eee333.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.1. The different voting concepts
</figcaption>

* 가장 인기 있는 앙상블 방법은 **과반수 투표(majority voting)** 방식임. 이는 이름에서 알 수 있듯 분류기의 **과반수가 예측한 클래스 레이블을 선택**하는 방법임.
* 과반수 투표는 이중 클래스 분류에 해당되지만, 다중 클래스 문제에서의 일반화도 가능함. 이를 **다수결 투표(plurality voting)** 라고 하며 최빈값을 선택하는 방법임. 
* 위 그림은 10개의 분류기로 구성된 앙상블에 대한 **과반수 투표(majority voting)** 및 **다수결 투표(plurality voting)** 의 개념을 보여줌. 여기서 각 고유한 기호(삼각형, 사각형 및 원)는 고유한 클래스 레이블을 나타냄.

![2](https://user-images.githubusercontent.com/113302607/196036808-cffb8eab-bacd-4751-a7d6-f2f5813b6c06.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.2. A general ensemble approach
</figcaption>

* 훈련 데이터 세트를 사용하여 $m$개의 다른 분류기$(C1, ..., Cm)$를 훈련하는 것으로 시작하며 이를 C라고 명명함. 
* 앙상블 방법에 따라, 결정 트리, 지원 벡터 머신, 로지스틱 회귀 분류기 등과 같은 다양한 알고리즘을 사용하여 구축 또는 훈련 데이터 세트의 부분 집합을 달리하여 훈련할 수 있음.
* 유명한 예는 서로 다른 의사 결정 트리 분류기를 결합한 랜덤 포레스트(random forest) 알고리듬이 있음.
* 위 그림은 다수결 투표를 이용한 일반적인 앙상블 접근법의 개념을 보여줌.
* 과반수 투표나 다수결 투표를 통해 클래스 레이블을 예측하기 위해 개별 분류기의 $C_{j}$의 예측 클래스 레이블을 결합하여 가장 많은 표를 얻은 클래스 레이블 $\hat{y}$을 선택할 수 있음.

$\hat{y} = mode\{C_{1}(x),C_{2}(x),\cdots,C_{m}(x)\}$

* 위 식에서와 같이 과반수 투표나 다수결 투표로 예측을 하면 개별 분류기의 예측 레이블을 모아 가장 많은 표를 받은 레이블 \hat{y}을 선택함.
* (위 식에서) 통계학에서 mode는 집합에서 가장 빈번한 event 또는 result를 의미함.
* 예를 들어, $class1 = –1$  및 $class2 = +1$인 이진 분류 작업에서 과반수 투표 예측은 다음과 같음.

$$ C(x)=sign \[ \sum_{j}^{m} C_{j}(x)\] = \left\{\begin{matrix}
1 & if \sum_{j} C_{j}(x) \geq 0\\ 
-1 & otherwise
\end{matrix}\right. $$ 

* **앙상블 방법(하나의 메타 분류기)이 개별 분류기보다 더 잘 작동할 수 있는 이유**를 설명하기 위해 조합론의 몇 가지 개념을 적용함.
* 예를 들어, 이진 분류 작업에 대한 동일한 에러율 $\varepsilon$를 가지는 $n$개의 분류기가 있으며, $n$개의 분류기가 서로에게 영향을 주지 않는 독립적 관계라고 가정함.
* 위와 같은 가정하에, 단순히 기저 분류기의 앙상블의 오차 확률을 이항 분포의 확률 질량 함수로 표현할 수 있음.

$P(y\geq k)=\sum_{k}^{n} \left \langle nk\right \rangle \varepsilon^{k}(1-0.25)^{n-k}=\varepsilon^{ensemble}$

* 위 식에서, 이항 계수로 $n$개의 원소에서 $k$개를 뽑는 조합을 의미함. 이 식은 앙상블이 틀릴 확률을 계산함. 
* 에러율이 $0.25$인 분류기 $11$개로 구성된 앙상블 에러율은 $0.034$의 값을 가짐.
* 위와 같이 계산이 됐을 때, 각 개별 분류기의 오차율인 $0.25$보다 앙상블의 오차율이 확연히 낮은 것을 확인할 수 있음.
* 만약, 에러율이 $0.5$인 분류기가 짝수라 예측이 반반으로 분할되면 에러로 취급됨.
* 일단 이상주의적 앙상블 분류기를 다양한 기본 에러율 범위에 걸쳐 기본 분류기와 비교하기 위해, 파이썬에서 확률 질량 함수를 아래와 같이 구현함. 

```python
>>> from scipy.special import comb
>>> import math
>>> def ensemble_error(n_classifier, error):
... k_start = int(math.ceil(n_classifier / 2.))
... probs = [comb(n_classifier, k) *
... error**k *
... (1-error)**(n_classifier - k)
... for k in range(k_start, n_classifier + 1)]
... return sum(probs)
>>> ensemble_error(n_classifier=11, error=0.25)
0.03432750701904297
```

* **angemble_error** 함수를 구현한 후, **0.0**에서 **1.0**까지의 다양한 기본 오류 범위에 대한 앙상블 에러율을 계산하며, 다음 코드를 통해 앙상블과 기본 오류 사이의 관계를 시각화할 수 있음. 

```python
>>> import numpy as np
>>> import matplotlib.pyplot as plt
>>> error_range = np.arange(0.0, 1.01, 0.01)
>>> ens_errors = [ensemble_error(n_classifier=11, error=error)
... for error in error_range]
>>> plt.plot(error_range, ens_errors,
... label='Ensemble error',
... linewidth=2)
>>> plt.plot(error_range, error_range,
... linestyle='--', label='Base error',
... linewidth=2)
>>> plt.xlabel('Base error')
>>> plt.ylabel('Base/Ensemble error')
>>> plt.legend(loc='upper left')
>>> plt.grid(alpha=0.5)
>>> plt.show()
```

![3](https://user-images.githubusercontent.com/113302607/196036814-f249c137-496a-48a7-befc-25d8277f2cda.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.3. A plot of the ensemble error versus the base error
</figcaption>

* 위 결과 그래프에서 점선은 개별 분류기 오차, 실선은 앙상블 오차를 나타냄.
* 그래프를 통해 **앙상블의 에러 확률이 개별 분류기 보다 항상 좋은 값($\varepsilon < 0.5)$을 가지는 것을 확인할 수 있음.** 
* 하지만, 개별 분류기의 에러율이 $0.5$보다 낮은 값을 가져야하는 조건을 가짐.

## 2. Combining classifiers via majority vote
* 파이썬에서 다수결 투표를 위한 간단한 앙상블 분류기를 구현

#### * Implementing a simple majority vote classifier

* 다수결 투표에서는 분류 모델의 신뢰도에 가중치를 부여함. 가중치가 부여된 다수결 투표(the weighted majority vote)는 아래와 같은 수학적 수식으로 표현됨. 

$\hat{y} = argmax\sum_{j=1}^{m}w_{j}\chi _{A}(C_{j}(x)=i)$

* 위 식에서, $w$는 $c$라는 분류기 연관된 가중치이며, $\hat{y}$은 앙상블에서 예측한 클래스 레이블, $\chi$는 특성함수를 의미함.
* 특성함수를 이용해서 가중치가 동일하다는 가정 하에 다수결 투표의 식을 다음과 같이 간단하게 표현이 가능함. 

$\hat{y} = mode{C_{1}(x), C_{2}(x), ... ,C_{m}(x)}$


* **가중치 개념을 더 잘 이해하기 위해, 좀 더 구체적인 예를 살펴봄.**

* 세 가지 기본 분류기 $C_{j}(j\in {1,2,3})$의 앙상블이 있고 주어진 예제의 클래스 레이블 $C_{j}(x)\in \{0,1 \}$을 예측하고 싶다고 가정
* 세 가지 중 두 가지 기본 분류기는 클래스 레이블을 0으로 예측하고, 한 가지 $C_{3}$는 해당 예가 1에 속한다고 예측, 각 기본 분류기의 예측에 균등하게 가중치를 부여하면 다수결은 예제가 클래스 0에 속한다고 예측함. 

$C_{1}\rightarrow 0, C_{2}\rightarrow 0, C_{3}\rightarrow 1$

$\hat{y} = mode \{0,0,1\} = 0$

* 이제, $C_{3}$에 0.6의 가중치를 할당하고, $C_{1}$과 $C_{2}$에 0.2의 계수를 부여하면, 1의 결과값을 얻음.
* 보다 간단하게, 3×0.2 = 0.6이므로, $C_{3}$에 의한 예측은 $C_{1}$과 $C_{2}$에 의한 예측보다 3배 더 많은 가중치를 가지고 있다고 말할 수 있으며, 이를 다음과 같이 쓸 수 있음.

$\hat{y} = mode \{0,0,1,1,1\} = 1$

* 가중 다수결의 개념을 파이썬 코드로 변환하기 위해, **NumPy**의 편리한 **argmax** 및 **bincount** 함수를 사용할 수 있음. 
* 그런 다음 **argmax** 함수는 다수 클래스 레이블에 해당하는 가장 높은 카운트의 인덱스 위치를 반환(이것은 클래스 레이블이 0에서 시작한다고 가정함).

```python
to the majority class label (this assumes that class labels start at 0):
>>> import numpy as np
>>> np.argmax(np.bincount([0, 0, 1],
... weights=[0.2, 0.2, 0.6]))
1
```

* 확률로부터 클래스 레이블을 예측하기 위한 다수결의 수정된 버전은 다음과 같이 작성됨.

$\hat{y} = argmax\sum_{j=1}^{m} w_{j}p_{ij}$

* 위 식에서 $p_{ij}$는 클래스 레이블 $i$에 대한 $j$번째 분류기의 확률임. 
* 이전 예제를 계속하기 위해 클래스 레이블 $C_{j}(j\in {1,2,3})$ 및 세 가지 분류기로 구성된 앙상블$C_{j}(x)\in \{0,1 \}$에 이진 분류 문제가 있다고 가정
* 분류자 $C_{j}$가 특정 예제 $x$에 대해 다음과 같은 클래스 멤버쉽 확률을 반환한다고 가정

$C_{1}(x)\rightarrow [0.9,0.1], C_{2}(x)\rightarrow [0.8,0.2], C_{3}(x)\rightarrow [0.4,0.6]$

* 이전과 동일한 가중치(0.2, 0.2, 0.6)를 사용하여 다음과 같이 개별 클래스 확률을 계산할 수 있음. 

$$
p(i_{0}|x)= 0.2\times 0.9 + 0.2\times 0.8 + 0.6\times 0.4 = 0.58
p(i_{1}|x)= 0.2\times 0.1 + 0.2\times 0.2 + 0.6\times 0.6 = 0.42
\hat{y} = argmax[p(i_{0}|x),p(i_{1}|x)] = 0
$$

* 클래스 확률을 기반으로 가중 다수 투표를 구현하기 위해 **np.average** 및 **np.argmax**를 사용하여 **NumPy**를 다시 사용할 수 있음.

```python
>>> ex = np.array([[0.9, 0.1],
... [0.8, 0.2],
... [0.4, 0.6]])
>>> p = np.average(ex, axis=0, weights=[0.2, 0.2, 0.6])
>>> p
array([0.58, 0.42])
>>> np.argmax(p)
0
```

#### * Using the majority voting principle to make predictions

* 이전 섹션에서 구현한 **Majority Vote Classifier**를 실행함. 이전에 테스트할 수 있는 데이터 세트를 준비함. 
* **sickit-learn**의 데이터 세트 모듈에서 Iris 데이터 세트를 로드하고 분류 작업을 더 어렵게 만들기 위해 Iris-versicolor와 Iris-verginica 두 가지 특징만 선택함.
* 다수표 분류기가 다중 클래스 문제에 일반화되어 있지만 Iris-versicolor 및 Iris-virginica 클래스의 꽃 예만 분류할 것이며, 나중에 ROC AUC를 계산할 것임. 코드는 다음과 같음.

```python
>>> from sklearn import datasets
>>> from sklearn.model_selection import train_test_split
>>> from sklearn.preprocessing import StandardScaler
>>> from sklearn.preprocessing import LabelEncoder
>>> iris = datasets.load_iris()
>>> X, y = iris.data[50:, [1, 2]], iris.target[50:]
>>> le = LabelEncoder()
>>> y = le.fit_transform(y)
```

* 다음으로, Iris 예제를 50% 훈련 데이터와 50% 테스트 데이터로 나눔.

```python
>>> X_train, X_test, y_train, y_test =\
... train_test_split(X, y,
... test_size=0.5,
... random_state=1,
... stratify=y)
```

* 훈련 데이터 세트를 사용하여 이제 **세 가지 분류기**를 교육할 것이다.
    1. **로지스틱 회귀 분석 분류기**
    2. **의사 결정 트리 분류기**
    3. **k-최근접  분류기** 

* 그런 다음 앙상블 분류기로 결합하기 전, 훈련 데이터 세트에 대한 10겹 교차 검증을 통해 각 분류기의 모델 성능을 평가할 것임. 
* 코드는 아래와 같음.

```python
>>> from sklearn.model_selection import cross_val_score
>>> from sklearn.linear_model import LogisticRegression
>>> from sklearn.tree import DecisionTreeClassifier
>>> from sklearn.neighbors import KNeighborsClassifier
>>> from sklearn.pipeline import Pipeline
>>> import numpy as np
>>> clf1 = LogisticRegression(penalty='l2',
... C=0.001,
... solver='lbfgs',
... random_state=1)
>>> clf2 = DecisionTreeClassifier(max_depth=1,
... criterion='entropy',
... random_state=0)
>>> clf3 = KNeighborsClassifier(n_neighbors=1,
... p=2,
... metric='minkowski')
>>> pipe1 = Pipeline([['sc', StandardScaler()],
... ['clf', clf1]])
>>> pipe3 = Pipeline([['sc', StandardScaler()],
... ['clf', clf3]])
>>> clf_labels = ['Logistic regression', 'Decision tree', 'KNN']
>>> print('10-fold cross validation:\n')
>>> for clf, label in zip([pipe1, clf2, pipe3], clf_labels):
... scores = cross_val_score(estimator=clf,
... X=X_train,
... y=y_train,
... cv=10,
... scoring='roc_auc')
... print(f'ROC AUC: {scores.mean():.2f} '
... f'(+/- {scores.std():.2f}) [{label}]')
```

* 다음 코드에서 보는 바와 같이, 세 가지 분류기(개별 분류기)에 대한 예측 성능이 거의 같다는 것을 보여줌.
* k-최근접 분류기와 로지스틱 회귀는 결정트리와 달리 스케일에 민감하게 반응하기 때문에 파이프 라인으로 훈련하였음.(제3장에서 논의한 내용)

```python
10-fold cross validation:
ROC AUC: 0.92 (+/- 0.15) [Logistic regression]
ROC AUC: 0.87 (+/- 0.18) [Decision tree]
ROC AUC: 0.85 (+/- 0.13) [KNN]
```

* 이제 **MajorityVoteClassifier**에서 다수결 투표를 위한 개별 분류기 클래스를 결합(앙상블)

```python
>>> mv_clf = MajorityVoteClassifier(
... classifiers=[pipe1, clf2, pipe3]
... )
>>> clf_labels += ['Majority voting']
>>> all_clf = [pipe1, clf2, pipe3, mv_clf]
>>> for clf, label in zip(all_clf, clf_labels):
... scores = cross_val_score(estimator=clf,
... X=X_train,
... y=y_train,
... cv=10,
... scoring='roc_auc')
... print(f'ROC AUC: {scores.mean():.2f} '
... f'(+/- {scores.std():.2f}) [{label}]')
ROC AUC: 0.92 (+/- 0.15) [Logistic regression]
ROC AUC: 0.87 (+/- 0.18) [Decision tree]
ROC AUC: 0.85 (+/- 0.13) [KNN]
ROC AUC: 0.98 (+/- 0.05) [Majority voting]
```

* 위 코드의 실행 결과를 통해 **MajorityVoteClassifier**의 성능이 10겹 교차 검증 평가에서 개별 분류기에 비해 향상되었음.
* 마지막 줄의 **Majority voting** 이 앙상블의 결과임.

## 3. Evaluating and tuning the ensemble classifier

* ROC를 통해 **MajorityVoteClassifier** 의 일반화 성능을 테스트 세트를 이용하여 확인함. 
* <mark style='background-color: #fff5b1'>테스트 데이터 세트가 모델 선택에 사용되어서는 안됨. 테스트 데이터 세트는 단지 분류기 시스템의 일반화 성능에 편견 없는 추정치를 구하는 것임.</mark>

```python
>>> from sklearn.metrics import roc_curve
>>> from sklearn.metrics import auc
>>> colors = ['black', 'orange', 'blue', 'green']
>>> linestyles = [':', '--', '-.', '-']
>>> for clf, label, clr, ls \
... in zip(all_clf, clf_labels, colors, linestyles):
... # assuming the label of the positive class is 1
... y_pred = clf.fit(X_train,
... y_train).predict_proba(X_test)[:, 1]
... fpr, tpr, thresholds = roc_curve(y_true=y_test,
... y_score=y_pred)
... roc_auc = auc(x=fpr, y=tpr)
... plt.plot(fpr, tpr,
... color=clr,
... linestyle=ls,
... label=f'{label} (auc = {roc_auc:.2f})')
>>> plt.legend(loc='lower right')
>>> plt.plot([0, 1], [0, 1],
... linestyle='--',
... color='gray',
... linewidth=2)
>>> plt.xlim([-0.1, 1.1])
>>> plt.ylim([-0.1, 1.1])
>>> plt.grid(alpha=0.5)
>>> plt.xlabel('False positive rate (FPR)')
>>> plt.ylabel('True positive rate (TPR)')
>>> plt.show()
```

![4](https://user-images.githubusercontent.com/113302607/196036821-1234ca52-1c3c-4904-ad2f-d72fad360315.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.4. The ROC curve for the different classifiers
</figcaption>

* 위 결과를 통해 앙상블 분류기가 테스트 데이터 세트에서도 우수한 성능을 가지는 것을 확인할 수 있음(auc = 0.95). 
* 로지스틱 회귀 분석 분류기 또한 auc = 0.95으로 좋은 성능을 내는 것을 확인할 수 있음. 이는 데이터 셋이 작을 때 생기는 높은 분산 때문임(데이터 집합을 분할하는 방법의 민감도).

* 또한 두 개의 특성만이 사용되었기 때문에 결정경계 확인이 가능함.
* 결정트리 결정 경계를 다른 모델과 스케일을 맞추기 위해 다음과 같은 코드가 작성되었음. 

```python
>>> sc = StandardScaler()
>>> X_train_std = sc.fit_transform(X_train)
>>> from itertools import product
>>> x_min = X_train_std[:, 0].min() - 1
>>> x_max = X_train_std[:, 0].max() + 1
>>> y_min = X_train_std[:, 1].min() - 1
>>>
>>> y_max = X_train_std[:, 1].max() + 1
>>> xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
... np.arange(y_min, y_max, 0.1))
>>> f, axarr = plt.subplots(nrows=2, ncols=2,
... sharex='col',
... sharey='row',
... figsize=(7, 5))
>>> for idx, clf, tt in zip(product([0, 1], [0, 1]),
... all_clf, clf_labels):
... clf.fit(X_train_std, y_train)
... Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
... Z = Z.reshape(xx.shape)
... axarr[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.3)
... axarr[idx[0], idx[1]].scatter(X_train_std[y_train==0, 0],
... X_train_std[y_train==0, 1],
... c='blue',
... marker='^',
... s=50)
... axarr[idx[0], idx[1]].scatter(X_train_std[y_train==1, 0],
... X_train_std[y_train==1, 1],
... c='green',
... marker='o',
... s=50)
... axarr[idx[0], idx[1]].set_title(tt)
>>> plt.text(-3.5, -5.,
... s='Sepal width [standardized]',
... ha='center', va='center', fontsize=12)
>>> plt.text(-12.5, 4.5,
... s='Petal length [standardized]',
... ha='center', va='center',
... fontsize=12, rotation=90)
>>> plt.show()
```

![5](https://user-images.githubusercontent.com/113302607/196036824-7d206fcd-3140-493d-b732-a6e1916968da.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.5. The decision boundaries for the different classifiers
</figcaption>

* 앙상블 튜닝을 위해 개별 분류기의 매개 변수를 조정하기 전, **GridSearchCV** 객체 내부의 개별 매개 변수에 액세스하는 방법을 찾기 위해 **get_params** 메서드를 호출

```python
>>> mv_clf.get_params()
{'decisiontreeclassifier':
 DecisionTreeClassifier(class_weight=None, criterion='entropy',
 max_depth=1, max_features=None,
 max_leaf_nodes=None, min_samples_leaf=1,
 min_samples_split=2,
 min_weight_fraction_leaf=0.0,
 random_state=0, splitter='best'),
 'decisiontreeclassifier__class_weight': None,
 'decisiontreeclassifier__criterion': 'entropy',
 [...]
 'decisiontreeclassifier__random_state': 0,
 'decisiontreeclassifier__splitter': 'best',
 'pipeline-1':
 Pipeline(steps=[('sc', StandardScaler(copy=True, with_mean=True,
 with_std=True)),
 ('clf', LogisticRegression(C=0.001,
Chapter 7 221
 class_weight=None,
 dual=False,
 fit_intercept=True,
 intercept_scaling=1,
 max_iter=100,
 multi_class='ovr',
 penalty='l2',
 random_state=0,
 solver='liblinear',
 tol=0.0001,
 verbose=0))]),
 'pipeline-1__clf':
 LogisticRegression(C=0.001, class_weight=None, dual=False,
 fit_intercept=True, intercept_scaling=1,
 max_iter=100, multi_class='ovr',
 penalty='l2', random_state=0,
 solver='liblinear', tol=0.0001, verbose=0),
 'pipeline-1__clf__C': 0.001,
 'pipeline-1__clf__class_weight': None,
 'pipeline-1__clf__dual': False,
 [...]
 'pipeline-1__sc__with_std': True,
 'pipeline-2':
 Pipeline(steps=[('sc', StandardScaler(copy=True, with_mean=True,
 with_std=True)),
 ('clf', KNeighborsClassifier(algorithm='auto',
 leaf_size=30,
 metric='minkowski',
 metric_params=None,
 n_neighbors=1,
 p=2,
 weights='uniform'))]),
 'pipeline-2__clf':
 KNeighborsClassifier(algorithm='auto', leaf_size=30,
 metric='minkowski', metric_params=None,
 n_neighbors=1, p=2, weights='uniform'),
 'pipeline-2__clf__algorithm': 'auto',
 [...]
 'pipeline-2__sc__with_std': True}
```

* 위의 긴 코드를 통해 **get_params** 메서드에서 반환된 값을 기반으로 이제 개별 분류자의 속성에 액세스하는 방법을 알게 되었음. 예시를 위해 그리드 서치를 통해 로지스틱 회귀 분류기의 역 정규화 매개 변수 C와 결정 트리 깊이를 조정함.

```python
>>> from sklearn.model_selection import GridSearchCV
>>> params = {'decisiontreeclassifier__max_depth': [1, 2],
... 'pipeline-1__clf__C': [0.001, 0.1, 100.0]}
>>> grid = GridSearchCV(estimator=mv_clf,
... param_grid=params,
... cv=10,
... scoring='roc_auc')
>>> grid.fit(X_train, y_train)
```

* 그리드 서치 완료 후, 다음과 같이 10배 교차 검증을 통해 계산된 다른 초 매개 변수 값 조합과 평균 ROC AUC 점수를 뽑을 수 있음.

```python
>>> for r, _ in enumerate(grid.cv_results_['mean_test_score']):
... mean_score = grid.cv_results_['mean_test_score'][r]
... std_dev = grid.cv_results_['std_test_score'][r]
... params = grid.cv_results_['params'][r]
... print(f'{mean_score:.3f} +/- {std_dev:.2f} {params}')
0.983 +/- 0.05 {'decisiontreeclassifier__max_depth': 1,
 'pipeline-1__clf__C': 0.001}
0.983 +/- 0.05 {'decisiontreeclassifier__max_depth': 1,
 'pipeline-1__clf__C': 0.1}
0.967 +/- 0.10 {'decisiontreeclassifier__max_depth': 1,
 'pipeline-1__clf__C': 100.0}
0.983 +/- 0.05 {'decisiontreeclassifier__max_depth': 2,
 'pipeline-1__clf__C': 0.001}
0.983 +/- 0.05 {'decisiontreeclassifier__max_depth': 2,
 'pipeline-1__clf__C': 0.1}
0.967 +/- 0.10 {'decisiontreeclassifier__max_depth': 2,
 'pipeline-1__clf__C': 100.0}
>>> print(f'Best parameters: {grid.best_params_}')
Best parameters: {'decisiontreeclassifier__max_depth': 1,
 'pipeline-1__clf__C': 0.001}
>>> print(f'ROC AUC : {grid.best_score_:.2f}')
ROC AUC: 0.98
```

* 위 결과를 통해, 정규화 강도가 가장 낮음(C=0.001)를 선택할 때 최상의 교차 검증 결과를 얻는 반면, 트리 깊이는 성능에 전혀 영향을 미치지 않는 것으로 보임. 

## 4. Bagging – building an ensemble of classifiers from bootstrap samples
Bagging은 이전 섹션에서 구현한 Majority Vote Classifier와 밀접한 관련이 있는 앙상블 학습 기법이다. 그러나 앙상블의 개별 분류기에 맞추기 위해 동일한 훈련 데이터 세트를 사용하는 대신 초기 훈련 데이터 세트에서 부트스트랩 샘플(교체가 있는 무작위 샘플)을 추출하기 때문에 백깅을 부트스트랩 집계라고도 한다.

* **Bagging**은 이전 섹션에서 구현한 **Majority Vote Classifier**와 관련이 있는 앙상블 학습 기법임.
* 그러나 앙상블의 개별 분류기에 맞추기 위해 동일한 훈련 데이터 세트를 사용하는 대신 초기 훈련 데이터 세트에서 부트스트랩 샘플(교체가 있는 랜덤 샘플)을 추출하므로 bagging을 **bootstrap aggregating**라고불림.

![6](https://user-images.githubusercontent.com/113302607/196036836-675e1a6c-94b9-4a80-badf-ef225f5ca8f3.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.6. The concept of bagging
</figcaption>

* 컨셉은 위 그림과 같음. 

#### * Bagging in a nutshell
백깅 분류기의 부트스트랩 집계가 작동하는 방식에 대한 보다 구체적인 예를 제공하기 위해 그림 7.7에 표시된 예를 고려해보자. 여기서, 우리는 각 포장 라운드에서 교체와 함께 무작위로 샘플링되는 7개의 다른 훈련 인스턴스(지수 1-7로 표시됨)를 가지고 있다. 그런 다음 각 부트스트랩 샘플을 사용하여 분류기 Cj를 맞추는데, 이는 가장 일반적으로 가지치기되지 않은 결정 트리이다.
그림 7.7에서 볼 수 있듯이, 각 분류기는 훈련 데이터 세트에서 예제의 무작위 하위 집합을 수신한다. 우리는 배깅을 통해 얻은 이러한 무작위 샘플을 Bagging 라운드 1, Bagging 라운드 2 등으로 나타냅니다. 각 하위 집합에는 중복의 특정 부분이 포함되어 있으며, 치환을 사용한 샘플링으로 인해 원래 예 중 일부가 리샘플링된 데이터 세트에 전혀 나타나지 않는다. 개별 분류기가 부트스트랩 샘플에 적합하면 다수결 투표를 사용하여 예측이 결합된다.

![7](https://user-images.githubusercontent.com/113302607/196036839-91cb64ab-4f1d-45c3-b529-0d566f85f6d6.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.7. An example of bagging
</figcaption>

* 위 그림은 bagging 분류기의 bootstrap aggregating이 작동하는 방식에 대한 보다 구체적인 예임.
* 여기서, 각 bagging 라운드에서 교체와 함께 랜덤으로 샘플링되는 7개의 다른 훈련 인스턴스(지수 1-7로 표시됨)를 가지고 있음.
* 각 분류기는 훈련 데이터 세트에서 예제의 랜덤 하위 집합을 수신함.
* bagging 통해 얻은 이러한 무작위 샘플을 Bagging round 1, Bagging round 2 등으로 나타냄.
* 각 하위 집합에는 중복의 특정 부분이 포함되어 있으며, 치환을 사용한 샘플링으로 인해 원래 예 중 일부가 리샘플링된 데이터 세트에 전혀 나타나지 않음.
* 개별 분류기가 bootstrap 샘플에 적합하면 다수결 투표를 사용하여 예측이 결합됨. 
* bagging은 3장에서 소개된 랜덤 포레스트 분류기와도 관련이 있음. 


#### * Applying bagging to classify examples in the Wine dataset

* bagging의 작동을 보기 위해, 4장의 wine data set을 사용하여 복잡한 분류 문제를 만듦. 

```python
>>> import pandas as pd
>>> df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/'
... 'machine-learning-databases/'
... 'wine/wine.data',
... header=None)
>>> df_wine.columns = ['Class label', 'Alcohol',
... 'Malic acid', 'Ash',
... 'Alcalinity of ash',
... 'Magnesium', 'Total phenols',
... 'Flavanoids', 'Nonflavanoid phenols',
... 'Proanthocyanins',
... 'Color intensity', 'Hue',
... 'OD280/OD315 of diluted wines',
... 'Proline']
>>> # drop 1 class
>>> df_wine = df_wine[df_wine['Class label'] != 1]
>>> y = df_wine['Class label'].values
>>> X = df_wine[['Alcohol',
... 'OD280/OD315 of diluted wines']].values
>>> from sklearn.preprocessing import LabelEncoder
>>> from sklearn.model_selection import train_test_split
>>> le = LabelEncoder()
>>> y = le.fit_transform(y)
>>> X_train, X_test, y_train, y_test =\
... train_test_split(X, y,
... test_size=0.2,
... random_state=1,
... stratify=y)
```

* 구현을 위해 와인 데이터 세트를 이용, 데이터 세트를 80% 훈련 및 20% 테스트 데이터 세트로 분할
* **BaggingClassifier** 알고리듬은 이미 skickit-learn에 구현되어 있으며, 앙상블 하위 모듈에서 가져올 수 있음.
* 가지치기되지 않은 의사 결정 트리를 기본 분류기로 사용, 훈련 데이터 세트의 다양한 부트스트랩 샘플에 맞는 500개의 의사 결정 트리의 앙상블을 만듦.

```python
>>> from sklearn.ensemble import BaggingClassifier
>>> tree = DecisionTreeClassifier(criterion='entropy',
... random_state=1,
... max_depth=None)
>>> bag = BaggingClassifier(base_estimator=tree,
... n_estimators=500,
... max_samples=1.0,
... max_features=1.0,
... bootstrap=True,
... bootstrap_features=False,
... n_jobs=1,
... random_state=1)
```

* 다음으로, 훈련 및 테스트 데이터 세트에 대한 예측의 정확도 점수를 계산하여 bagging 분류기의 성능을 단일 가지 결정의 성능과 비교함.

```python
>>> from sklearn.metrics import accuracy_score
>>> tree = tree.fit(X_train, y_train)
>>> y_train_pred = tree.predict(X_train)
>>> y_test_pred = tree.predict(X_test)
>>> tree_train = accuracy_score(y_train, y_train_pred)
>>> tree_test = accuracy_score(y_test, y_test_pred)
>>> print(f'Decision tree train/test accuracies '
... f'{tree_train:.3f}/{tree_test:.3f}')
Decision tree train/test accuracies 1.000/0.833
```

* 위 코드 결과값을 통해 가지치기되지 않은 의사 결정 트리는 훈련 예제의 모든 클래스 레이블을 정확하게 예측하지만, 테스트 정확도가 상당히 낮다는 것은 모델의 분산(과대적합)이 높다는 것을 나타냄.

```python
>>> from sklearn.metrics import accuracy_score
>>> tree = tree.fit(X_train, y_train)
>>> y_train_pred = tree.predict(X_train)
>>> y_test_pred = tree.predict(X_test)
>>> tree_train = accuracy_score(y_train, y_train_pred)
>>> tree_test = accuracy_score(y_test, y_test_pred)
>>> print(f'Decision tree train/test accuracies '
... f'{tree_train:.3f}/{tree_test:.3f}')
Decision tree train/test accuracies 1.000/0.833
```


```python
>>> bag = bag.fit(X_train, y_train)
>>> y_train_pred = bag.predict(X_train)
>>> y_test_pred = bag.predict(X_test)
>>> bag_train = accuracy_score(y_train, y_train_pred)
>>> bag_test = accuracy_score(y_test, y_test_pred)
>>> print(f'Bagging train/test accuracies '
... f'{bag_train:.3f}/{bag_test:.3f}')
Bagging train/test accuracies 1.000/0.917
```

* 의사 결정 트리와 bagging 분류기의 훈련 정확도는 훈련 데이터 세트(둘 다 100%)에서 유사하지만, bagging 분류기가 테스트 데이터 세트에서 추정된 대로 조금 더 나은 일반화 성능을 가지고 있음을 알 수 있음. 다음으로 의사 결정 트리와 배깅 분류기 사이의 의사 결정 영역을 비교

```python
>>> x_min = X_train[:, 0].min() - 1
>>> x_max = X_train[:, 0].max() + 1
>>> y_min = X_train[:, 1].min() - 1
>>> y_max = X_train[:, 1].max() + 1
>>> xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
... np.arange(y_min, y_max, 0.1))
>>> f, axarr = plt.subplots(nrows=1, ncols=2,
... sharex='col',
... sharey='row',
... figsize=(8, 3))
>>> for idx, clf, tt in zip([0, 1],
... [tree, bag],
... ['Decision tree', 'Bagging']):
... clf.fit(X_train, y_train)
...
... Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
... Z = Z.reshape(xx.shape)
... axarr[idx].contourf(xx, yy, Z, alpha=0.3)
... axarr[idx].scatter(X_train[y_train==0, 0],
... X_train[y_train==0, 1],
... c='blue', marker='^')
... axarr[idx].scatter(X_train[y_train==1, 0],
... X_train[y_train==1, 1],
... c='green', marker='o')
... axarr[idx].set_title(tt)
>>> axarr[0].set_ylabel('Alcohol', fontsize=12)
>>> plt.tight_layout()
>>> plt.text(0, -0.2,
... s='OD280/OD315 of diluted wines',
... ha='center',
... va='center',
... fontsize=12,
... transform=axarr[1].transAxes)
>>> plt.show()
```

![8](https://user-images.githubusercontent.com/113302607/196036843-9cb05646-8daa-4b82-8489-1060ceeca80e.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.8. : The piece-wise linear decision boundary of a decision tree versus bagging
</figcaption>

* 위 실행결과를 통해 결과 3노드 심층 결정 트리의 조각별 선형 결정 경계는 Bagging 앙상블에서 더 부드러워 보이는 결과를 확인할 수 있음.
* Bagging 알고리듬이 모델의 분산을 줄이는 효과적인 접근법이 될 수 있다는 것에 주목해야 함.
* 그러나, Bagging은 모델 편향, 즉 데이터의 추세를 잘 포착하기에는 너무 간단한 모델을 줄이는 데 효과적이지 않음.
* 이것이 우리가 낮은 편향을 가진 분류기 앙상블, 예를 들어 가지 없는 결정 트리에서 Bagging 수행하고자 하는 이유임.

## 6. Leveraging weak learners via adaptive boosting
* 앙상블 방법 마지막 섹션에서는 가장 일반적인 구현에 특히 중점을 두고 부스팅에 대해 논의할 것: **적응 부스팅(AdaBoost)**
* 부스팅에서 앙상블은 매우 간단한 기본 분류기로 구성, 부스팅의 핵심 개념은 분류하기 어려운 훈련 예에 초점을 맞추는 것임. 

#### * How adaptive boosting works
* Bagging과 달리, 부스팅 알고리듬의 초기 공식은 대체 없이 훈련 데이터 세트에서 도출된 훈련 예제의 무작위 하위 집합을 사용

![9](https://user-images.githubusercontent.com/113302607/196036848-eebf1de9-1b89-4073-8ea5-7fef930b2ae5.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.9. The concept of AdaBoost to improve weak learners
</figcaption>

* 위 그림은 AdaBoost의 기본 개념을 나타냄.

#### * Applying AdaBoost using scikit-learn
* 이제 사이킷런을 통해 AdaBoost 앙상블 분류기를 훈련
* 와인 서브셋을 사용하여 배깅 메타 분류기를 훈련함.

* base_estimator 속성을 통해 500개의 의사 결정 트리 스택에 대해 AdaBoostClassifier를 훈련함. 

```python
>>> from sklearn.ensemble import AdaBoostClassifier
>>> tree = DecisionTreeClassifier(criterion='entropy',
... random_state=1,
... max_depth=1)
>>> ada = AdaBoostClassifier(base_estimator=tree,
... n_estimators=500,
... learning_rate=0.1,
... random_state=1)
>>> tree = tree.fit(X_train, y_train)
>>> y_train_pred = tree.predict(X_train)
>>> y_test_pred = tree.predict(X_test)
>>> tree_train = accuracy_score(y_train, y_train_pred)
>>> tree_test = accuracy_score(y_test, y_test_pred)
>>> print(f'Decision tree train/test accuracies '
... f'{tree_train:.3f}/{tree_test:.3f}')
Decision tree train/test accuracies 0.916/0.875
```

* 결과를 통해, 의사결정 트리 스텀프는 이전 섹션에서 보았던 가지치기되지 않은 의사결정 트리와 달리 훈련 데이터에 적합하지 않은 것으로 보임.

```python
>>> ada = ada.fit(X_train, y_train)
>>> y_train_pred = ada.predict(X_train)
>>> y_test_pred = ada.predict(X_test)
>>> ada_train = accuracy_score(y_train, y_train_pred)
>>> ada_test = accuracy_score(y_test, y_test_pred)
>>> print(f'AdaBoost train/test accuracies '
... f'{ada_train:.3f}/{ada_test:.3f}')
AdaBoost train/test accuracies 1.000/0.917
```

* 여기서, AdaBoost 모델이 훈련 데이터 세트의 모든 클래스 레이블을 정확하게 예측하고 의사 결정 트리 스텀프에 비해 약간 개선된 테스트 데이터 세트 성능을 보여줌.
* 그러나 모델 편향을 줄이기 위한 시도로 추가적인 분산이 도입되었다는 것을 알 수 있음. (즉, 훈련과 테스트 성능 간의 차이가 더 큼.)
* 마지막으로 결정영역을 확인함. 

```python
>>> x_min = X_train[:, 0].min() - 1
>>> x_max = X_train[:, 0].max() + 1
>>> y_min = X_train[:, 1].min() - 1
>>> y_max = X_train[:, 1].max() + 1
>>> xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
... np.arange(y_min, y_max, 0.1))
>>> f, axarr = plt.subplots(1, 2,
... sharex='col',
... sharey='row',
... figsize=(8, 3))
>>> for idx, clf, tt in zip([0, 1],
... [tree, ada],
... ['Decision tree', 'AdaBoost']):
... clf.fit(X_train, y_train)
... Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
... Z = Z.reshape(xx.shape)
... axarr[idx].contourf(xx, yy, Z, alpha=0.3)
... axarr[idx].scatter(X_train[y_train==0, 0],
... X_train[y_train==0, 1],
... c='blue',
... marker='^')
... axarr[idx].scatter(X_train[y_train==1, 0],
... X_train[y_train==1, 1],
... c='green',
... marker='o')
... axarr[idx].set_title(tt)
... axarr[0].set_ylabel('Alcohol', fontsize=12)
>>> plt.tight_layout()
>>> plt.text(0, -0.2,
... s='OD280/OD315 of diluted wines',
... ha='center',
... va='center',
... fontsize=12,
... transform=axarr[1].transAxes)
>>> plt.show()
```

![11](https://user-images.githubusercontent.com/113302607/196036853-67579602-1365-4b3f-9873-2d77efaea545.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.11. The decision boundaries of the decision tree versus AdaBoost
</figcaption>

* 결정 영역을 통해서 AdaBoost 모델의 결정 경계가 결정 스텀프의 결정 경계보다 훨씬 복잡하다는 것을 알 수 있음.
* 또한 AdaBoost 모델은 이전 섹션에서 학습한 Bagging 분류기와 매우 유사하게  공간을 구분
* 앙상블 학습은 개별 분류기에 비해 계산 복잡성을 증가시킨다는 점에 주목할 필요가 있음.
* 실제로, 우리는 예측 성능의 종종 비교적 완만한 개선을 위해 증가한 계산 비용의 대가를 지불하기를 원하는지에 대해 신중하게 생각할 필요가 있음.
