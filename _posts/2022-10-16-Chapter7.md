---
layout: single
title:  "Chapter7. Combining Different Models for Ensemble Learning"
use_math: true
---

## 1. Learning with ensembles

* <mark style='background-color: #fff5b1'>앙상블 방법(ensemble methods)</mark> 의 목표는 서로 다른 분류기를 일반화 성능이 우수한 메타 분류기로 결합하는 것임.
* 예를 들어, 10명의 전문가로부터 예측을 수집했을 때, 앙상블 방법을 사용하면 10명의 전문가의 예측을 결합하여 각 개별 전문가의 예측보다 더 정확하고 강력한 예측을 도출할 수 있음.
* 앙상블에는 몇 가지 다른 접근법이 존재하며, 이 절에서는 앙상블의 작동원리와 좋은 일반화 성능을 내는 이유에 대해 알아봄.

![1](https://user-images.githubusercontent.com/113302607/196036804-692b08c3-42ca-4355-ae6b-753ef2eee333.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.1. The different voting concepts
</figcaption>

* 가장 인기 있는 앙상블 방법은 **다수결 투표(majority voting)** 방식임. 
* 위 그림은 10개의 분류기로 구성된 앙상블에 대한 다수결(majority) 및 **다수결 투표(plurality voting)** 의 개념을 보여줌. 여기서 각 고유한 기호(삼각형, 사각형 및 원)는 고유한 클래스 레이블을 나타냄.
* 다수결 이름 그대로 50% 이상의 표를 받은 등급표를 선정하는 방식을 의미함. 

![2](https://user-images.githubusercontent.com/113302607/196036808-cffb8eab-bacd-4751-a7d6-f2f5813b6c06.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.2. A general ensemble approach
</figcaption>

* 훈련 데이터 세트를 사용하여 m개의 다른 분류기$(C1, ..., Cm)$를 교육하는 것으로 시작하며 앙상블 방법에 따라, 결정 트리, 지원 벡터 머신, 로지스틱 회귀 분류기 등과 같은
다양한 분류 알고리즘으로 구성될 수 있음. (또는 훈련 데이터 세트의 다른 하위 집합에 맞는 동일한 기본 분류 알고리듬을 사용할 수 있음.)
* 유명한 예는 서로 다른 의사 결정 트리 분류기를 결합한 랜덤 포레스트(random forest) 알고리듬이 있음.
* 위 그림은 다수결 투표를 이용한 일반적인 앙상블 접근법의 개념을 보여줌.
* 과반수 투표나 다수결 투표를 통해 클래스 레이블을 예측하기 위해 개별 분류기의 $C_{j}$의 예측 클래스 레이블을 결합하여 가장 많은 표를 얻은 클래스 레이블 $\hat{y}$을 선택할 수 있음.

$\hat{y} = mode\{C_{1}(x),C_{2}(x),\cdots,C_{m}(x)\}$

* (위 식에서) 통계학에서 mode는 집합에서 가장 빈번한 event 또는 result임.
* 예를 들어, $class1 = –1$  및 $class2 = +1$인 이진 분류 작업에서 다수결 예측은 다음과 같음.

$ C(x)=sign \[ \sum_{j}^{m} C_{j}(x)\] = \left\{\begin{matrix}
1 & if \sum_{j} C_{j}(x) \geq 0\\ 
-1 & otherwise
\end{matrix}\right. $ 

* 앙상블 방법이 개별 분류기보다 더 잘 작동할 수 있는 이유를 설명하기 위해 조합론의 몇 가지 개념을 적용함.
* 예를 들어, 이진 분류 작업에 대한 모든 n개의 분류기가 동일한 에러율인 $\varepsilon$를 갖는 분류기가 있으며 분류기가 서로에게 영향을 주지 않는 독립적 관계라고 가정함.
* 위와 같은 가정하에, 단순히 기저 분류기의 앙상블의 오차 확률을 이항 분포의 확률 질량 함수로 표현할 수 있음.

$P(y\geq k)=\sum_{k}^{n} \left \langle nk\right \rangle \varepsilon^{k}(1-0.25)^{n-k}=\varepsilon^{ensemble}$

* 위 식에서, 이항 계수 n개 원소에서 k개를 뽑는 조합임. 이 식은 앙상블이 틀릴 확률을 계산함. 에러율이 0.25인 분류기 11개로 구성된 앙상블 에러율은 0.034의 값을 가짐.
* 보시다시피 모든 가정이 충족될 경우 앙상블의 오차율(0.034)이 각 개별 분류기의 오차율(0.25)보다 훨씬 낮음. 
* 만약 에러율이 0.5인 분류기가 짝수 개의 분류기 n으로 분할하면 오류로 처리됨.
* 일단 이상주의적 앙상블 분류기를 다양한 기본 에러율 범위에 걸쳐 기본 분류기와 비교하기 위해, 파이썬에서 확률 질량 함수를 아래와 같이 구현함. 

```python
>>> from scipy.special import comb
>>> import math
>>> def ensemble_error(n_classifier, error):
... k_start = int(math.ceil(n_classifier / 2.))
... probs = [comb(n_classifier, k) *
... error**k *
... (1-error)**(n_classifier - k)
... for k in range(k_start, n_classifier + 1)]
... return sum(probs)
>>> ensemble_error(n_classifier=11, error=0.25)
0.03432750701904297
```

* angemble_error 함수를 구현한 후, 0.0에서 1.0까지의 다양한 기본 오류 범위에 대한 앙상블 에러율을 계산하여, 앙상블과 기본 오류 사이의 관계를 시각화할 수 있음. 

```python
>>> import numpy as np
>>> import matplotlib.pyplot as plt
>>> error_range = np.arange(0.0, 1.01, 0.01)
>>> ens_errors = [ensemble_error(n_classifier=11, error=error)
... for error in error_range]
>>> plt.plot(error_range, ens_errors,
... label='Ensemble error',
... linewidth=2)
>>> plt.plot(error_range, error_range,
... linestyle='--', label='Base error',
... linewidth=2)
>>> plt.xlabel('Base error')
>>> plt.ylabel('Base/Ensemble error')
>>> plt.legend(loc='upper left')
>>> plt.grid(alpha=0.5)
>>> plt.show()
```

![3](https://user-images.githubusercontent.com/113302607/196036814-f249c137-496a-48a7-befc-25d8277f2cda.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.3. A plot of the ensemble error versus the base error
</figcaption>

* 위 그림의 y축은 기본 오차(점선)과 앙상블 오차(실선)을 나타내며, 이를 통해 기본 분류기가 랜덤 추측보다 더 나은 성능을 발휘하는 한 앙상블의 오차 확률은 항상 개별 분류기의 오차보다 높음.($\varepsilon < 0.5$)

## 2. Combining classifiers via majority vote
* 파이썬에서 다수결 투표를 위한 간단한 앙상블 분류기를 구현

#### * Implementing a simple majority vote classifier

* 특정 데이터 세트에서 개별 분류기의 약점을 균형 있게 조정하는 더 강력한 메타 분류기를 구축하는데 목표
* 보다 정확한 수학 용어로, 가중 다수표를 다음과 같이 쓸 수 있음.

$\hat{y} = argmax\sum_{j=1}^{m}w_{j}\chi _{A}(C_{j}(x)=i)$

* 위 식에서, w는 c라는 분류기 연관된 가중치이며, $\hat{y}$은 앙상블에서 예측한 클래스 레이블, $\chi$는 특성함수를 의미함.
* 가중치 개념을 더 잘 이해하기 위해, 좀 더 구체적인 예를 살펴봄. 세 가지 기본 분류기 $C_{j}(j\in {1,2,3})$의 앙상블이 있고 주어진 예제의 클래스 레이블 $C_{j}(x)\in \{0,1 \}$을 예측하고 싶다고 가정
* 세 가지 중 두 가지 기본 분류기는 클래스 레이블을 0으로 예측하고, 한 가지 $C_{3}$는 해당 예가 1에 속한다고 예측, 각 기본 분류기의 예측에 균등하게 가중치를 부여하면 다수결은 예제가 클래스 0에 속한다고 예측함. 

$C_{1}\rightarrow 0, C_{2}\rightarrow 0, C_{3}\rightarrow 1$

$\hat{y} = mode \{0,0,1\} = 0$

* 이제, $C_{3}$에 0.6의 가중치를 할당하고, $C_{1}$과 $C_{2}$에 0.2의 계수를 부여하면, 1의 결과값을 얻음.
* 보다 간단하게, 3×0.2 = 0.6이므로, $C_{3}$에 의한 예측은 $C_{1}$과 $C_{2}$에 의한 예측보다 3배 더 많은 가중치를 가지고 있다고 말할 수 있으며, 이를 다음과 같이 쓸 수 있음.

$\hat{y} = mode \{0,0,1,1,1\} = 1$

* 가중 다수결의 개념을 파이썬 코드로 변환하기 위해, 빈카운트가 각 클래스 라벨의 발생 횟수를 카운트하는 NumPy의 편리한 argmax 및 bincount 함수를 사용할 수 있음.
* 그런 다음 argmax 함수는 다수 클래스 레이블에 해당하는 가장 높은 카운트의 인덱스 위치를 반환(이것은 클래스 레이블이 0에서 시작한다고 가정함).

```python
to the majority class label (this assumes that class labels start at 0):
>>> import numpy as np
>>> np.argmax(np.bincount([0, 0, 1],
... weights=[0.2, 0.2, 0.6]))
1
```

* 확률로부터 클래스 레이블을 예측하기 위한 다수결의 수정된 버전은 다음과 같이 작성됨.

$\hat{y} = argmax\sum_{j=1}^{m} w_{j}p_{ij}$

* 위 식에서 $p_{ij}$는 클래스 레이블 $i$에 대한 $j$번째 분류기의 확률임. 
* 이전 예제를 계속하기 위해 클래스 레이블 $C_{j}(j\in {1,2,3})$ 및 세 가지 분류기로 구성된 앙상블$C_{j}(x)\in \{0,1 \}$에 이진 분류 문제가 있다고 가정
* 분류자 $C_{j}$가 특정 예제 $x$에 대해 다음과 같은 클래스 멤버쉽 확률을 반환한다고 가정

$C_{1}(x)\rightarrow [0.9,0.1], C_{2}(x)\rightarrow [0.8,0.2], C_{3}(x)\rightarrow [0.4,0.6]$

* 이전과 동일한 가중치(0.2, 0.2, 0.6)를 사용하여 다음과 같이 개별 클래스 확률을 계산할 수 있음. 

$$
p(i_{0}|x)= 0.2\times 0.9 + 0.2\times 0.8 + 0.6\times 0.4 = 0.58
p(i_{1}|x)= 0.2\times 0.1 + 0.2\times 0.2 + 0.6\times 0.6 = 0.42
\hat{y} = argmax[p(i_{0}|x),p(i_{1}|x)] = 0
$$

* 클래스 확률을 기반으로 가중 다수 투표를 구현하기 위해 np.average 및 np.argmax를 사용하여 NumPy를 다시 사용할 수 있음.

```python
>>> ex = np.array([[0.9, 0.1],
... [0.8, 0.2],
... [0.4, 0.6]])
>>> p = np.average(ex, axis=0, weights=[0.2, 0.2, 0.6])
>>> p
array([0.58, 0.42])
>>> np.argmax(p)
0
```

#### * Using the majority voting principle to make predictions
* 이전 섹션에서 구현한 Majority Vote Classifier를 실행함. 이전에 테스트할 수 있는 데이터 세트를 준비함. 
* sickit-learn의 데이터 세트 모듈에서 Iris 데이터 세트를 로드하고 분류 작업을 더 어렵게 만들기 위해 세팔 폭과 꽃잎 길이 두 가지 특징만 선택함.
* 다수표 분류기가 다중 클래스 문제에 일반화되어 있지만 Iris-versicolor 및 Iris-virginica 클래스의 꽃 예만 분류할 것이며, 나중에 ROC AUC를 계산할 것임. 코드는 다음과 같음.

```python
>>> from sklearn import datasets
>>> from sklearn.model_selection import train_test_split
>>> from sklearn.preprocessing import StandardScaler
>>> from sklearn.preprocessing import LabelEncoder
>>> iris = datasets.load_iris()
>>> X, y = iris.data[50:, [1, 2]], iris.target[50:]
>>> le = LabelEncoder()
>>> y = le.fit_transform(y)
```

* 다음으로, Iris 예제를 50% 훈련 데이터와 50% 테스트 데이터로 나눔.

```python
>>> X_train, X_test, y_train, y_test =\
... train_test_split(X, y,
... test_size=0.5,
... random_state=1,
... stratify=y)
```

* 훈련 데이터 세트를 사용하여 이제 세 가지 분류자를 교육할 것이다.
    1. 로지스틱 회귀 분석 분류기
    2. 의사 결정 트리 분류기
    3. k-nearest 인접 분류기 

* 그런 다음 앙상블 분류기로 결합하기 전에 교육 데이터 세트에 대한 10배 교차 검증을 통해 각 분류기의 모델 성능을 평가할 것임. 

```python
>>> from sklearn.model_selection import cross_val_score
>>> from sklearn.linear_model import LogisticRegression
>>> from sklearn.tree import DecisionTreeClassifier
>>> from sklearn.neighbors import KNeighborsClassifier
>>> from sklearn.pipeline import Pipeline
>>> import numpy as np
>>> clf1 = LogisticRegression(penalty='l2',
... C=0.001,
... solver='lbfgs',
... random_state=1)
>>> clf2 = DecisionTreeClassifier(max_depth=1,
... criterion='entropy',
... random_state=0)
>>> clf3 = KNeighborsClassifier(n_neighbors=1,
... p=2,
... metric='minkowski')
>>> pipe1 = Pipeline([['sc', StandardScaler()],
... ['clf', clf1]])
>>> pipe3 = Pipeline([['sc', StandardScaler()],
... ['clf', clf3]])
>>> clf_labels = ['Logistic regression', 'Decision tree', 'KNN']
>>> print('10-fold cross validation:\n')
>>> for clf, label in zip([pipe1, clf2, pipe3], clf_labels):
... scores = cross_val_score(estimator=clf,
... X=X_train,
... y=y_train,
... cv=10,
... scoring='roc_auc')
... print(f'ROC AUC: {scores.mean():.2f} '
... f'(+/- {scores.std():.2f}) [{label}]')
```

* 다음 코드에서 보는 바와 같이, 우리가 받은 출력은 개별 분류기의 예측 성능이 거의 같다는 것을 보여줌.

```python
10-fold cross validation:
ROC AUC: 0.92 (+/- 0.15) [Logistic regression]
ROC AUC: 0.87 (+/- 0.18) [Decision tree]
ROC AUC: 0.85 (+/- 0.13) [KNN]
```

* 파이프라인의 일부로 로지스틱 회귀 분석 및 k-근접 이웃 분류기를 훈련하는 이유는 제3장에서 논의한 바와 같이 로지스틱 회귀 알고리즘과 k-근접 이웃 알고리즘(유클리드 거리 메트릭 사용)이 모두 의사결정 트리와 달리 스케일 불변성이 아니기 때문임.
* Iris 특성은 모두 동일한 척도(cm)로 측정되지만 표준화된 기능으로 작업하는 것은 좋은 습관임.
* 이제 MajorityVoteClassifier에서 다수결 투표를 위한 개별 분류기를 결합

```python
>>> mv_clf = MajorityVoteClassifier(
... classifiers=[pipe1, clf2, pipe3]
... )
>>> clf_labels += ['Majority voting']
>>> all_clf = [pipe1, clf2, pipe3, mv_clf]
>>> for clf, label in zip(all_clf, clf_labels):
... scores = cross_val_score(estimator=clf,
... X=X_train,
... y=y_train,
... cv=10,
... scoring='roc_auc')
... print(f'ROC AUC: {scores.mean():.2f} '
... f'(+/- {scores.std():.2f}) [{label}]')
ROC AUC: 0.92 (+/- 0.15) [Logistic regression]
ROC AUC: 0.87 (+/- 0.18) [Decision tree]
ROC AUC: 0.85 (+/- 0.13) [KNN]
ROC AUC: 0.98 (+/- 0.05) [Majority voting]
```

* 위 코드의 실행 결과를 통해 MajorityVotingClassifier의 성능이 10배 교차 검증 평가에서 개별 분류기에 비해 향상되었음.

## 3. Evaluating and tuning the ensemble classifier

* 이 섹션에선 MajorityVoteClassifier가 보이지 않는 데이터로 잘 일반화되는지 확인하기 위해 테스트 데이터 세트에서 ROC 곡선을 계산
* 테스트 데이터 세트가 모델 선택에 사용되어서는 안됨. 테스트 데이터 세트는 단지 분류기 시스템의 일반화 성능에 편견 없는 추정치를 구하는 것임.

```python
>>> from sklearn.metrics import roc_curve
>>> from sklearn.metrics import auc
>>> colors = ['black', 'orange', 'blue', 'green']
>>> linestyles = [':', '--', '-.', '-']
>>> for clf, label, clr, ls \
... in zip(all_clf, clf_labels, colors, linestyles):
... # assuming the label of the positive class is 1
... y_pred = clf.fit(X_train,
... y_train).predict_proba(X_test)[:, 1]
... fpr, tpr, thresholds = roc_curve(y_true=y_test,
... y_score=y_pred)
... roc_auc = auc(x=fpr, y=tpr)
... plt.plot(fpr, tpr,
... color=clr,
... linestyle=ls,
... label=f'{label} (auc = {roc_auc:.2f})')
>>> plt.legend(loc='lower right')
>>> plt.plot([0, 1], [0, 1],
... linestyle='--',
... color='gray',
... linewidth=2)
>>> plt.xlim([-0.1, 1.1])
>>> plt.ylim([-0.1, 1.1])
>>> plt.grid(alpha=0.5)
>>> plt.xlabel('False positive rate (FPR)')
>>> plt.ylabel('True positive rate (TPR)')
>>> plt.show()
```

![4](https://user-images.githubusercontent.com/113302607/196036821-1234ca52-1c3c-4904-ad2f-d72fad360315.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.4. The ROC curve for the different classifiers
</figcaption>

* 결과에서 볼 수 있듯이 앙상블 분류기는 테스트 데이터 세트에서도 우수한 성능을 발휘(ROC AUC = 0.95).
* 그러나 로지스틱 회귀 분석 분류기가 동일한 데이터 집합에서 유사하게 수행된다는 것을 알 수 있음. 이는 데이터 집합의 크기가 작을 때 분산(데이터 집합을 분할하는 방법의 민감도)이 높기 때문임.

* 이 예시에서는 두 개의 특성만이 사용되기 때문에 결정경계 확인이 가능함. 
* 아래 코드는 결정트리 결정 경계를 다른 모델과 특성 크기를 맞추기 위해 작성되었음. 

```python
>>> sc = StandardScaler()
>>> X_train_std = sc.fit_transform(X_train)
>>> from itertools import product
>>> x_min = X_train_std[:, 0].min() - 1
>>> x_max = X_train_std[:, 0].max() + 1
>>> y_min = X_train_std[:, 1].min() - 1
>>>
>>> y_max = X_train_std[:, 1].max() + 1
>>> xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
... np.arange(y_min, y_max, 0.1))
>>> f, axarr = plt.subplots(nrows=2, ncols=2,
... sharex='col',
... sharey='row',
... figsize=(7, 5))
>>> for idx, clf, tt in zip(product([0, 1], [0, 1]),
... all_clf, clf_labels):
... clf.fit(X_train_std, y_train)
... Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
... Z = Z.reshape(xx.shape)
... axarr[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.3)
... axarr[idx[0], idx[1]].scatter(X_train_std[y_train==0, 0],
... X_train_std[y_train==0, 1],
... c='blue',
... marker='^',
... s=50)
... axarr[idx[0], idx[1]].scatter(X_train_std[y_train==1, 0],
... X_train_std[y_train==1, 1],
... c='green',
... marker='o',
... s=50)
... axarr[idx[0], idx[1]].set_title(tt)
>>> plt.text(-3.5, -5.,
... s='Sepal width [standardized]',
... ha='center', va='center', fontsize=12)
>>> plt.text(-12.5, 4.5,
... s='Petal length [standardized]',
... ha='center', va='center',
... fontsize=12, rotation=90)
>>> plt.show()
```

![5](https://user-images.githubusercontent.com/113302607/196036824-7d206fcd-3140-493d-b732-a6e1916968da.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.5. The decision boundaries for the different classifiers
</figcaption>


* 앙상블 분류를 위해 개별 분류기의 매개 변수를 조정하기 전에 GridSearchCV 개체 내부의 개별 매개 변수에 액세스하는 방법을  get_params 메서드를 호출

```python
>>> mv_clf.get_params()
{'decisiontreeclassifier':
 DecisionTreeClassifier(class_weight=None, criterion='entropy',
 max_depth=1, max_features=None,
 max_leaf_nodes=None, min_samples_leaf=1,
 min_samples_split=2,
 min_weight_fraction_leaf=0.0,
 random_state=0, splitter='best'),
 'decisiontreeclassifier__class_weight': None,
 'decisiontreeclassifier__criterion': 'entropy',
 [...]
 'decisiontreeclassifier__random_state': 0,
 'decisiontreeclassifier__splitter': 'best',
 'pipeline-1':
 Pipeline(steps=[('sc', StandardScaler(copy=True, with_mean=True,
 with_std=True)),
 ('clf', LogisticRegression(C=0.001,
Chapter 7 221
 class_weight=None,
 dual=False,
 fit_intercept=True,
 intercept_scaling=1,
 max_iter=100,
 multi_class='ovr',
 penalty='l2',
 random_state=0,
 solver='liblinear',
 tol=0.0001,
 verbose=0))]),
 'pipeline-1__clf':
 LogisticRegression(C=0.001, class_weight=None, dual=False,
 fit_intercept=True, intercept_scaling=1,
 max_iter=100, multi_class='ovr',
 penalty='l2', random_state=0,
 solver='liblinear', tol=0.0001, verbose=0),
 'pipeline-1__clf__C': 0.001,
 'pipeline-1__clf__class_weight': None,
 'pipeline-1__clf__dual': False,
 [...]
 'pipeline-1__sc__with_std': True,
 'pipeline-2':
 Pipeline(steps=[('sc', StandardScaler(copy=True, with_mean=True,
 with_std=True)),
 ('clf', KNeighborsClassifier(algorithm='auto',
 leaf_size=30,
 metric='minkowski',
 metric_params=None,
 n_neighbors=1,
 p=2,
 weights='uniform'))]),
 'pipeline-2__clf':
 KNeighborsClassifier(algorithm='auto', leaf_size=30,
 metric='minkowski', metric_params=None,
 n_neighbors=1, p=2, weights='uniform'),
 'pipeline-2__clf__algorithm': 'auto',
 [...]
 'pipeline-2__sc__with_std': True}
```

* 위의 긴 코드를 통해 get_params 메서드에서 반환된 값을 기반으로 이제 개별 분류자의 속성에 액세스하는 방법을 알게 되었음. 예시를 위해 그리드 서치를 통해 로지스틱 회귀 분류기의 역 정규화 매개 변수 C와 결정 트리 깊이를 조정함.

```python
>>> from sklearn.model_selection import GridSearchCV
>>> params = {'decisiontreeclassifier__max_depth': [1, 2],
... 'pipeline-1__clf__C': [0.001, 0.1, 100.0]}
>>> grid = GridSearchCV(estimator=mv_clf,
... param_grid=params,
... cv=10,
... scoring='roc_auc')
>>> grid.fit(X_train, y_train)
```

* 그리드 서치 완료 후, 다음과 같이 10배 교차 검증을 통해 계산된 다른 초 매개 변수 값 조합과 평균 ROC AUC 점수를 뽑을 수 있음.

```python
>>> for r, _ in enumerate(grid.cv_results_['mean_test_score']):
... mean_score = grid.cv_results_['mean_test_score'][r]
... std_dev = grid.cv_results_['std_test_score'][r]
... params = grid.cv_results_['params'][r]
... print(f'{mean_score:.3f} +/- {std_dev:.2f} {params}')
0.983 +/- 0.05 {'decisiontreeclassifier__max_depth': 1,
 'pipeline-1__clf__C': 0.001}
0.983 +/- 0.05 {'decisiontreeclassifier__max_depth': 1,
 'pipeline-1__clf__C': 0.1}
0.967 +/- 0.10 {'decisiontreeclassifier__max_depth': 1,
 'pipeline-1__clf__C': 100.0}
0.983 +/- 0.05 {'decisiontreeclassifier__max_depth': 2,
 'pipeline-1__clf__C': 0.001}
0.983 +/- 0.05 {'decisiontreeclassifier__max_depth': 2,
 'pipeline-1__clf__C': 0.1}
0.967 +/- 0.10 {'decisiontreeclassifier__max_depth': 2,
 'pipeline-1__clf__C': 100.0}
>>> print(f'Best parameters: {grid.best_params_}')
Best parameters: {'decisiontreeclassifier__max_depth': 1,
 'pipeline-1__clf__C': 0.001}
>>> print(f'ROC AUC : {grid.best_score_:.2f}')
ROC AUC: 0.98
```

* 위 결과를 통해, 정규화 강도가 가장 낮음(C=0.001)를 선택할 때 최상의 교차 검증 결과를 얻는 반면, 트리 깊이는 성능에 전혀 영향을 미치지 않는 것으로 보임. 


## 4. Bagging – building an ensemble of classifiers from bootstrap samples
* Bagging은 이전 섹션에서 구현한 Majority Vote Classifier와 관련이 있는 앙상블 학습 기법임.
* 그러나 앙상블의 개별 분류기에 맞추기 위해 동일한 훈련 데이터 세트를 사용하는 대신 초기 훈련 데이터 세트에서 부트스트랩 샘플(교체가 있는 랜덤 샘플)을 추출하므로 bagging을 bootstrap aggregating라고불림.

![6](https://user-images.githubusercontent.com/113302607/196036836-675e1a6c-94b9-4a80-badf-ef225f5ca8f3.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.6. The concept of bagging
</figcaption>

* 컨셉은 위 그림과 같음. 


#### * Bagging in a nutshell

![7](https://user-images.githubusercontent.com/113302607/196036839-91cb64ab-4f1d-45c3-b529-0d566f85f6d6.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.7. An example of bagging
</figcaption>

* 위 그림은 bagging 분류기의 bootstrap aggregating가 작동하는 방식에 대한 보다 구체적인 예임.
* 여기서, 각 bagging 라운드에서 교체와 함께 랜덤으로 샘플링되는 7개의 다른 훈련 인스턴스(지수 1-7로 표시됨)를 가지고 있음.
* 각 분류기는 훈련 데이터 세트에서 예제의 랜덤 하위 집합을 수신함.
* bagging 통해 얻은 이러한 무작위 샘플을 Bagging 라운드 1, Bagging 라운드 2 등으로 나타냄.
* 각 하위 집합에는 중복의 특정 부분이 포함되어 있으며, 치환을 사용한 샘플링으로 인해 원래 예 중 일부가 리샘플링된 데이터 세트에 전혀 나타나지 않음.
* 개별 분류기가 부트스트랩 샘플에 적합하면 다수결 투표를 사용하여 예측이 결합

#### * Applying bagging to classify examples in the Wine dataset

```python
>>> import pandas as pd
>>> df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/'
... 'machine-learning-databases/'
... 'wine/wine.data',
... header=None)
>>> df_wine.columns = ['Class label', 'Alcohol',
... 'Malic acid', 'Ash',
... 'Alcalinity of ash',
... 'Magnesium', 'Total phenols',
... 'Flavanoids', 'Nonflavanoid phenols',
... 'Proanthocyanins',
... 'Color intensity', 'Hue',
... 'OD280/OD315 of diluted wines',
... 'Proline']
>>> # drop 1 class
>>> df_wine = df_wine[df_wine['Class label'] != 1]
>>> y = df_wine['Class label'].values
>>> X = df_wine[['Alcohol',
... 'OD280/OD315 of diluted wines']].values
>>> from sklearn.preprocessing import LabelEncoder
>>> from sklearn.model_selection import train_test_split
>>> le = LabelEncoder()
>>> y = le.fit_transform(y)
>>> X_train, X_test, y_train, y_test =\
... train_test_split(X, y,
... test_size=0.2,
... random_state=1,
... stratify=y)
```

* 구현을 위해 와인 데이터 세트를 이용, 데이터 세트를 80% 훈련 및 20% 테스트 데이터 세트로 분할
* Bagging 분류기 알고리듬은 이미 skickit-learn에 구현되어 있으며, 앙상블 하위 모듈에서 가져올 수 있음.
* 가지치기되지 않은 의사 결정 트리를 기본 분류기로 사용, 훈련 데이터 세트의 다양한 부트스트랩 샘플에 맞는 500개의 의사 결정 트리의 앙상블을 만듦.

![8](https://user-images.githubusercontent.com/113302607/196036843-9cb05646-8daa-4b82-8489-1060ceeca80e.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.8. : The piece-wise linear decision boundary of a decision tree versus bagging
</figcaption>

* 위 실행결과를 통해 결과 3노드 심층 결정 트리의 조각별 선형 결정 경계는 Bagging 앙상블에서 더 부드러워 보이는 결과를 확인할 수 있음.
* Bagging 알고리듬이 모델의 분산을 줄이는 효과적인 접근법이 될 수 있다는 것에 주목해야 함.
* 그러나, Bagging은 모델 편향, 즉 데이터의 추세를 잘 포착하기에는 너무 간단한 모델을 줄이는 데 효과적이지 않음.
* 이것이 우리가 낮은 편향을 가진 분류기 앙상블, 예를 들어 가지 없는 결정 트리에서 Bagging 수행하고자 하는 이유임.

## 6. Leveraging weak learners via adaptive boosting
* 앙상블 방법 마지막 섹션에서는 가장 일반적인 구현에 특히 중점을 두고 부스팅에 대해 논의할 것: **적응 부스팅(AdaBoost)**
* 부스팅에서 앙상블은 매우 간단한 기본 분류기로 구성, 부스팅의 핵심 개념은 분류하기 어려운 훈련 예에 초점을 맞추는 것임. 

#### * How adaptive boosting works
* Bagging과 달리, 부스팅 알고리듬의 초기 공식은 대체 없이 훈련 데이터 세트에서 도출된 훈련 예제의 무작위 하위 집합을 사용

![9](https://user-images.githubusercontent.com/113302607/196036848-eebf1de9-1b89-4073-8ea5-7fef930b2ae5.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.9. The concept of AdaBoost to improve weak learners
</figcaption>

* 위 그림은 AdaBoost의 기본 개념을 나타냄.


#### * Applying AdaBoost using scikit-learn
* 이제 사이킷런을 통해 AdaBoost 앙상블 분류기를 훈련
* 와인 서브셋을 사용하여 배깅 메타 분류기를 훈련함.

* base_estimator 속성을 통해 500개의 의사 결정 트리 스택에 대해 AdaBoostClassifier를 훈련함. 

```python
>>> from sklearn.ensemble import AdaBoostClassifier
>>> tree = DecisionTreeClassifier(criterion='entropy',
... random_state=1,
... max_depth=1)
>>> ada = AdaBoostClassifier(base_estimator=tree,
... n_estimators=500,
... learning_rate=0.1,
... random_state=1)
>>> tree = tree.fit(X_train, y_train)
>>> y_train_pred = tree.predict(X_train)
>>> y_test_pred = tree.predict(X_test)
>>> tree_train = accuracy_score(y_train, y_train_pred)
>>> tree_test = accuracy_score(y_test, y_test_pred)
>>> print(f'Decision tree train/test accuracies '
... f'{tree_train:.3f}/{tree_test:.3f}')
Decision tree train/test accuracies 0.916/0.875
```

* 결과를 통해, 의사결정 트리 스텀프는 이전 섹션에서 보았던 가지치기되지 않은 의사결정 트리와 달리 훈련 데이터에 적합하지 않은 것으로 보임.

```python
>>> ada = ada.fit(X_train, y_train)
>>> y_train_pred = ada.predict(X_train)
>>> y_test_pred = ada.predict(X_test)
>>> ada_train = accuracy_score(y_train, y_train_pred)
>>> ada_test = accuracy_score(y_test, y_test_pred)
>>> print(f'AdaBoost train/test accuracies '
... f'{ada_train:.3f}/{ada_test:.3f}')
AdaBoost train/test accuracies 1.000/0.917
```

* 여기서, AdaBoost 모델이 훈련 데이터 세트의 모든 클래스 레이블을 정확하게 예측하고 의사 결정 트리 스텀프에 비해 약간 개선된 테스트 데이터 세트 성능을 보여줌.
* 그러나 모델 편향을 줄이기 위한 시도로 추가적인 분산이 도입되었다는 것을 알 수 있음. (즉, 훈련과 테스트 성능 간의 차이가 더 큼.)
* 마지막으로 결정영역을 확인함. 

```python
>>> x_min = X_train[:, 0].min() - 1
>>> x_max = X_train[:, 0].max() + 1
>>> y_min = X_train[:, 1].min() - 1
>>> y_max = X_train[:, 1].max() + 1
>>> xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
... np.arange(y_min, y_max, 0.1))
>>> f, axarr = plt.subplots(1, 2,
... sharex='col',
... sharey='row',
... figsize=(8, 3))
>>> for idx, clf, tt in zip([0, 1],
... [tree, ada],
... ['Decision tree', 'AdaBoost']):
... clf.fit(X_train, y_train)
... Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
... Z = Z.reshape(xx.shape)
... axarr[idx].contourf(xx, yy, Z, alpha=0.3)
... axarr[idx].scatter(X_train[y_train==0, 0],
... X_train[y_train==0, 1],
... c='blue',
... marker='^')
... axarr[idx].scatter(X_train[y_train==1, 0],
... X_train[y_train==1, 1],
... c='green',
... marker='o')
... axarr[idx].set_title(tt)
... axarr[0].set_ylabel('Alcohol', fontsize=12)
>>> plt.tight_layout()
>>> plt.text(0, -0.2,
... s='OD280/OD315 of diluted wines',
... ha='center',
... va='center',
... fontsize=12,
... transform=axarr[1].transAxes)
>>> plt.show()
```

![11](https://user-images.githubusercontent.com/113302607/196036853-67579602-1365-4b3f-9873-2d77efaea545.png)
<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.11. The decision boundaries of the decision tree versus AdaBoost
</figcaption>

* 결정 영역을 통해서 AdaBoost 모델의 결정 경계가 결정 스텀프의 결정 경계보다 훨씬 복잡하다는 것을 알 수 있음.
* 또한 AdaBoost 모델은 이전 섹션에서 학습한 Bagging 분류기와 매우 유사하게  공간을 구분
* 앙상블 학습은 개별 분류기에 비해 계산 복잡성을 증가시킨다는 점에 주목할 필요가 있음.
* 실제로, 우리는 예측 성능의 종종 비교적 완만한 개선을 위해 증가한 계산 비용의 대가를 지불하기를 원하는지에 대해 신중하게 생각할 필요가 있음.
