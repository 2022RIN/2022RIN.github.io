---
layout: single
title: "Chapter 19. 복잡한 환경에서의 의사결정을 위한 강화학습"
use_math: true
---

## Topic 

* 이 장에서는 전반적인 **보상(reward)** 를 최적화하기 위한 일련의 **행동(action)** 을 학습하는데 중점을 두고 있기에 이전 범주와는 다른 기계 학습, **강화 학습(reinforcement learning; RL)** 로 관심을 돌리며 다음과 같은 항목을 다룸.

    * 1) 강화학습의 기초를 배우고, 에이전트/환경 상호 작용에 익숙해지고, 보상 프로세스가 어떻게 작동하는지 이해
    * 2) 다양한 범주의 RL 문제, 모델 기반 및 모델 없는 학습 과제, 몬테 카를로, 시간적 차이 학습 알고리즘 도입
    * 3) 구현 표 형식의 Q-러닝 알고리즘
    * 4) RL 문제를 해결하기 위한 함수 근사치 이해, 심층 Q-러닝 알고리즘을 구현하여 RL과 딥러닝을 결합
    

## 1. Introduction – learning from experience(경험을 통한 학습)

* 이 섹션에서는 강화학습의 개념을 머신러닝의 다른 작업과 비교하여 주요 차이점을 확인함. 
* 강화학습의 기본 구성 요소를 다룸.
* 마르코프 결정과정을 기반으로 한 강화학습의 수학 공식을 살펴봄.

### Understanding reinforcement learning: 강화학습의 이해 

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.1. 머신러닝의 학습 방법 분류
</figcaption>

* 앞의 내용에서는 지도학습과 비지도 학습에 초점을 맞춰왔으며, 머신러닝의 3가지 분류는 다음과 같음. 
* **지도학습:** 감독자가 제공하는 레이블링 된 훈련 예제에 의존함.(즉, 정답이 있는 데이터로 훈련을 함. 컴퓨터는 자신이 낸 답과 정답의 차이를 통해 지속적으로 학습함.)
* **비지도학습:** 정답없이 주어진 데이터 세트의 기본 구조를 학습 또는 캡처 하여 유사한 분포를 가지는 훈련예제를 생성하는 것을 의미함.
* --------------------------------------------------------------------------------------


* **강화학습:** 강화학습은 지도 및 비지도 학습과 크게 다르기 때문에 종종 "기계학습의 세 번째 범주"로 간주 됨. 강화학습을 지도 학습 및 비지도 학습과 같은 머신 러닝의 다른 하위 작업과 구별하는 핵심 요소는 **상호 작용에 의한 학습 개념을 중심으로 한다는 것** 임. 
* 이는 **강화학습에서 모델이 보상 함수를 최대화하기 위해 환경과의 상호 작용에서 학습한다는 것을 의미함.**
* 보상 함수를 최대화 하는 것은 지도 학습에서 손실 함수(loss function)를 최소화하는 개념과 관련이 있지만, 일련의 동작을 학습하기 위한 **올바른 레이블(정답을 의미)은 강화학습에서 사전에 알려져 있거나 정의되지 않음.** 
* 대신, **원하는 결과(게임에서 이기는 것 등)를 달성하기 위해서 환경과의 상호 작용을 통해 학습해야함.** 
* 모델(에이전트라고도 함)은 환경과 상호 작용하며, 그렇게 함으로써 **에피소드**라고 불리는 일련의 상호 작용을 생성함. 
* 이러한 상호 작용을 통해 에이전트는 환경에 의해 결정된 일련의 보상을 수집함. 이러한 **보상은 긍정적이거나 부정적**일 수 있으며, 때로는 에피소드가 끝날 때까지 에이전트에게 공개되지 않음.
* 강화학습에서 우리는 에이전트에게 일을하는 방법을 가르치지 않고, 에이전트가 성취하기를 원하는 것(목표)만을 명시할 수 있음. 그런 다음 에이전트의 승패에 따라 보상을 결정할 수 있음. 이는 특히 **문제 해결 과제가 알수 없거나 복잡한 의사 결정 문제**에서 매력적인 알고리즘으로 적용됨. 

### Defining the agent-environment interface of a reinforcement learning system: 강화학습 시스템의 에이전트-환경 인터페이스 정의 

* 강화학습에서 **에이전트(agnet)와 환경(environment)** 라는 두 개의 별개 실체를 정의할 수 있음.
* **에이전트**: 공식적으로 결정을 내리는 방법을 배우고 조치를 취함으로써 주변 환경과 상호작용하는 개체를 의미함. 
* **환경**: 에이전트 외부에 있는 모든 것으로, 환경은 에이전트과의 커뮤니케이션을 통해 에이전트의 작업 및 관찰에 대한 보상 신호를 결정함. 
* **보상**: 에이전트가 환경과의 상호작용하여 받는 피드백으로 일반적으로 스칼라 값 형태로 제공되며 양 또는 음일 수 있음. **보상의 목적은 에이전트에게 그것이 얼마나 잘 수행되었는지 알려주는 것임.** 에이전트가 보상을 받는 빈도는 주어진 작업 또는 문제에 따라 달라짐.
* 즉, 강화학습의 목적은 알려지지 않은 문제에서 에이전트가 환경과의 상호작용을 통해 얻은 누적 보상의 합을 최대로 하는 최적의 정책을 찾고자함임. 


<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.1. 에이전트와 환경과의 상호 작용 
</figcaption>

* 그림 1은 에이전트와 환경 간의 상호 작용 및 통신을 보여줌.
* 그림 1과 같이 에이전트의 상태는 모든 변수 (1)의 집합임.
* 예를 들어, 로봇 드론의 경우 이러한 변수들은 드론의 현재 위치(경도, 위도, 고도), 드론의 남은 배터리 수명, 각 팬의 속도 등을 포함할 수 있음. 
* 각 시간 단계에서 에이전트는 (2)에서 사용 가능한 일련의 작업을 통해 환경과 상호작용을 함. 
* 상태 $S_{t}$에 있는 동안 $A_{t}$로 표시된 에이전트가 수행하는 동작에 기초하여 에이전트는 보상 신호 (3) $R_{t+1}$을 수신하며 상태는(4) $S_{t+1}$이 됨. 


* 학습 과정 중 에이전트는 총 누적 보상을 극대화하기 위해 **어떤 행동을 선호하고 더 자주 수행(= 활용 exploitation)**해야 하는지 점진적으로 학습할 수 있도록 **다양한 행동(= 탐험 exploration)**을 시도해야함. 
* 위 개념(활용과 탐험)을 이해하기 위해 공학도 졸업생이 학문에 대해 더 깊게 배우기 위해 석사 학위를 추구(= 활용 exploitation)해야할지 회사에서 일을 시작(= 탐험 exploration)해야할지에 대한 고민을 예로 들 수 있음.
* 일반적으로 활용(exploitation)은 단기적 보상이 더 큰 행동을 선택하는 반면, 탐험(exploration)은 장기적으로 더 큰 총 보상을 잠재적으로 초래할 수 있음. 활용과 탐험에 사이에 대한 균형은 광범위하게 연구되어 왔지만, 이 의사 결정 딜레마에 대한 보편적인 해답은 없음. 

## 2. The theoretical foundations of RL: RL의 이론적 기초

* 다음 섹션에서는 먼저 마르코프 의사결정(Markov decision processes)과정의 수학적 공식화, 에피소드 대 연속 작업, 일부 핵심 강화학습 용어 및 벨만 방적식을 사용한 동적 프로그래밍 검토

### Markov decision processes: 마르코프 의사결정 과정 

* 일반적으로 강화학습이 다루는 문제의 유형은 일반적으로 **마르코프 의사결정 프로세스(MDP)**로 공식화됨.
* MDP 문제를 해결하기 위한 표준 접근법은 동적 프로그래밍을 사용하는 것이지만 강화학습은 동적 프로그래밍에 비해 몇 가지 주요 이점을 제공함. 
* 그러나 동적 프로그래밍은 상태의 크기(즉, 가능한 구성의 수)가 상대적으로 클 때 실현 가능한 접근법이 아님. 이러한 경우, 강화학습은 MDP를 해결하기 위한 훨씬 더 효율적이고 실용적인 대안 접근법으로 간주됨. 

### The mathematical formulation of Markov decision processes: 마르코프 결정 과정의 수학적 공식화 

* 시간 단계 $t$의 결정이 후속 상황에 영향을 미치는 대화형 및 순차적 의사 결정 과정을 학습해야하는 문제 유형은 수학적으로 MDP로 공식화됨. 

* 강화학습에서 에이전트 환경과의 상호작용의 경우 에이전트 시작 상태를 $S_{0}$로 나타내면 에이전트와 환경 사이의 상호작용은 다음과 같은 순서로 이루어짐.: 
* $\{ S_{0}, A_{0}, R_{1} \}, \{ S_{1}, A_{1}, R_{2} \}, \{ S_{2}, A_{2}, R_{3} \}$

* 여기서, $S_{t}$ 와 $A_{t}$는 시간 단계 $t$에서 상태와 수행된 행동을 나타냄. 
* $R_{t+1}$는 $A_{t}$ 행동을 수행한 후 환경으로부터 받은 보상을 의미함. 

* $S_{t}$, $A_{t}$, $R_{t+1}$은 각각 $s \in \widehat{S}$, $r \in \widehat{R}$, $a \in \widehat{A}$로 표시하는 미리 정의된 유한 집합의 값을 취하는 시간 의존 랜덤 변수임. 

* **** 무슨의미??*****
* MDP에서 이러한 시간 종속 랜덤 변수인 $S_{t}$ 및 $R_{t+1}$은 이전 시간 단계인 $t – 1$에서만 값에 의존하는 확률 분포를 가짐.
* $S_{t+1}=s^{'}$ 및 $R_{t+1}=r$에 대한 확률 분포는 이전 상태 $S_{t}$에 대한 조건부 확률로 작성할 수 있으며 다음과 같은 행동 $A_{t}$를 취할 수 있음. 

* $p(s^{'},r \mid s,a) = p(S_{t+1}=s^{'}, R_{t+1}=r \mid S_{t}=s, A_{t}=a)$

* 이 확률 분포는 환경(또는 환경 모델)의 역학을 완전히 정의함. 이 분포를 기반으로 환경의 모든 전이 확률을 계산할 수 있기 때문임.
* 따라서 환경 역학은 다양한 강화학습 방법을 분류하는 중심 기준임. 
* 환경의 모델을 필요로 하거나 환경의 모델(즉, 환경 역학(environment dynamics))을 배우려고 하는 강화학습 방법의 유형을 모델 프리(model-free) 방법과 반대로 모델 기반 방법(model-base)이라함. 
* 환경 역학은 주어진 상태에 대한 특정 행동이 항상 취해지거나 취해지지 않는다는 것을 통해 결정론적인 것으로 간주될 수 있음. 그것은 $p(s^{'},r|s,a) \in \{0,1\}$로 표현됨.
* 더 일반적인 경우, 환경은 확률적인 행동을 가짐. 

* 이러한 확률적 행동을 이해하기 위해 현재 상태와 수행된 행동에서 조건화된 미래 상태를 관찰할 확률을 고려하면 다음과 같이 표시됨. 
* $p(s^{'} \mid s,a) = p(S_{t+1}=s^{'}, R_{t+1}=r \mid S_{t}=s, A_{t}=a)$

* 가능한 모든 보상에 대해 합계를 취함으로써 한계 확률로 계산할 수 있음.
* 이 확률을 상태 전이 확률이라고함. 상태 전이 확률에 기초하여 환경 역학이 결정론적이라면, 에이전트가 상태 $S_{t}=s$에서 조치를 취할 때, 다음 상태인 $S_{t+1}=s^{'}$로의 전이가 백퍼센트 확실하다는 것을 의미함. 즉 $p(s^{'}|s,a) = 1$

### Visualization of a Markov process

* 마르코프 과정은 그래프의 노드가 환경의 다른 상태를 나타내는 방향 순환 그래프로 표현될 수 있음.
* 그래프의 가장자리(즉, 노드 간 연결)는 상태 간의 전이 확률을 나타냄. 

### Episodic versus continuing tasks

* 에이전트가 환경과 상호 작용할 때, 관찰 또는 상태의 시퀀스는 궤적을 형성함. 궤적에는 두가지 유형이 존재함. 
* 에이전트의 궤적이 각각 시간 t=0에서 시작하여 터미널 상태 $S_{T}(at t=T)$에서 끝나도록 하위 부분으로 분할될 수 있는 경우, 작업을 **에피소드 작업(episode task)** 이라함. 
* 반면에, 궤도가 터미널 상태 없이 무한히 연속적인 경우, 그 작업을 연속 작업(continuing task)이라함. 이 장에서는 에피소드 작업만을 고려함. 

* 에피소드는 에이전트가 시작 상태 $S_{0}$에서 끝 상태 $S_T$로 이동하는 일련의 과정 또는 궤적을 의미함.

## 3. RL terminology: return, policy, and value function
* RL 특정 용어를 정의함. 

### The return

* 시간 $t$에서의 리턴은 에피소드 전체 지속 시간으로부터 얻은 누적 보상을 의미함. $R_{t+1}=r$은 시간 $t$에서 행동 $A_{t}$를 수행한 후 얻은 즉각적인 보상이며, 후속 보상은 $R_{t+2}=r$, $R_{t+3}=r$ 등등 임. 
* 시간 $t$에서의 리턴은 다음과 같이 즉각적인 보상과 후속 보상으로 계산될 수 있음. 
* $G_{t} = R_{t+1} +  \gamma R_{t+2} + \gamma^{2} R_{t+3} + ... =  \Sigma \gamma^{k} R_{t+k+1}$

* 여기서 $\gamma$는 [0, 1]범위의 할인률(discount factor)임. 매개변수 $\gamma$는 현재 순간(시간 $t$)에서 **미래 보상이 얼마나 가치가 있는지**를 나타냄.
* $\gamma = 0$이면 설정하면 향후 보상에 대해 신경 쓰지 않는다는 것을 의미함. 이 경우 $t + 1$ 이후의 후속 보상은 무시하고 즉시 보상과 동일하게 반환되며 에이전트는 근시안적이됨.

* 또한, 리턴 방정식은 다음과 같이 재귀를 사용하여 더 간단한 방식으로 표현할 수 있음.
* $ G_{t}=R_{t+1}+ \gamma G_{t+1} = r + \gamma G_{t+1} $
* 이는 시간 t의 수익이 즉시 보상자에 시간 t + 1의 할인된 미래 수익을 더한 것과 같다는 것을 의미하며 반환 계산을 용이하게 하는 매우 중요한 속성임. 

### Policy

* 일반적으로 $ \pi(a|s)$로 표시되는 정책은 다음에 취할 행동을 결정하는 기능으로, 결정론적이거나 확률적일 수 있다. (즉, 다음 행동을 취할 확률임. 확률적 정책은 주어진 상태에서 에이전트가 취할 수 있는 행동에 대한 확률 분포를 가짐.)
* 학습 과정 중 에이전트가 더 많은 경험을 쌓으면 정책이 변경될 수 있음. 예를 들어, 에이전트는 모든 작업의 확률이 균일한 임의의 정책에서 시작할 수 있지만, 에이전트는 최적의 정책(optimal policy)에 도달하기 위해 정책을 최적화하는 방법을 배울 수 있음. 최적 정책은 가장 높은 수익을 내는 정책임. 

### Value function

* 상태-값 함수라고하는 값 함수는 각 상태의 선량, 즉 특정 상태에 있는 것이 얼마나 좋은지 또는 나쁜지를 측정함. (선량에 대한 기준은 반환에 기초한다는 점에 유의)
* 이제 리턴 $G_{t}$를 기반으로 다음 정책을 따른 후 상태의 값 함수를 반환값의 기댓값(가능한 모든 에피소드의 평균 반환값)로 정의함. 
* 실제 구현에서는 대개 룩업 테이블을 사용하여 값 함수를 추정하므로 여러 번 다시 계산할 필요가 없음.(이것이 동적 프로그래밍 측면임) 
* 예를 들어, 실제로 이러한 표 형식 방법을 사용하여 값 함수를 추정할 때 모든 상태 값을 $V(s)$로 표시된 표에 저장함. 
* 파이썬 구현에서 이것은 인덱스가 서로 다른 상태를 참조하는 목록이나 NumPy 배열일 수도 있고, 사전 키가 상태를 각각의 값에 매핑하는 파이썬 사전일 수도 있음.
* 또한, 우리는 각 상태-행동 쌍에 대한 값을 정의할 수 있는데, 이를 행동-값 함수라고 하며, 이를 $q_{\pi}(s,a)$로 표시함.
* 행동 값 함수는 에이전트가 $S_{t} = s$ 상태에 있고 $A_{t} = a$에서 작용할 때 예상되는 반환 $G_{t}$를 나타냄.
* 일부 RL 알고리듬으로 직접 이동하기 전에 정책 평가를 구현하는 데 사용할 수 있는 벨만 방정식에 대한 파생에 대해 간략히 살펴봄.

## 4. Dynamic programming using the Bellman equation: 벨만 방정식을 이용한 동적 프로그래밍

* 벨만 방정식은 많은 RL 알고리듬의 중심 요소 중 하나임. 벨만 방정식은 값 함수의 계산을 단순화하여 여러 시간 단계에 걸쳐 합산하는 대신 반환 계산을 위한 재귀와 유사한 재귀를 사용


## 5. Reinforcement learning algorithms

* 이 섹션에서는 일련의 학습 알고리즘을 다룸. 알려지지 않은 환경 역학을 해결하기 위해, 환경과의 상호 작용을 통해 학습하는 RL 기술이 개발되었음. 
* 이러한 기술에는 **monte carlo(MC), temporal difference(TD) 학습, 점점 더 인기를 끌고 있는 Q-러닝 및 심층 Q-learning 접근법**이 포함됨. 

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.2. 다양한 유형의 RL 알고리즘 (동적 프로그래밍에서 Q-러닝으로 RL 알고리즘을 발전시키는 과정)
</figcaption>

### Dynamic programming

* 이 섹션에서는 다음과 같은 가정 하에 RL 문제를 해결하는 데 초점을 맞춤. 
* 우리는 환경 역학에 대한 완전한 지식을 가지고 있음. 즉 모든 전이 확률이 알려져 있음.
* 에이전트의 상태는 마르코프 속성을 가지고 있으며, 이는 다음 행동과 보상이 현재 또는 현재 시간 단계에서 수행하는 행동의 선택에만 의존한다는 것을 의미함.

* 동적 프로그래밍이 RL 문제를 해결하기 위한 실용적인 접근법이 아니라는 것을 강조해야함.
* 동적 프로그래밍 사용의 문제는 환경 역학에 대한 완전한 지식을 가정한다는 것임. 이는 대부분 실제 응용 프로그램에 비합리적, 비실용적임.
* 그러나 교육적 관점에서, 동적 프로그래밍은 RL을 단순한 방식으로 도입하는데 도움이 되며 더 복잡합 RL 알고리즘의 사용에 동기를 부여함. 

### Policy evaluation – predicting the value function with dynamic programming

* 벨만 방정식을 기반으로 환경 역학이 알려져 있을 때 동적 프로그래밍으로 임의의 정책 $\pi$에 대한 값 함수를 계산할 수 있음.
* 이 값 함수를 계산하기 위해, 우리는 각 상태에 대한 0 값으로 초기화되는 $v^{<0>}(s)$ 에서 시작하는 반복 솔루션을 적용할 수 있음. 그런 다음 각 반복 i + 1에서 벨만 방정식을 기반으로 각 상태에 대한 값을 업데이트함. 벨만 방정식은 이전 반복 i의 상태 값을 기반으로함.
* 반복이 무한대로 증가함에 따라 $v^{i}$(s)는 실제 상태 값 함수 $v_{\pi}$(s)로 수렴한다는 것을 알 수 있음.
* 또한 환경과 상호 작용할 필요가 없다는 점에 유의, 그 이유는 우리가 이미 환경 역학을 정확하게 알고 있기 때문임. 
* 그 결과, 우리는 이 정보를 활용하고 가치 함수를 쉽게 추정할 수 있음. 가치 함수를 계산한 후, 우리의 정책이 여전히 무작위 정책이라면 그 가치 함수가 우리에게 어떻게 유용할 수 있는가 하는 것이 명백한 질문임. 정답은 다음에 보게 될 것처럼 실제로 이 컴퓨터를 사용하여 정책을 개선할 수 있다는 것임.
  
  
### Improving the policy using the estimated value function



## 6. Reinforcement learning with Monte Carlo

* 이전 섹션에서 보았듯이 동적 프로그래밍은 환경의 역학이 완전히 알려져 있다는 단순한 가정에 의존함. 
* 동적 프로그래밍 접근 방식에서 벗어나 이제 우리는 환경 역학에 대한 지식이 없다고 가정함. 즉, 우리는 환경의 상태 전이 확률을 알지 못하며, 대신 에이전트가 환경과 상호 작용을 통해 학습하기를 원함. 
* MC 기반 RL의 경우 확률론적 정책 $\pi$ 를 따르는 에이전트 클래스를 정의하고, 이 정책을 기반으로 에이전트는 각 단계에서 행동을 취함. 이렇게 하면 시뮬레이션된 에피소드가 발생함.
* MC 기반 방법은 에이전트가 환경과 상호 작용하는 시뮬레이션된 에피소드를 생성하여 이 문제를 해결함. 이러한 시뮬레이션된 에피소드로부터, 우리는 시뮬레이션된 에피소드에서 방문한 각 주의 평균 수익을 계산할 수 있음.

### State-value function estimation using MC
  
* 각 상태에서 일련의 에피소드를 생성한 후, 모든 에피소드가 상태를 통과하는 일련의 에피소드를 고려하여 상태 값을 계산함. 룩업 테이블을 사용하여 값 함수 $V(S_{t}=s)$에 해당하는 값을 구한다고 가정함. 
* 값 함수를 추정하기 위한 MC 업데이트는 상태를 처음 방문한 시점부터 해당 에피소드에서 얻은 총 반환값을 기반으로함. 이 알고리즘을 첫 방문 몬테카를로 값 예측(first-visit Monte Carlo)이라함.
  
### Action-value function estimation using MC
  
* 환경 역학을 알 수 없는 경우, 첫 번째 방문 MC 상태-값 예측을 추정하는 알고리즘을 확장할 수 있음. 
* 예를 들어, 행동-값 함수를 사용하여 각 상태-행동 쌍에 대한 추정 수익을 계산할 수 있음.
* 이 추정 수익을 얻기 위해, 우리는 각 상태-행동 쌍(s, a)에 대한 방문을 고려하는데, 이는 상태를 방문하고 행동을 취하는 것을 의미함. 그러나 일부 행동이 선택되지 않아 탐색이 탐색이 부족할 수 있기 때문에 문제가 발생함.
* 이 문제를 해결하는 몇 가지 방법이 있습니다. 가장 간단한 접근법은 탐색적 시작(exploratory start)이라고 하며, 모든 상태-행동 쌍이 에피소드의 시작에서 0이 아닌 확률을 갖는다고 가정. 이러한 탐구 부족 문제를 해결하기 위한 또 다른 접근법은 **𝜖-그리디 정책(𝜖𝜖-greedy policy)** 이라고 하며, 이는 정책 개선에 대한 다음 섹션에서 논의될 것임.
  
### Finding an optimal policy using MC control
  
* MC 제어는 정책을 개선하기 위한 최적화 절차를 의미함. 최적의 정책에 도달할 때까지 정책 평가와 정책 개선 사이를 반복적으로 교대할 수 있음. 따라서 임의의 정책인$\pi_{0}$부터 시작하여 정책평가와 정책개선을 번갈아 수행하는 과정을 다음과 같이 설명할 수 있음.
  
(1)
  
### Policy improvement – computing the greedy policy from the actionvalue function

* 행동 값 함수 q(s, a)가 주어지면, 우리는 다음과 같이 탐욕스러운(결정론적) 정책을 생성할 수 있음. 

## 7. Temporal difference learning

* 동적 프로그래밍은 환경 역학에 대한 완전하고 정확한 지식에 의존함. 반면, MC 기반 방법은 시뮬레이션 경험을 통해 학습함. 
* 이 섹션에서는 이제 MC 기반 RL 접근 방식의 개선 또는 확장으로 간주할 수 있는 **TD 학습**이라는 세 번째 RL 방법을 소개할 것임.
* MC 기법과 유사하게 TD 학습도 경험에 의한 학습을 기반으로 하므로 환경 역학 및 전환 확률에 대한 지식이 필요하지 않음.
* TD와 MC 기법의 가장 큰 차이점은 MC에서는 에피소드가 끝날 때까지 기다려야 총 수익률을 계산할 수 있다는 점임.
* 그러나 TD 학습에서는 학습된 속성 중 일부를 활용하여 에피소드의 끝에 도달하기 전에 추정된 값을 업데이트할 수 있음. 이를 **부트스트랩(bootstrapping)이라함.**


### TD prediction

* 먼저 MC의 가치 예측은 다음과 같음. 각 에피소드가 끝날 때마다 각 시간 단계 t에 대한 수익률 $G_{t}$를 추정할 수 있음. 따라서 방문한 주에 대한 추정치를 다음과 같이 업데이트할 수 있음. 


여기서 Gt는 추정치를 업데이트하기 위한 목표 복귀로 사용되며, (Gt – V(St))는 현재 추정치 V(St)에 추가된 보정 용어입니다. 𝛼는 학습 속도를 나타내는 초 매개 변수로, 학습 중에 일정하게 유지된다. MC에서 수정 용어는 실제 반환인 Gt를 사용하며, 이는 에피소드가 끝날 때까지 알려지지 않습니다. 이를 명확히 하기 위해 실제 반환값 Gt를 Gt:T로 이름을 바꿀 수 있는데, 여기서 subst:T는 시간 단계 t에서 얻은 반환값임을 나타내며, 시간 단계 T에서 마지막 시간 단계 T까지 발생한 모든 이벤트를 고려한다. TD 학습에서 실제 반환값 Gt:T를 새로운 목표 반환값 Gt:t+1로 대체하는데, 이는 상당히 단순하다.값 함수 V(St)에 대한 업데이트를 활성화합니다. TD 학습에 기반한 업데이트 공식은 다음과 같습니다.




## 8. Implementing our first RL algorithm

* 이 섹션에선 그리드 월드(grid world) 문제를 해결하기 위한 Q-learning 알고리즘 구현에 대해 다룰 것임.(그리드 월드는 에이전트가 가능한한 많은 보상을 받기 위해 네 방향으로 움직이는 2차원 셀 기반 환경임.) OpenAI Gym 툴킷을 사용함. 

### Introducing the OpenAI Gym toolkit

* OpenAI Gym은 RL 모델 개발을 촉진하기 위한 전문 툴킷임. OpenAI Gym은 몇 가지 사전 정의된 환경을 제공함. 
* 몇 가지 기본적인 예로는 카트폴과 마운틴카가 있는데, 각각 이름에서 알 수 있듯이 폴의 균형을 잡는 것과 언덕 위로 차를 옮기는 것이 임무임.
* 이 라이브러리는 pip를 사용하여 쉽게 수행할 수 있음. 

```python
pip install gym==0.20
```

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.3. The CartPole example in Gym
</figcaption>

* 이 예제 환경에서는 위 그림과 같이 수평으로 이동할 수 있는 카트에 폴이 부착되어 있음. 
* RL 에이전트의 목표는 카트를 이동하여 극을 안정화하고 카트가 어느 쪽으로 넘어지는 것을 방지하는 방법을 배우는 것임. 
* 이제 RL의 맥락에서 상태(또는 관찰) 공간, 행동 공간, 행동 실행 방법과 같은 CartPole 환경의 몇 가지 속성을 살펴봄. 

```python
>>> import gym
>>> env = gym.make('CartPole-v1')
>>> env.observation_space
Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)
>>> env.action_space
Discrete(2)
```

* 위의 코드를 통해 카트폴 문제를 위한 환경을 만듦. 이 환경의 관측 공간은 Box(4,)(-inf에서 inf까지의 플로트 값 포함)이며, 카트의 위치, 카트의 속도, 극의 각도, 극의 끝의 속도 등 네 가지 실제 값에 해당하는 4차원 공간을 나타냄. 행동 공간은 카트를 왼쪽 또는 오른쪽으로 미는 두 가지 선택이 가능한 이산 공간인 이산 공간 Discrete(2),

* 이전에 gym.make('CartPole-v1')를 호출하여 생성한 환경 개체 env는 각 에피소드에 앞서 환경을 재초기화하는 데 사용할 수 있는 재설정() 메서드를 가지고 있음. reset() 메서드를 호출하면 기본적으로 폴의 시작 상태($S_{0}$)가 설정됨.

```python
>>> env.reset()
array([-0.03908273, -0.00837535, 0.03277162, -0.0207195 ])
```

* 새 상태(또는 관측치)에 대한 배열
* 보상(플로트 유형의 스칼라 값)
* 종료 플래그(참 또는 거짓)
* 보조 정보가 포함된 Python 사전


### A grid world example

* 우리는 m 행과 n 열이 있는 단순한 환경인 그리드 세계 예제를 사용하여 작업할 것임. m = 5 및 n = 6을 고려하면 이 환경을 아래 그림과 같이 요약할 수 있음.

<figcaption style="text-align:center; font-size:15px; color:#808080">
Fig.3. An example of a grid world environment
</figcaption>

* 이 환경에서는 30개의 다른 가능한 상태가 있음. 이 중 4개 주는 말단 상태: 16개 주에서 금 한 솥, 10, 15, 22개 주에서 3개의 덫. 이 네 개의 최종 상태 중 하나에 착륙하면 에피소드가 종료됨.
* 골드 상태와 트랩 상태 간의 차이가 발생, 골드 상태에 착륙하면 +1의 양의 보상이 발생하는 반면, 에이전트를 트랩 상태 중 하나로 이동하면 -1의 음의 보상이 발생함. 다른 모든 주는 0의 보상을 가짐.
* 에이전트는 항상 상태 0부터 시작함. 따라서 환경을 재설정할 때마다 에이전트 상태가 0 상태로 돌아감. 행동 공간은 위, 아래, 왼쪽, 오른쪽 네 방향으로 구성됨.
* 에이전트가 그리드의 외부 경계에 있는 경우 그리드를 떠나는 작업을 선택해도 상태는 변경되지 않음. 

### Implementing the grid world environment in OpenAI Gym


```python
## Script: gridworld_env.py
import numpy as np
from gym.envs.toy_text import discrete
from collections import defaultdict
import time
import pickle
import os
from gym.envs.classic_control import rendering
CELL_SIZE = 100
MARGIN = 10
def get_coords(row, col, loc='center'):
 xc = (col+1.5) * CELL_SIZE
 yc = (row+1.5) * CELL_SIZE
 if loc == 'center':
 return xc, yc
 elif loc == 'interior_corners':
 half_size = CELL_SIZE//2 - MARGIN
 xl, xr = xc - half_size, xc + half_size
 yt, yb = xc - half_size, xc + half_size
 return [(xl, yt), (xr, yt), (xr, yb), (xl, yb)]
 elif loc == 'interior_triangle':
 x1, y1 = xc, yc + CELL_SIZE//3
 x2, y2 = xc + CELL_SIZE//3, yc - CELL_SIZE//3
 x3, y3 = xc - CELL_SIZE//3, yc - CELL_SIZE//3
 return [(x1, y1), (x2, y2), (x3, y3)]
def draw_object(coords_list):
 if len(coords_list) == 1: # -> circle
 obj = rendering.make_circle(int(0.45*CELL_SIZE))
 obj_transform = rendering.Transform()
 obj.add_attr(obj_transform)
 obj_transform.set_translation(*coords_list[0])
 obj.set_color(0.2, 0.2, 0.2) # -> black
 elif len(coords_list) == 3: # -> triangle
 obj = rendering.FilledPolygon(coords_list)
 obj.set_color(0.9, 0.6, 0.2) # -> yellow
 elif len(coords_list) > 3: # -> polygon
 obj = rendering.FilledPolygon(coords_list)
 obj.set_color(0.4, 0.4, 0.8) # -> blue
return obj
```

```python
class GridWorldEnv(discrete.DiscreteEnv):
 def __init__(self, num_rows=4, num_cols=6, delay=0.05):
 self.num_rows = num_rows
 self.num_cols = num_cols
 self.delay = delay
 move_up = lambda row, col: (max(row-1, 0), col)
 move_down = lambda row, col: (min(row+1, num_rows-1), col)
 move_left = lambda row, col: (row, max(col-1, 0))
 move_right = lambda row, col: (
 row, min(col+1, num_cols-1))
 self.action_defs={0: move_up, 1: move_right,
 2: move_down, 3: move_left}
 ## Number of states/actions
 nS = num_cols*num_rows
 nA = len(self.action_defs)
 self.grid2state_dict={(s//num_cols, s%num_cols):s
 for s in range(nS)}
 self.state2grid_dict={s:(s//num_cols, s%num_cols)
  for s in range(nS)}
 ## Gold state
 gold_cell = (num_rows//2, num_cols-2)
 
 ## Trap states
 trap_cells = [((gold_cell[0]+1), gold_cell[1]),
 (gold_cell[0], gold_cell[1]-1),
 ((gold_cell[0]-1), gold_cell[1])]
 gold_state = self.grid2state_dict[gold_cell]
 trap_states = [self.grid2state_dict[(r, c)]
 for (r, c) in trap_cells]
 self.terminal_states = [gold_state] + trap_states
 print(self.terminal_states)
 ## Build the transition probability
 P = defaultdict(dict)
 for s in range(nS):
 row, col = self.state2grid_dict[s]
 P[s] = defaultdict(list)
 for a in range(nA):
 action = self.action_defs[a]
 next_s = self.grid2state_dict[action(row, col)]
 
 ## Terminal state
 if self.is_terminal(next_s):
 r = (1.0 if next_s == self.terminal_states[0]
 else -1.0)
 else:
 r = 0.0
 if self.is_terminal(s):
 done = True
 next_s = s
 else:
 done = False
 P[s][a] = [(1.0, next_s, r, done)]
 ## Initial state distribution
 isd = np.zeros(nS)
 isd[0] = 1.0
 super().__init__(nS, nA, P, isd)
 self.viewer = None
 self._build_display(gold_cell, trap_cells)
  def is_terminal(self, state):
 return state in self.terminal_states
 def _build_display(self, gold_cell, trap_cells):
 screen_width = (self.num_cols+2) * CELL_SIZE
 screen_height = (self.num_rows+2) * CELL_SIZE
 self.viewer = rendering.Viewer(screen_width,
 screen_height)
 all_objects = []
 ## List of border points' coordinates
 bp_list = [
 (CELL_SIZE-MARGIN, CELL_SIZE-MARGIN),
 (screen_width-CELL_SIZE+MARGIN, CELL_SIZE-MARGIN),
 (screen_width-CELL_SIZE+MARGIN,
 screen_height-CELL_SIZE+MARGIN),
 (CELL_SIZE-MARGIN, screen_height-CELL_SIZE+MARGIN)
 ]
 border = rendering.PolyLine(bp_list, True)
 border.set_linewidth(5)
 all_objects.append(border)
 ## Vertical lines
 for col in range(self.num_cols+1):
 x1, y1 = (col+1)*CELL_SIZE, CELL_SIZE
 x2, y2 = (col+1)*CELL_SIZE,\
 (self.num_rows+1)*CELL_SIZE
 line = rendering.PolyLine([(x1, y1), (x2, y2)], False)
 all_objects.append(line)
 
 ## Horizontal lines
 for row in range(self.num_rows+1):
 x1, y1 = CELL_SIZE, (row+1)*CELL_SIZE
 x2, y2 = (self.num_cols+1)*CELL_SIZE,\
 (row+1)*CELL_SIZE
 line=rendering.PolyLine([(x1, y1), (x2, y2)], False)
 all_objects.append(line)
 
 ## Traps: --> circles
 for cell in trap_cells:
 trap_coords = get_coords(*cell, loc='center')
 all_objects.append(draw_object([trap_coords]))
 
  ## Gold: --> triangle
 gold_coords = get_coords(*gold_cell,
 loc='interior_triangle')
 all_objects.append(draw_object(gold_coords))
 ## Agent --> square or robot
 if (os.path.exists('robot-coordinates.pkl') and
 CELL_SIZE==100):
 agent_coords = pickle.load(
 open('robot-coordinates.pkl', 'rb'))
 starting_coords = get_coords(0, 0, loc='center')
 agent_coords += np.array(starting_coords)
 else:
 agent_coords = get_coords(
 0, 0, loc='interior_corners')
 agent = draw_object(agent_coords)
 self.agent_trans = rendering.Transform()
 agent.add_attr(self.agent_trans)
 all_objects.append(agent)
 for obj in all_objects:
 self.viewer.add_geom(obj)
 def render(self, mode='human', done=False):
 if done:
 sleep_time = 1
 else:
 sleep_time = self.delay
 x_coord = self.s % self.num_cols
 y_coord = self.s // self.num_cols
 x_coord = (x_coord+0) * CELL_SIZE
 y_coord = (y_coord+0) * CELL_SIZE
 self.agent_trans.set_translation(x_coord, y_coord)
 rend = self.viewer.render(
 return_rgb_array=(mode=='rgb_array'))
 time.sleep(sleep_time)
 return rend
 def close(self):
 if self.viewer:
 self.viewer.close()
 self.viewer = None
```

* 위 코드는 그리드 세계 환경을 구현하며, 이 환경의 인스턴스를 만들 수 있음. 그런 다음 카트폴 예제와 유사한 방식으로 상호 작용할 수 있습니다. 구현된 클래스인 GridWorldEnv는 상태를 재설정하기 위한 reset()과 작업을 실행하기 위한 step()과 같은 메서드를 상속합니다. 구현 세부사항은 다음과 같습니다.


### Solving the grid world problem with Q-learning

