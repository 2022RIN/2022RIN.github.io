---
layout: single
title:  "Chapter8. Applying Machine Learning to Sentiment Analysis"
use_math: true
---
* 소셜 미디어 시대에 사람들의 의견, 리뷰, 그리고 추천은 정치 과학과 비즈니스의 귀중한 자원임.
* 현대 기술로 이러한 데이터를 효율적으로 수집하고 분석할 수 있음. 
* 이번 챕터에서는 감정분석(sentiment analysis)이라는 자연어 처리(natural language processing; NLP)의 하위 분야를 살펴보고 기계 학습 알고리즘을 사용하여 감정 등 문서 분류 방법에 대해 논의함.
* Internet Movie Database(IMDb)에서 5만편의 영화 리뷰 데이터 세트를 활용하여 긍정과 부정 리뷰 구분 예측 기기를 구축할 것임. 

## 1. Preparing the IMDb movie review data for text processing
* 감정 분석(sentiment analysis)은 opinion mining이라고도 불리며 NLP의 인기있는 하위 분야임. 감정 분석에서 인기 있는 작업은 특정 주제에 대한 저자들의 표현, 의견, 감정을 바탕으로 문서를 분류하는 것을 의미하는 것임.
* 이번 챕터는 IMDb의 영화 리뷰로 작업되며, 긍정 또는 부정으로 분류되는 50,000개의 리뷰로 구성됨.

#### * Preprocessing the movie dataset into a more convenient format
* 데이터 세트를 추출한 후, 압축 해제된 다운로드 아카이브의 개별 텍스트 문서를 단일 CSV 파일로 assemble함.
* 아래의 코드는 표준 데스크톱 컴퓨터에서 pandas 데이터 프레임 개체로 영화 리뷰를 읽음.
* 50,000번의 반복으로 새로운 진행 표시줄 개체인 pbar를 초기화하였음. 
* 루프에 대한 중첩을 사용하여, 우리는 메인 aclImdb 디렉토리의 열차 및 테스트 하위 디렉토리를 반복하고 정수 클래스 레이블(1 = 양수 및 0 = 음수)과 함께 최종적으로 dfpanda DataFrame에 추가한 pos 및 neg 하위 디렉토리에서 개별 텍스트 파일을 읽음.

```python
>>> import pyprind # IMDb 데이터 Preprocessing을 위한 PyPrind 라이브러리
>>> import pandas as pd # pandas  데이터 프레임 개체로 리뷰를 읽음 
>>> import os
>>> import sys
>>> #'베이스 경로'를 다음 디렉터리로 변경
>>> # 압축 해제된 동영상 데이터 세트
>>> basepath = 'aclImdb'
>>>
>>> labels = {'pos': 1, 'neg': 0}
>>> pbar = pyprind.ProgBar(50000, stream=sys.stdout)
>>> df = pd.DataFrame()
>>> for s in ('test', 'train'):
... for l in ('pos', 'neg'):
... path = os.path.join(basepath, s, l)
... for file in sorted(os.listdir(path)):
... with open(os.path.join(path, file),
... 'r', encoding='utf-8') as infile:
... txt = infile.read()
... df = df.append([[txt, labels[l]]],
... ignore_index=True)
... pbar.update()
>>> df.columns = ['review', 'sentiment']

# 결과값
0% 100%
[##############################] | ETA: 00:00:00
Total time elapsed: 00:00:25
```

* 조립된 데이터 세트의 클래스 레이블이 정렬되었으므로 이제 np.random 하위 모듈의 순열 함수를 사용하여 DataFrame을 섞음.
* 이는 로컬 드라이브에서 데이터를 직접 스트리밍할 때 이후 섹션에서 데이터 세트를 훈련 및 테스트 데이터 세트로 분할하는 데 유용
* 편의를 위해, 저장 및 셔플된 영화 리뷰 데이터 세트를 CSV 파일로 저장함. 

```python
>>> import numpy as np
>>> np.random.seed(0)
>>> df = df.reindex(np.random.permutation(df.index))
>>> df.to_csv('movie_data.csv', index=False, encoding='utf-8')
```

## 2. Introducing the bag-of-words model
* 이 섹션에서는 텍스트를 수치 특징 벡터로 표현할 수 있는 bag-of-words model을 소개함. 해당 모델은 간단하며 다음과 같이 요약됨.
    1. 전체 문서 집합에서 고유 토큰(예: 단어)의 어휘를 만듬.
    2. 각 단어가 특정 문서에서 얼마나 자주 발생하는지를 포함하는 특징 벡터를 구성

#### * Transforming words into feature vectors

* 각 문서의 단어 수를 기반으로 bag-of-words model을 구성하기 위해 사이킷런에서 구현된 CountVectorizer 클래스를 사용함. 
* 다음 코드와 같이 CountVectorizer는 문서 또는 문장이 될 수 있는 텍스트 데이터 배열을 가져와서 bag-of-words model을 구성
* CountVectorizer에서 fit_transform을 호출하여 bag-of-words model의 어휘를 구성하고 다음 세 문장을 희소 형상 벡터로 변환하였음.
    * 'The sun is shining'
    * 'The weather is sweet'
    * 'The sun is shining, the weather is sweet, and one and one is two'

```python
>>> import numpy as np
>>> from sklearn.feature_extraction.text import CountVectorizer
>>> count = CountVectorizer()
>>> docs = np.array(['The sun is shining',... 'The weather is sweet',
... 'The sun is shining, the weather is sweet,'
... 'and one and one is two'])
>>> bag = count.fit_transform(docs)
```

* 결과는 다음과 같음.

```python
>>> print(count.vocabulary_)
{'and': 0,
'two': 7,
'shining': 3,
'one': 2,
'sun': 4,
'weather': 8,
'the': 6,
'sweet': 5,
'is': 1}

>>> print(bag.toarray())
[[0 1 0 1 1 0 1 0 0]
 [0 1 0 0 0 1 1 0 1]
 [2 3 2 1 1 1 2 1 1]]
```

* 여기서 표시된 특성 벡터의 각 인덱스 위치는 CountVectorizer 어휘의 사전 항목으로 저장된 정수 값에 해당함. 예를 들어, 인덱스 위치 0의 첫 번째 특성은 마지막 문서에서만 나타나는 단어 'and'의 카운트와 유사하며 인덱스 위치 1(문서 벡터의 두 번째 피쳐)의 단어 'is'는 세 문장 모두에서 발생함.
* 특성 벡터의 이러한 값은 raw term frequencies라고도 함: $tf(t, d)$ - 문서 $d$에서 용어 $t$가 발생하는 횟수. 
* bag-of-words model에서 문서의 단어 또는 용어 순서는 중요하지 않다는 점에 유의해야함. 
* 특징 벡터에서 빈도라는 용어가 나타나는 순서는 일반적으로 알파벳 순으로 할당되는 어휘 지수에서 파생됨.


#### * Assessing word relevancy via term frequency-inverse document frequency
* 텍스트 데이터를 분석할 때, 두 클래스의 자주 발생하는 단어들이 생겨남. 이러한 단어들은 일반적으로 유용하거나 구분이 가능한 차별적 정보를 포함하지 않음.
* 특성 벡터에서 이와 같은 단어들의 가중치를 낮추는데 유용한 **the term frequency-in-verse document frequency (tf-idf)** 에 대해 알아봄.

$tf-idf(t, d) = tf(t, d) × idf(t, d)$

* 여기서 tf(t, d)는 이전 섹션에서 소개한 용어 빈도(term frequency)이고 idf(t, d)는 역문서 빈도(inverse document frequency)이며, 다음과 같이 계산할 수 있음. 

$idf(t,d)=log\frac{n_{d}}{1+df(d,t)}$

* 여기서 $n_{d}$는 총 문서 수이고 $df(d, t)$는 $t$라는 용어가 포함된 문서 수 $d$임.
* 분모에 상수 1을 추가하는 것은 선택 사항이며, 교육 예제에서 발생하지 않는 항에 0이 아닌 값을 할당하는 데 사용됨. 로그는 낮은 문서 빈도에 너무 많은 가중치가 부여되지 않도록 하기 위해 사용됨.

* sickit-learn 라이브러리는 또 다른 변압기 TfidfTransformer 클래스를 구현 TfidfTransformer 클래스는 CountVectorizer 클래스의 원시 용어 주파수를 입력으로 가져와서 tf-idf로 변환함. 


```python
>>> from sklearn.feature_extraction.text import TfidfTransformer
>>> tfidf = TfidfTransformer(use_idf=True,
... norm='l2',
... smooth_idf=True)
>>> np.set_printoptions(precision=2)
>>> print(tfidf.fit_transform(count.fit_transform(docs))
... .toarray())
[[ 0. 0.43 0. 0.56 0.56 0. 0.43 0. 0. ]
 [ 0. 0.43 0. 0. 0. 0.56 0.43 0. 0.56]
 [ 0.5 0.45 0.5 0.19 0.19 0.19 0.3 0.25 0.19]]
```

* 'is'라는 단어는 세 번째 문서에서 가장 자주 나타나는 단어인 용어 빈도가 가장 높았음. 그러나 동일한 특징 벡터를 tf-idf로 변환한 후, 'is'라는 단어는 첫 번째와 두 번째 문서에도 존재하기 때문에 유용한 차별 정보를 포함할 가능성이 낮아 세 번째 문서에서 상대적으로 작은 tf-idf(0.45)와 연관됨.

* sickit-learn에서 구현된 역문서 빈도에 대한 방정식은 다음과 같이 계산

$idf(t,d)=log\frac{1+n_{d}}{1+df(d,t)}$

* 마찬가지로 skickit-learn에서 계산된 tf-idf는 앞에서 정의한 기본 방정식에서 약간 변형됨. 

$tf-idf(t, d) = tf(t, d) × (idf(t, d)+1)$


* 이는 이전 idf 방정식의 "+1"은 smooth_idf= 설정 때문임.
* 이는 모든 문서에서 발생하는 용어에 0 가중치(즉, idf(t, d) = log(1) = 0)를 할당하는 데 유용

## 3. Cleaning text data

* 단어 가방 모델(the bag-of-words model)을 구축하기 전에 첫 번째 중요한 단계는 원하지 않는 문자를 모두 제거하여 텍스트 데이터를 정리하는 것임. 
* 이것이 왜 중요한지 설명하기 위해 재구성된 영화 리뷰 데이터 세트의 첫 번째 문서에서 마지막 50자를 표시

```python
>>> df.loc[0, 'review'][-50:]
'is seven.<br /><br />Title (Brazil): Not Available'
```

* 여기서 볼 수 있듯이 텍스트에는 HTML 마크업과 구두점 및 기타 문자가 아닌 문자가 포함되어 있음. HTML 마크업에는 많은 유용한 의미론이 포함되어 있지 않지만 문장 부호는 특정 NLP 컨텍스트에서 유용한 추가 정보를 나타낼 수 있음. 그러나 이제 :)와 같은 이모티콘 문자를 제외한 모든 문장 부호는 정서 분석에서 확실히 유용하기 때문에 간단히 제거할 것임.

* 이 작업을 수행하려면 여기에 표시된 것처럼 파이썬의 정규식(regex) 라이브러리를 사용합

```python
>>> import re
>>> def preprocessor(text):
... text = re.sub('<[^>]*>', '', text)
... emoticons = re.findall('(?::|;|=)(?:-)?(?:\)|\(|D|P)',
... text)
... text = (re.sub('[\W]+', ' ', text.lower()) +
... ' '.join(emoticons).replace('-', ''))
... return text
```

* 정리된 문서 문자열 끝에 이모티콘 문자를 추가하는 것이 가장 우아한 접근 방식처럼 보이지 않을 수 있지만, 어휘가 단 한 단어 토큰으로만 구성되어 있다면 단어 순서도 중요하지 않다는 점에 유의
* 그러나 개별 용어, 단어 또는 토큰으로 문서를 분할하는 것에 대해 더 이야기하기 전에, 우리의 프리프로세서 기능이 제대로 작동하는지 확인함.

```python
>>> preprocessor(df.loc[0, 'review'][-50:])
'is seven title brazil not available'
>>> preprocessor("</a>This :) is :( a test :-)!")
'this is a test :) :( :)'
```

* 마지막으로, 다음 섹션에서 정리된 텍스트 데이터를 계속해서 사용할 예정이므로, 이제 프리프로세서 기능을 데이터 프레임의 모든 영화 리뷰에 적용함.

```python
>>> df['review'] = df['review'].apply(preprocessor)
```

## 4. Processing documents into tokens

* 영화 리뷰 데이터 세트를 성공적으로 준비한 후, 이제 텍스트 말뭉치를 개별 요소로 분할하는 방법에 대해 생각해 볼 필요가 있음. 문서를 토큰화하는 한 가지 방법은 정리된 문서를 공백 문자로 분할하여 개별 단어로 분할하는 것임.

```python
>>> def tokenizer(text):
... return text.split()
>>> tokenizer('runners like running and thus they run')
['runners', 'like', 'running', 'and', 'thus', 'they', 'run']
```

* 토큰화의 맥락에서, 또 다른 유용한 기술은 단어를 루트 형태로 변환하는 과정인 단어 고정(word stemming)이며, 우리가 관련된 단어들을 같은 줄기에 매핑할 수 있게 해줌.
* 다음 코드는 포터 스트레이닝 알고리즘을 사용하는 방법을 보여줌.


```python
>>> from nltk.stem.porter import PorterStemmer
>>> porter = PorterStemmer()
>>> def tokenizer_porter(text):
... return [porter.stem(word) for word in text.split()]
>>> tokenizer_porter('runners like running and thus they run')
['runner', 'like', 'run', 'and', 'thu', 'they', 'run']
```

* 다음 섹션으로 넘어가기 전에 단어 봉지 모델을 사용하여 기계 학습 모델을 교육함. 
* 단어 제거 중지(stop word removal)라는 또 다른 유용한 주제에 대해 간략하게 설명함. 정지 단어는 모든 종류의 텍스트에서 매우 일반적인 단어일 뿐이며 다른 종류의 문서를 구별하는 데 사용될 수 있는 유용한 정보가 없을 수도 있음.
* 정지 단어의 예는 is, and, has, and like입니다. 정지 단어를 제거하는 것은 우리가 이미 자주 발생하는 단어의 무게를 줄이는 tf-idfs가 아닌 원시 또는 정규화된 용어 빈도로 작업할 때 유용할 수 있음.
*  영화 리뷰에서 스톱워드를 제거하기 위해, 우리는 NLTK 라이브러리에서 사용할 수 있는 127개의 영어 스톱워드 세트를 사용할 임. 이것은 nltk.download 함수를 호출하여 얻을 수 있음.

```python
>>> import nltk
>>> nltk.download('stopwords')
>>> from nltk.corpus import stopwords
>>> stop = stopwords.words('english')
>>> [w for w in tokenizer_porter('a runner likes'
... ' running and runs a lot')
... if w not in stop]
['runner', 'like', 'run', 'run', 'lot']
```

## 5. Training a logistic regression model for document classification

* 이 섹션에서는 단어장 모델을 기반으로 영화 리뷰를 긍정적인 리뷰와 부정적인 리뷰로 분류하는 로지스틱 회귀 모델을 교육할 것임.
* 먼저, 클리닝된 텍스트 문서의 데이터 프레임을 교육을 위한 25,000개의 문서와 테스트를 위한 25,000개의 문서로 나눔.

```python
>>> X_train = df.loc[:25000, 'review'].values
>>> y_train = df.loc[:25000, 'sentiment'].values
>>> X_test = df.loc[25000:, 'review'].values
>>> y_test = df.loc[25000:, 'sentiment'].values
```

* 그런 다음 GridSearchCV 개체를 사용하여 5배 계층화된 교차 검증을 사용하여 로지스틱 회귀 모형에 대한 최적의 모수 집합을 찾음.

```python
>>> from sklearn.model_selection import GridSearchCV
>>> from sklearn.pipeline import Pipeline
>>> from sklearn.linear_model import LogisticRegression
>>> from sklearn.feature_extraction.text import TfidfVectorizer
>>> tfidf = TfidfVectorizer(strip_accents=None,
... lowercase=False,
... preprocessor=None)
>>> small_param_grid = [
... {
... 'vect__ngram_range': [(1, 1)],
... 'vect__stop_words': [None],
... 'vect__tokenizer': [tokenizer, tokenizer_porter],
... 'clf__penalty': ['l2'],
... 'clf__C': [1.0, 10.0]
... },
... {
... 'vect__ngram_range': [(1, 1)],
... 'vect__stop_words': [stop, None],
... 'vect__tokenizer': [tokenizer],
... 'vect__use_idf':[False],
... 'vect__norm':[None],
... 'clf__penalty': ['l2'],
... 'clf__C': [1.0, 10.0]
... },
... ]
>>> lr_tfidf = Pipeline([
... ('vect', tfidf),
... ('clf', LogisticRegression(solver='liblinear'))
... ])
>>> gs_lr_tfidf = GridSearchCV(lr_tfidf, small_param_grid,
... scoring='accuracy', cv=5,
... verbose=2, n_jobs=1)
>>> gs_lr_tfidf.fit(X_train, y_train)
```


#### * Working with bigger data – online algorithms and out of-core learnin

* 이전 섹션의 코드 예를 실행한 경우 그리드 검색 중에 50,000편의 영화 검토 데이터 세트에 대한 특성 벡터를 구성하는 데 계산적으로 상당한 비용이 들 수 있음.
* 이제 우리는 분류기를 데이터 세트의 더 작은 배치에 점진적으로 맞추어서 그러한 큰 데이터 세트로 작업할 수 있는 아웃 오브 코어 학습이라는 기술을 적용할 것임. 
* 2장 '분류를 위한 간단한 기계 학습 알고리즘 훈련'에서 확률적 그레이디언트 강하 개념이 소개되었음. 이것은 한 번에 하나의 예를 사용하여 모델의 가중치를 업데이트하는 최적화 알고리즘임. 
* 이 섹션에서는 skickit-learn에서 SGDC 분류기의 partial_fit 함수를 사용하여 로컬 드라이브에서 직접 문서를 스트리밍하고 작은 미니 문서 배치를 사용하여 로지스틱 회귀 모델을 교육함.
* 먼저, 이 장의 시작 부분에서 구성한 movie_data.csv 파일에서 처리되지 않은 텍스트 데이터를 정리하고 정지 단어를 제거하면서 워드 토큰으로 분리하는 토큰라이저 함수를 정의함.


```python
>>> import numpy as np
>>> import re
>>> from nltk.corpus import stopwords
>>> stop = stopwords.words('english')
>>> def tokenizer(text):
... text = re.sub('<[^>]*>', '', text)
... emoticons = re.findall('(?::|;|=)(?:-)?(?:\)|\(|D|P)',
... text.lower())
... text = re.sub('[\W]+', ' ', text.lower()) \
... + ' '.join(emoticons).replace('-', '')
... tokenized = [w for w in text.split() if w not in stop]
... return tokenized
```

* 다음으로 한 번에 하나의 문서를 읽고 반환하는 생성 함수 stream_docs를 정의

```python
a time:
>>> def stream_docs(path):
... with open(path, 'r', encoding='utf-8') as csv:
... next(csv) # skip header
... for line in csv:
... text, label = line[:-3], int(line[-2])
... yield text, label
```

* stream_docs 함수가 제대로 작동하는지 확인하기 위해 movie_data.csv 파일에서 첫 번째 문서를 movie_data.csv 파일은 검토 텍스트와 해당 클래스 레이블로 구성된 튜플을 반환함. 
* 이제 stream_docs 함수에서 문서 스트림을 가져오고 size 매개 변수로 지정된 특정 수의 문서를 반환하는 함수 get_minibatch를 정의

```python
>>> def get_minibatch(doc_stream, size):
... docs, y = [], []
... try:
... for _ in range(size):
... text, label = next(doc_stream)
... docs.append(text)
... y.append(label)
... except StopIteration:
... return None, None
... return docs, y
```

* skickit-learn에서 구현된 텍스트 처리에 대한 또 다른 유용한 벡터기는 HashingVectorizer임/. Hashing Vectorizer는 데이터에 구애받지 않으며 Austin Appleby의 32비트 MurmurHash3 함수를 통해 해싱 트릭을 사용해야함.

```python
>>> from sklearn.feature_extraction.text import HashingVectorizer
>>> from sklearn.linear_model import SGDClassifier
>>> vect = HashingVectorizer(decode_error='ignore',
... n_features=2**21,
... preprocessor=None,
... tokenizer=tokenizer)
>>> clf = SGDClassifier(loss='log', random_state=1)
>>> doc_stream = stream_docs(path='movie_data.csv')
```

* 모든 보완 기능을 설정한 후에는 다음 코드를 사용하여 핵심 외 학습을 시작할 수 있음. 

```python
>>> import pyprind
>>> pbar = pyprind.ProgBar(45)
>>> classes = np.array([0, 1])
>>> for _ in range(45):
... X_train, y_train = get_minibatch(doc_stream, size=1000)
... if not X_train:
... break
... X_train = vect.transform(X_train)
... clf.partial_fit(X_train, y_train, classes=classes)
... pbar.update()
0% 100%
[##############################] | ETA: 00:00:00
Total time elapsed: 00:00:21
```

* PyPrind 패키지를 사용하여 학습 알고리듬의 진행률을 추정했음. 45번의 반복으로 진행 표시줄 개체를 초기화하고 루프에 대한 다음에서 각 미니 배치가 1,000개의 문서로 구성된 45개의 미니 배치 이상의 문서를 반복했음. 증분 학습 프로세스를 완료한 후 마지막 5,000개의 문서를 사용하여 모델의 성능을 평가함.

```python
>>> X_test, y_test = get_minibatch(doc_stream, size=5000)
>>> X_test = vect.transform(X_test)
>>> print(f'Accuracy: {clf.score(X_test, y_test):.3f}')
Accuracy: 0.868
```

* 모델의 정확도는 약 87%로 이전 섹션에서 하이퍼 파라미터 조정을 위해 그리드 검색을 사용하여 달성한 정확도보다 약간 낮음.
* 하지만 코어 외 학습은 메모리 효율성이 매우 높고 완료하는 데 1분도 걸리지 않았으며, 마지막 5,000개의 문서를 사용하여 모델을 업데이트할 수 있음.


```python
>>> clf = clf.partial_fit(X_test, y_test)
```

## 6. Topic modeling with latent Dirichlet allocation 

* 주제 모델링(Topic modeling)은 레이블이 지정되지 않은 텍스트 문서에 주제를 할당하는 광범위한 작업을 설명함.
* 예를 들어, 일반적인 응용 프로그램은 신문 기사의 큰 텍스트 말뭉치에 있는 문서를 분류하는 것임. 그런 다음 주제 모델링 응용 프로그램에서 스포츠, 금융, 세계 뉴스, 정치 및 지역 뉴스와 같은 기사에 범주 레이블을 지정하는 것을 목표로 함. 
* 이 섹션에서는 잠재 디리클레 할당(LDA)이라고 하는 인기 있는 주제 모델링 기술에 대해 논의

#### * LDA with scikit-learn

* 이 하위 섹션에서는 skickit-learn에서 구현된 잠재 DirichletAllocation 클래스를 사용하여 영화 리뷰 데이터 세트를 분해하고 다른 주제로 분류함.
* 다음 예제에서는 분석을 10개의 다른 주제로 제한하지만, 독자들은 이 데이터 세트에서 찾을 수 있는 주제를 더 자세히 탐색하기 위해 알고리듬의 초 매개 변수를 실험하는 것이 좋음.
* 먼저 이 장의 시작 부분에서 만든 영화 리뷰의 로컬 movie_data.csv 파일을 사용하여 데이터 세트를 판다 데이터 프레임에 로드함.


```python
>>> import pandas as pd
>>> df = pd.read_csv('movie_data.csv', encoding='utf-8')
>>> # the following is necessary on some computers:
>>> df = df.rename(columns={"0": "review", "1": "sentiment"})
```

* 다음으로, 우리는 이미 익숙한 CountVectorizer를 사용하여 LDA에 대한 입력으로 단어 가방 행렬을 만듦.
* 편의상 scickit-learn의 내장 영어 스톱 워드 라이브러리를 stop_words='english'를 통해 사용함.


```python
>>> from sklearn.feature_extraction.text import CountVectorizer
>>> count = CountVectorizer(stop_words='english',
... max_df=.1,
... max_features=5000)
>>> X = count.fit_transform(df['review'].values)
```

* 자주 발생하는 단어를 제외하기 위해 고려할 단어의 최대 문서 빈도를 10%(max_df=.1)로 설정
* 자주 발생하는 단어를 제거하는 이유는 모든 문서에서 나타나는 일반적인 단어일 수 있기 때문에 특정 문서의 특정 주제 카테고리와 연관될 가능성이 낮기 때문임.
* 또한 LDA가 수행하는 추론을 개선하기 위해 이 데이터 세트의 차원성을 제한하기 위해 고려할 단어의 수를 가장 자주 발생하는 5,000단어로 제한(max_delay=delays). 
* 그러나 max_df=.1 및 max_df=devolution은 모두 임의로 선택된 초 매개 변수 값이며, 독자들은 결과를 비교하면서 조정하는 것이 좋음.

* 다음 코드 예제에서는 Bag-of-Words 매트릭스에 잠재 Dirichlet Allocation 추정기를 맞추고 문서로부터 10가지 다른 주제를 추론하는 방법을 보여 줌.

```python
>>> from sklearn.decomposition import LatentDirichletAllocation
>>> lda = LatentDirichletAllocation(n_components=10,
... random_state=123,
... learning_method='batch')
>>> X_topics = lda.fit_transform(X)
```

* learning_message='message'를 설정함으로써, 우리는 lda 추정자가 한 번의 반복에서 사용 가능한 모든 훈련 데이터(단어 모음 행렬)를 기반으로 추정하도록함.

* LDA를 적합시킨 후, lda 인스턴스의 components_속성에 액세스할 수 있음. lda 인스턴스의 components_속성은 10개 주제 각각에 대한 중요도(여기, 5000)라는 단어를 포함하는 행렬을 순서대로 저장함.

```python
>>> lda.components_.shape
(10, 5000)
```

* 결과 분석을 위해 10개의 주제 각각에 대해 가장 중요한 단어 5개를 인쇄함.
* 상위 5개 단어를 인쇄하려면 주제 배열을 역순으로 정렬해야함.

```python
>>> n_top_words = 5
>>> feature_names = count.get_feature_names_out()
>>> for topic_idx, topic in enumerate(lda.components_):
... print(f'Topic {(topic_idx + 1)}:')
... print(' '.join([feature_names[i]
... for i in topic.argsort()\
... [:-n_top_words - 1:-1]]))
Topic 1:
worst minutes awful script stupid
Topic 2:
family mother father children girl
Topic 3:
american war dvd music tv
Topic 4:
human audience cinema art sense
Topic 5:
police guy car dead murder
Topic 6:
horror house sex girl woman
Topic 7:
role performance comedy actor performances
Topic 8:
series episode war episodes tv
Topic 9:
book version original read novel
Topic 10:
action fight guy guys cool
```

* 리뷰를 바탕으로 범주가 이치에 맞는지 확인하기 위해 공포 영화 범주에서 세 개의 영화를 플롯함.

```python
>>> horror = X_topics[:, 5].argsort()[::-1]
>>> for iter_idx, movie_idx in enumerate(horror[:3]):
... print(f'\nHorror movie #{(iter_idx + 1)}:')
... print(df['review'][movie_idx][:300], '...')
Horror movie #1:
House of Dracula works from the same basic premise as House of Frankenstein 
from the year before; namely that Universal's three most famous monsters; 
Dracula, Frankenstein's Monster and The Wolf Man are appearing in the movie 
together. Naturally, the film is rather messy therefore, but the fact that ...
Horror movie #2:
Okay, what the hell kind of TRASH have I been watching now? "The Witches' 
Mountain" has got to be one of the most incoherent and insane Spanish 
exploitation flicks ever and yet, at the same time, it's also strangely 
compelling. There's absolutely nothing that makes sense here and I even doubt 
there ...
Horror movie #3:
<br /><br />Horror movie time, Japanese style. Uzumaki/Spiral was a total 
freakfest from start to finish. A fun freakfest at that, but at times it was 
a tad too reliant on kitsch rather than the horror. The story is difficult to 
summarize succinctly: a carefree, normal teenage girl starts coming fac ...
```
